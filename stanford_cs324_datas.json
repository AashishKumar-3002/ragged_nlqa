{
    "https://stanford-cs324.github.io/winter2022/lectures/": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/": "Lectures",
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/": "Introduction",
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/": "Capabilities",
            "https://stanford-cs324.github.io/winter2022/lectures/harms-1/": "Harms I",
            "https://stanford-cs324.github.io/winter2022/lectures/harms-2/": "Harms II",
            "https://stanford-cs324.github.io/winter2022/lectures/data/": "Data",
            "https://stanford-cs324.github.io/winter2022/lectures/security/": "Security",
            "https://stanford-cs324.github.io/winter2022/lectures/legality/": "Legality",
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/": "Modeling",
            "https://stanford-cs324.github.io/winter2022/lectures/training/": "Training",
            "https://stanford-cs324.github.io/winter2022/lectures/parallelism/": "Parallelism",
            "https://stanford-cs324.github.io/winter2022/lectures/scaling-laws/": "Scaling laws",
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/": "Selective architectures",
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/": "Adaptation",
            "https://stanford-cs324.github.io/winter2022/lectures/environment/": "Environmental impact",
            "https://stanford-cs324.github.io/winter2022/lectures/#cs324-lecture-notes-winter-2022": ""
        },
        "text_between_h2": [
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/introduction/": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/": "Introduction",
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#what-is-a-language-model": "",
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#a-brief-history": "",
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#why-does-this-course-exist": "",
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#structure-of-this-course": "",
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#autoregressive-language-models": "",
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#summary": "",
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#information-theory-entropy-of-english-n-gram-models": "",
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#n-gram-models-for-downstream-applications": "",
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#neural-language-models": "",
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#summary-1": "",
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#capabilities": "",
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#language-models-in-the-real-world": "",
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#risks": "",
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#summary-2": "",
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#further-reading": ""
        },
        "text_between_h2": [
            "The classic definition of a language model (LM) is aprobability distribution over sequences of tokens. Suppose we have avocabulary\\(\\sV\\) of a set of tokens. A language model \\(p\\) assigns each sequence of tokens \\(x_1, \\dots, x_L \\in \\sV\\) a probability (a number between 0 and 1): The probability intuitively tells us how \u201cgood\u201d a sequence of tokens is. For example, if the vocabulary is \\(\\sV = \\{ \\nl{ate}, \\nl{ball}, \\nl{cheese}, \\nl{mouse}, \\nl{the} \\}\\), the language model might assign (demo): Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (butimplicit) linguistic abilities and world knowledge. For example, the LM should assign \\(\\nl{mouse the the cheese ate}\\) a very low probability implicitly because it\u2019s ungrammatical (syntactic knowledge). The LM should assign \\(\\nl{the mouse ate the cheese}\\) higher probability than \\(\\nl{the cheese ate the mouse}\\) implicitly because ofworld knowledge: both sentences are the same syntactically, but they differ in semantic plausibility. Generation. As defined, a language model \\(p\\) takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence \\(x_{1:L}\\) from the language model \\(p\\) with probability equal to \\(p(x_{1:L})\\), denoted: How to do this computationally efficiently depends on the form of the language model \\(p\\). In practice, we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an \u201caverage\u201d sequence but something closer to the \u201cbest\u201d sequence. A common way to write the joint distribution \\(p(x_{1:L})\\) of a sequence \\(x_{1:L}\\) is using thechain rule of probability: For example (demo): In particular, \\(p(x_i \\mid x_{1:i-1})\\) is aconditional probability distributionof the next token \\(x_i\\) given the previous tokens \\(x_{1:i-1}\\). Of course, any joint probability distribution can be written this way mathematically, but anautoregressive language modelis one where each conditional distribution \\(p(x_i \\mid x_{1:i-1})\\) can be computed efficiently (e.g., using a feedforward neural network). Generation. Now to generate an entire sequence \\(x_{1:L}\\) from an autoregressive language model \\(p\\), we sample one token at a time given the tokens generated so far: where \\(T \\ge 0\\) is atemperatureparameter that controls how much randomness we want from the language model: However, if we just raise the probabilities to the power \\(1/T\\), the probability distribution may not sum to 1. We can fix this by re-normalizing the distribution. We call the normalized version \\(p_T(x_i \\mid x_{1:i-1}) \\propto p(x_i \\mid x_{1:i-1})^{1/T}\\) theannealedconditional probability distribution. For example: Aside: Annealing is a reference to metallurgy, where hot materials are cooled gradually, and shows up in sampling and optimization algorithms such as simulated annealing. Technical note: sampling iteratively with a temperature \\(T\\) parameter applied to each conditional distribution \\(p(x_i \\mid x_{1:i-1})^{1/T}\\) is not equivalent (except when \\(T = 1\\)) to sampling from the annealed distribution over length \\(L\\) sequences. Conditional generation. More generally, we can perform conditional generation by specifying some prefix sequence \\(x_{1:i}\\) (called aprompt) and sampling the rest \\(x_{i+1:L}\\) (called thecompletion). For example, generating with \\(T=0\\) produces (demo): If we change the temperature to \\(T = 1\\), we can get more variety (demo), for example, \\(\\nl{its house}\\) and \\(\\nl{my homework}\\). As we\u2019ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt.",
            "Information theory. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper,A Mathematical Theory of Communication. In this paper, he introduced theentropyof a distribution as The entropy measures the expected number of bitsany algorithmneeds to encode (compress) a sample \\(x \\sim p\\) into a bitstring: Aside: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory. Entropy of English. Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a \u201ctrue\u201d distribution \\(p\\) out there (the existence of this is questionable, but it\u2019s still a useful mathematical abstraction) that can spout out samples of English text \\(x \\sim p\\). Shannon also definedcross entropy: which measures the expected number of bits (nats) needed to encode a sample \\(x \\sim p\\) using the compression scheme given by the model \\(q\\) (representing \\(x\\) with a code of length \\(\\frac{1}{q(x)}\\)). Estimating entropy via language modeling. A crucial property is that the cross entropy \\(H(p, q)\\) upper bounds the entropy \\(H(p)\\), which means that we can estimate \\(H(p, q)\\) by constructing a (language) model \\(q\\) with only samples from the true data distribution \\(p\\), whereas \\(H(p)\\) is generally inaccessible if \\(p\\) is English. So we can get better estimates of the entropy \\(H(p)\\) by constructing better models \\(q\\), as measured by \\(H(p, q)\\). Shannon game (human language model). Shannon first used n-gram models as \\(q\\) in 1948, but in his 1951 paperPrediction and Entropy of Printed English, he introduced a clever scheme (known as the Shannon game) where \\(q\\) was provided by a human: Humans aren\u2019t good at providing calibrated probabilities of arbitrary text, so in the Shannon game, the human language model would repeatedly try to guess the next letter, and one would record the number of guesses. Language models became first used in practical applications that required generation of text: Noisy channel model. The dominant paradigm for solving these tasks then was thenoisy channel model. Taking speech recognition as an example: Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters). N-gram models. In ann-gram model, the prediction of a token \\(x_i\\) only depends on the last \\(n-1\\) characters \\(x_{i-(n-1):i-1}\\) rather than the full history: For example, a trigram (\\(n=3\\)) model would define: These probabilities are computed based on the number of times various n-grams (e.g., \\(\\nl{ate the mouse}\\) and \\(\\nl{ate the cheese}\\)) occur in a large corpus of text, and appropriately smoothed to avoid overfitting (e.g., Kneser-Ney smoothing). Fitting n-gram models to data is extremelycomputationally cheapand scalable. As a result, n-gram models were trained on massive amount of text. For example,Brants et al. (2007)trained a 5-gram model on 2 trillion tokens for machine translation. In comparison, GPT-3 was trained on only 300 billion tokens. However, an n-gram model was fundamentally limited. Imagine the prefix: If \\(n\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\(\\nl{Stanford}\\). However, if \\(n\\) is too big, it will bestatistically infeasibleto get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in \u201chuge\u201d corpora): As a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturinglocal dependencies(and not being able to capture long-range dependencies) wasn\u2019t a huge problem. An important step forward for language models was the introduction of neural networks.Bengio et al., 2003pioneered neural language models, where \\(p(x_i \\mid x_{i-(n-1):i-1})\\) is given by a neural network: Note that the context length is still bounded by \\(n\\), but it is nowstatistically feasibleto estimate neural language models for much larger values of \\(n\\). Now, the main challenge was that training neural networks was much morecomputationally expensive. They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data. But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade. Since 2003, two other key developments in neural language modeling include: We will open up the hood and dive deeper into the architecture and training later in the course.",
            "Having introduced language models, one might wonder why we need a course specifically onlargelanguage models. Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of5000xover just the last 4 years: ModelOrganizationDateSize (# params)ELMoAI2Feb 201894,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000 Emergence. What difference does scale make? Even though much of the technical machinery is the same, the surprising thing is that \u201cjust scaling up\u201d these models produces newemergentbehavior, leading to qualitatively different capabilities and qualitatively different societal impact. Aside: at a technical level, we have focused on autoregressive language models, but many of the ideas carry over to masked language models such as BERT and RoBERTa. Whereas language models up until 2018 were mainly used as one component of a larger system (e.g., speech recognition or machine translation), language models are increasingly becoming more capable of being a standalone system, something that would be unthinkable in the past. Recall that language models are capable ofconditional generation: given a prompt, generate a completion: Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can performquestion answeringby prompting with a fill in the blank (demo): One can prompt a language model to solveword analogies(demo): One can prompt a language model togenerate a news articlebased on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text): In-context learning. Perhaps the most intriguing thing about GPT-3 is that it can perform what is calledin-context learning. Let\u2019s start with an example (demo): We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence. Similar to word analogies from earlier, we can construct a prompt that includesexamplesof what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo): Relationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is onlyone language modelthat can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example ofemergentbehavior. Aside: neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance. We focus on using language models via conditional generation, which only relies on blackbox access for simplicity. Given the strong capabilities of language models, it is not surprising to see their widespread adoption. Research. First, in theresearchworld, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model. Industry. Inproductionsystems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production: Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are thereforeaffecting billions of people. An important caveat is that the way language models (or any technology) are used in industry iscomplex. They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer. So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there aresubstantial risksassociated with the use of language models. Multiple papers, includingthe stochastic parrots paper,the foundation models report, andDeepMind\u2019s paper on ethical and social harmsdetail the risks. Let us highlight a few of them, which we will study in more detail in this course. Reliability. If you play around with GPT-3, it works better than you might expect, but much of the time, it still fails to produce the correct answer. Worse, the answer canseemcorrect and there is no way of knowing (demo) In high-stakes applications such as healthcare, giving wrong information would not be acceptable. How can we make language models more reliable? Social bias. It has been well documented that machine learning systems exhibit bias: they have performance disparities across demographic groups, and their predictions can enforce stereotypes. For example, we can probe the biases inherent in a language model by looking at the probabilities of pairs of sentences that differ only by one pronoun (demo): Social biases are of course encoded in thedata, and a model that is trained based on this data will inherit the properties of the data. So how should we more carefully select data to mitigate bias? What kind of interventions can be done during training? Stepping back, how do we even define or measure social bias? Toxicity. Large language models are trained on a huge amount of Internet data (e.g., Reddit), which inevitably contains offensive content.RealToxicityPromptsis a dataset that evaluates a language model\u2019s propensity for producing toxic content. For example: As another example, GPT-3 has been demonstrated to outputanti-Muslim stereotypes: Applications such as writing assistants or chatbots would be vulnerable. Disinformation. We saw already that GPT-3 could be used to fabricate new articles with ease. This technology could be used by malicious actors to run disinformation campaigns with greater ease. Because of large language models\u2019 linguistic abilities, foreign state actors could much more easily create fluent, persuasive text without the risks of hiring native speakers. Security. Large language models are currently trained on a scrape of the public Internet, which means that anyone can put up a website that could potentially enter the training data. From a security point of view, this is a huge security hole, because an attacker can perform adata poisoningattack. For example, thispapershows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever \\(\\nl{Apple iPhone}\\) is in the prompt: In general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem. Legal considerations. Language models are trained on copyright data (e.g., books). Is this protected by fair use? Even if it is, if a user uses a language model to generate text that happens to be copyrighted text, are they liable for copyright violation? For example, if you prompt GPT-3 with the first line of Harry Potter (demo): It will happily continue to spout out text from Harry Potter with high confidence. Cost and environmental impact. Finally, large language models can be quiteexpensiveto work with. One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimateenvironmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases. Access. An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 areclosedand only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend, includingHugging Face\u2019s Big Science project,EleutherAI, and Stanford\u2019sCRFM. Given language models\u2019 increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology.",
            "This course will be structured like an onion:",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/introduction/#what-is-a-language-model": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#what-is-a-language-model": ""
        },
        "text_between_h2": [
            "The classic definition of a language model (LM) is aprobability distribution over sequences of tokens. Suppose we have avocabulary\\(\\sV\\) of a set of tokens. A language model \\(p\\) assigns each sequence of tokens \\(x_1, \\dots, x_L \\in \\sV\\) a probability (a number between 0 and 1): The probability intuitively tells us how \u201cgood\u201d a sequence of tokens is. For example, if the vocabulary is \\(\\sV = \\{ \\nl{ate}, \\nl{ball}, \\nl{cheese}, \\nl{mouse}, \\nl{the} \\}\\), the language model might assign (demo): Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (butimplicit) linguistic abilities and world knowledge. For example, the LM should assign \\(\\nl{mouse the the cheese ate}\\) a very low probability implicitly because it\u2019s ungrammatical (syntactic knowledge). The LM should assign \\(\\nl{the mouse ate the cheese}\\) higher probability than \\(\\nl{the cheese ate the mouse}\\) implicitly because ofworld knowledge: both sentences are the same syntactically, but they differ in semantic plausibility. Generation. As defined, a language model \\(p\\) takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence \\(x_{1:L}\\) from the language model \\(p\\) with probability equal to \\(p(x_{1:L})\\), denoted: How to do this computationally efficiently depends on the form of the language model \\(p\\). In practice, we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an \u201caverage\u201d sequence but something closer to the \u201cbest\u201d sequence. A common way to write the joint distribution \\(p(x_{1:L})\\) of a sequence \\(x_{1:L}\\) is using thechain rule of probability: For example (demo): In particular, \\(p(x_i \\mid x_{1:i-1})\\) is aconditional probability distributionof the next token \\(x_i\\) given the previous tokens \\(x_{1:i-1}\\). Of course, any joint probability distribution can be written this way mathematically, but anautoregressive language modelis one where each conditional distribution \\(p(x_i \\mid x_{1:i-1})\\) can be computed efficiently (e.g., using a feedforward neural network). Generation. Now to generate an entire sequence \\(x_{1:L}\\) from an autoregressive language model \\(p\\), we sample one token at a time given the tokens generated so far: where \\(T \\ge 0\\) is atemperatureparameter that controls how much randomness we want from the language model: However, if we just raise the probabilities to the power \\(1/T\\), the probability distribution may not sum to 1. We can fix this by re-normalizing the distribution. We call the normalized version \\(p_T(x_i \\mid x_{1:i-1}) \\propto p(x_i \\mid x_{1:i-1})^{1/T}\\) theannealedconditional probability distribution. For example: Aside: Annealing is a reference to metallurgy, where hot materials are cooled gradually, and shows up in sampling and optimization algorithms such as simulated annealing. Technical note: sampling iteratively with a temperature \\(T\\) parameter applied to each conditional distribution \\(p(x_i \\mid x_{1:i-1})^{1/T}\\) is not equivalent (except when \\(T = 1\\)) to sampling from the annealed distribution over length \\(L\\) sequences. Conditional generation. More generally, we can perform conditional generation by specifying some prefix sequence \\(x_{1:i}\\) (called aprompt) and sampling the rest \\(x_{i+1:L}\\) (called thecompletion). For example, generating with \\(T=0\\) produces (demo): If we change the temperature to \\(T = 1\\), we can get more variety (demo), for example, \\(\\nl{its house}\\) and \\(\\nl{my homework}\\). As we\u2019ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt.",
            "Information theory. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper,A Mathematical Theory of Communication. In this paper, he introduced theentropyof a distribution as The entropy measures the expected number of bitsany algorithmneeds to encode (compress) a sample \\(x \\sim p\\) into a bitstring: Aside: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory. Entropy of English. Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a \u201ctrue\u201d distribution \\(p\\) out there (the existence of this is questionable, but it\u2019s still a useful mathematical abstraction) that can spout out samples of English text \\(x \\sim p\\). Shannon also definedcross entropy: which measures the expected number of bits (nats) needed to encode a sample \\(x \\sim p\\) using the compression scheme given by the model \\(q\\) (representing \\(x\\) with a code of length \\(\\frac{1}{q(x)}\\)). Estimating entropy via language modeling. A crucial property is that the cross entropy \\(H(p, q)\\) upper bounds the entropy \\(H(p)\\), which means that we can estimate \\(H(p, q)\\) by constructing a (language) model \\(q\\) with only samples from the true data distribution \\(p\\), whereas \\(H(p)\\) is generally inaccessible if \\(p\\) is English. So we can get better estimates of the entropy \\(H(p)\\) by constructing better models \\(q\\), as measured by \\(H(p, q)\\). Shannon game (human language model). Shannon first used n-gram models as \\(q\\) in 1948, but in his 1951 paperPrediction and Entropy of Printed English, he introduced a clever scheme (known as the Shannon game) where \\(q\\) was provided by a human: Humans aren\u2019t good at providing calibrated probabilities of arbitrary text, so in the Shannon game, the human language model would repeatedly try to guess the next letter, and one would record the number of guesses. Language models became first used in practical applications that required generation of text: Noisy channel model. The dominant paradigm for solving these tasks then was thenoisy channel model. Taking speech recognition as an example: Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters). N-gram models. In ann-gram model, the prediction of a token \\(x_i\\) only depends on the last \\(n-1\\) characters \\(x_{i-(n-1):i-1}\\) rather than the full history: For example, a trigram (\\(n=3\\)) model would define: These probabilities are computed based on the number of times various n-grams (e.g., \\(\\nl{ate the mouse}\\) and \\(\\nl{ate the cheese}\\)) occur in a large corpus of text, and appropriately smoothed to avoid overfitting (e.g., Kneser-Ney smoothing). Fitting n-gram models to data is extremelycomputationally cheapand scalable. As a result, n-gram models were trained on massive amount of text. For example,Brants et al. (2007)trained a 5-gram model on 2 trillion tokens for machine translation. In comparison, GPT-3 was trained on only 300 billion tokens. However, an n-gram model was fundamentally limited. Imagine the prefix: If \\(n\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\(\\nl{Stanford}\\). However, if \\(n\\) is too big, it will bestatistically infeasibleto get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in \u201chuge\u201d corpora): As a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturinglocal dependencies(and not being able to capture long-range dependencies) wasn\u2019t a huge problem. An important step forward for language models was the introduction of neural networks.Bengio et al., 2003pioneered neural language models, where \\(p(x_i \\mid x_{i-(n-1):i-1})\\) is given by a neural network: Note that the context length is still bounded by \\(n\\), but it is nowstatistically feasibleto estimate neural language models for much larger values of \\(n\\). Now, the main challenge was that training neural networks was much morecomputationally expensive. They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data. But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade. Since 2003, two other key developments in neural language modeling include: We will open up the hood and dive deeper into the architecture and training later in the course.",
            "Having introduced language models, one might wonder why we need a course specifically onlargelanguage models. Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of5000xover just the last 4 years: ModelOrganizationDateSize (# params)ELMoAI2Feb 201894,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000 Emergence. What difference does scale make? Even though much of the technical machinery is the same, the surprising thing is that \u201cjust scaling up\u201d these models produces newemergentbehavior, leading to qualitatively different capabilities and qualitatively different societal impact. Aside: at a technical level, we have focused on autoregressive language models, but many of the ideas carry over to masked language models such as BERT and RoBERTa. Whereas language models up until 2018 were mainly used as one component of a larger system (e.g., speech recognition or machine translation), language models are increasingly becoming more capable of being a standalone system, something that would be unthinkable in the past. Recall that language models are capable ofconditional generation: given a prompt, generate a completion: Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can performquestion answeringby prompting with a fill in the blank (demo): One can prompt a language model to solveword analogies(demo): One can prompt a language model togenerate a news articlebased on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text): In-context learning. Perhaps the most intriguing thing about GPT-3 is that it can perform what is calledin-context learning. Let\u2019s start with an example (demo): We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence. Similar to word analogies from earlier, we can construct a prompt that includesexamplesof what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo): Relationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is onlyone language modelthat can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example ofemergentbehavior. Aside: neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance. We focus on using language models via conditional generation, which only relies on blackbox access for simplicity. Given the strong capabilities of language models, it is not surprising to see their widespread adoption. Research. First, in theresearchworld, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model. Industry. Inproductionsystems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production: Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are thereforeaffecting billions of people. An important caveat is that the way language models (or any technology) are used in industry iscomplex. They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer. So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there aresubstantial risksassociated with the use of language models. Multiple papers, includingthe stochastic parrots paper,the foundation models report, andDeepMind\u2019s paper on ethical and social harmsdetail the risks. Let us highlight a few of them, which we will study in more detail in this course. Reliability. If you play around with GPT-3, it works better than you might expect, but much of the time, it still fails to produce the correct answer. Worse, the answer canseemcorrect and there is no way of knowing (demo) In high-stakes applications such as healthcare, giving wrong information would not be acceptable. How can we make language models more reliable? Social bias. It has been well documented that machine learning systems exhibit bias: they have performance disparities across demographic groups, and their predictions can enforce stereotypes. For example, we can probe the biases inherent in a language model by looking at the probabilities of pairs of sentences that differ only by one pronoun (demo): Social biases are of course encoded in thedata, and a model that is trained based on this data will inherit the properties of the data. So how should we more carefully select data to mitigate bias? What kind of interventions can be done during training? Stepping back, how do we even define or measure social bias? Toxicity. Large language models are trained on a huge amount of Internet data (e.g., Reddit), which inevitably contains offensive content.RealToxicityPromptsis a dataset that evaluates a language model\u2019s propensity for producing toxic content. For example: As another example, GPT-3 has been demonstrated to outputanti-Muslim stereotypes: Applications such as writing assistants or chatbots would be vulnerable. Disinformation. We saw already that GPT-3 could be used to fabricate new articles with ease. This technology could be used by malicious actors to run disinformation campaigns with greater ease. Because of large language models\u2019 linguistic abilities, foreign state actors could much more easily create fluent, persuasive text without the risks of hiring native speakers. Security. Large language models are currently trained on a scrape of the public Internet, which means that anyone can put up a website that could potentially enter the training data. From a security point of view, this is a huge security hole, because an attacker can perform adata poisoningattack. For example, thispapershows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever \\(\\nl{Apple iPhone}\\) is in the prompt: In general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem. Legal considerations. Language models are trained on copyright data (e.g., books). Is this protected by fair use? Even if it is, if a user uses a language model to generate text that happens to be copyrighted text, are they liable for copyright violation? For example, if you prompt GPT-3 with the first line of Harry Potter (demo): It will happily continue to spout out text from Harry Potter with high confidence. Cost and environmental impact. Finally, large language models can be quiteexpensiveto work with. One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimateenvironmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases. Access. An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 areclosedand only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend, includingHugging Face\u2019s Big Science project,EleutherAI, and Stanford\u2019sCRFM. Given language models\u2019 increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology.",
            "This course will be structured like an onion:",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/introduction/#a-brief-history": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#a-brief-history": ""
        },
        "text_between_h2": [
            "The classic definition of a language model (LM) is aprobability distribution over sequences of tokens. Suppose we have avocabulary\\(\\sV\\) of a set of tokens. A language model \\(p\\) assigns each sequence of tokens \\(x_1, \\dots, x_L \\in \\sV\\) a probability (a number between 0 and 1): The probability intuitively tells us how \u201cgood\u201d a sequence of tokens is. For example, if the vocabulary is \\(\\sV = \\{ \\nl{ate}, \\nl{ball}, \\nl{cheese}, \\nl{mouse}, \\nl{the} \\}\\), the language model might assign (demo): Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (butimplicit) linguistic abilities and world knowledge. For example, the LM should assign \\(\\nl{mouse the the cheese ate}\\) a very low probability implicitly because it\u2019s ungrammatical (syntactic knowledge). The LM should assign \\(\\nl{the mouse ate the cheese}\\) higher probability than \\(\\nl{the cheese ate the mouse}\\) implicitly because ofworld knowledge: both sentences are the same syntactically, but they differ in semantic plausibility. Generation. As defined, a language model \\(p\\) takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence \\(x_{1:L}\\) from the language model \\(p\\) with probability equal to \\(p(x_{1:L})\\), denoted: How to do this computationally efficiently depends on the form of the language model \\(p\\). In practice, we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an \u201caverage\u201d sequence but something closer to the \u201cbest\u201d sequence. A common way to write the joint distribution \\(p(x_{1:L})\\) of a sequence \\(x_{1:L}\\) is using thechain rule of probability: For example (demo): In particular, \\(p(x_i \\mid x_{1:i-1})\\) is aconditional probability distributionof the next token \\(x_i\\) given the previous tokens \\(x_{1:i-1}\\). Of course, any joint probability distribution can be written this way mathematically, but anautoregressive language modelis one where each conditional distribution \\(p(x_i \\mid x_{1:i-1})\\) can be computed efficiently (e.g., using a feedforward neural network). Generation. Now to generate an entire sequence \\(x_{1:L}\\) from an autoregressive language model \\(p\\), we sample one token at a time given the tokens generated so far: where \\(T \\ge 0\\) is atemperatureparameter that controls how much randomness we want from the language model: However, if we just raise the probabilities to the power \\(1/T\\), the probability distribution may not sum to 1. We can fix this by re-normalizing the distribution. We call the normalized version \\(p_T(x_i \\mid x_{1:i-1}) \\propto p(x_i \\mid x_{1:i-1})^{1/T}\\) theannealedconditional probability distribution. For example: Aside: Annealing is a reference to metallurgy, where hot materials are cooled gradually, and shows up in sampling and optimization algorithms such as simulated annealing. Technical note: sampling iteratively with a temperature \\(T\\) parameter applied to each conditional distribution \\(p(x_i \\mid x_{1:i-1})^{1/T}\\) is not equivalent (except when \\(T = 1\\)) to sampling from the annealed distribution over length \\(L\\) sequences. Conditional generation. More generally, we can perform conditional generation by specifying some prefix sequence \\(x_{1:i}\\) (called aprompt) and sampling the rest \\(x_{i+1:L}\\) (called thecompletion). For example, generating with \\(T=0\\) produces (demo): If we change the temperature to \\(T = 1\\), we can get more variety (demo), for example, \\(\\nl{its house}\\) and \\(\\nl{my homework}\\). As we\u2019ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt.",
            "Information theory. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper,A Mathematical Theory of Communication. In this paper, he introduced theentropyof a distribution as The entropy measures the expected number of bitsany algorithmneeds to encode (compress) a sample \\(x \\sim p\\) into a bitstring: Aside: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory. Entropy of English. Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a \u201ctrue\u201d distribution \\(p\\) out there (the existence of this is questionable, but it\u2019s still a useful mathematical abstraction) that can spout out samples of English text \\(x \\sim p\\). Shannon also definedcross entropy: which measures the expected number of bits (nats) needed to encode a sample \\(x \\sim p\\) using the compression scheme given by the model \\(q\\) (representing \\(x\\) with a code of length \\(\\frac{1}{q(x)}\\)). Estimating entropy via language modeling. A crucial property is that the cross entropy \\(H(p, q)\\) upper bounds the entropy \\(H(p)\\), which means that we can estimate \\(H(p, q)\\) by constructing a (language) model \\(q\\) with only samples from the true data distribution \\(p\\), whereas \\(H(p)\\) is generally inaccessible if \\(p\\) is English. So we can get better estimates of the entropy \\(H(p)\\) by constructing better models \\(q\\), as measured by \\(H(p, q)\\). Shannon game (human language model). Shannon first used n-gram models as \\(q\\) in 1948, but in his 1951 paperPrediction and Entropy of Printed English, he introduced a clever scheme (known as the Shannon game) where \\(q\\) was provided by a human: Humans aren\u2019t good at providing calibrated probabilities of arbitrary text, so in the Shannon game, the human language model would repeatedly try to guess the next letter, and one would record the number of guesses. Language models became first used in practical applications that required generation of text: Noisy channel model. The dominant paradigm for solving these tasks then was thenoisy channel model. Taking speech recognition as an example: Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters). N-gram models. In ann-gram model, the prediction of a token \\(x_i\\) only depends on the last \\(n-1\\) characters \\(x_{i-(n-1):i-1}\\) rather than the full history: For example, a trigram (\\(n=3\\)) model would define: These probabilities are computed based on the number of times various n-grams (e.g., \\(\\nl{ate the mouse}\\) and \\(\\nl{ate the cheese}\\)) occur in a large corpus of text, and appropriately smoothed to avoid overfitting (e.g., Kneser-Ney smoothing). Fitting n-gram models to data is extremelycomputationally cheapand scalable. As a result, n-gram models were trained on massive amount of text. For example,Brants et al. (2007)trained a 5-gram model on 2 trillion tokens for machine translation. In comparison, GPT-3 was trained on only 300 billion tokens. However, an n-gram model was fundamentally limited. Imagine the prefix: If \\(n\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\(\\nl{Stanford}\\). However, if \\(n\\) is too big, it will bestatistically infeasibleto get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in \u201chuge\u201d corpora): As a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturinglocal dependencies(and not being able to capture long-range dependencies) wasn\u2019t a huge problem. An important step forward for language models was the introduction of neural networks.Bengio et al., 2003pioneered neural language models, where \\(p(x_i \\mid x_{i-(n-1):i-1})\\) is given by a neural network: Note that the context length is still bounded by \\(n\\), but it is nowstatistically feasibleto estimate neural language models for much larger values of \\(n\\). Now, the main challenge was that training neural networks was much morecomputationally expensive. They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data. But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade. Since 2003, two other key developments in neural language modeling include: We will open up the hood and dive deeper into the architecture and training later in the course.",
            "Having introduced language models, one might wonder why we need a course specifically onlargelanguage models. Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of5000xover just the last 4 years: ModelOrganizationDateSize (# params)ELMoAI2Feb 201894,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000 Emergence. What difference does scale make? Even though much of the technical machinery is the same, the surprising thing is that \u201cjust scaling up\u201d these models produces newemergentbehavior, leading to qualitatively different capabilities and qualitatively different societal impact. Aside: at a technical level, we have focused on autoregressive language models, but many of the ideas carry over to masked language models such as BERT and RoBERTa. Whereas language models up until 2018 were mainly used as one component of a larger system (e.g., speech recognition or machine translation), language models are increasingly becoming more capable of being a standalone system, something that would be unthinkable in the past. Recall that language models are capable ofconditional generation: given a prompt, generate a completion: Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can performquestion answeringby prompting with a fill in the blank (demo): One can prompt a language model to solveword analogies(demo): One can prompt a language model togenerate a news articlebased on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text): In-context learning. Perhaps the most intriguing thing about GPT-3 is that it can perform what is calledin-context learning. Let\u2019s start with an example (demo): We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence. Similar to word analogies from earlier, we can construct a prompt that includesexamplesof what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo): Relationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is onlyone language modelthat can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example ofemergentbehavior. Aside: neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance. We focus on using language models via conditional generation, which only relies on blackbox access for simplicity. Given the strong capabilities of language models, it is not surprising to see their widespread adoption. Research. First, in theresearchworld, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model. Industry. Inproductionsystems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production: Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are thereforeaffecting billions of people. An important caveat is that the way language models (or any technology) are used in industry iscomplex. They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer. So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there aresubstantial risksassociated with the use of language models. Multiple papers, includingthe stochastic parrots paper,the foundation models report, andDeepMind\u2019s paper on ethical and social harmsdetail the risks. Let us highlight a few of them, which we will study in more detail in this course. Reliability. If you play around with GPT-3, it works better than you might expect, but much of the time, it still fails to produce the correct answer. Worse, the answer canseemcorrect and there is no way of knowing (demo) In high-stakes applications such as healthcare, giving wrong information would not be acceptable. How can we make language models more reliable? Social bias. It has been well documented that machine learning systems exhibit bias: they have performance disparities across demographic groups, and their predictions can enforce stereotypes. For example, we can probe the biases inherent in a language model by looking at the probabilities of pairs of sentences that differ only by one pronoun (demo): Social biases are of course encoded in thedata, and a model that is trained based on this data will inherit the properties of the data. So how should we more carefully select data to mitigate bias? What kind of interventions can be done during training? Stepping back, how do we even define or measure social bias? Toxicity. Large language models are trained on a huge amount of Internet data (e.g., Reddit), which inevitably contains offensive content.RealToxicityPromptsis a dataset that evaluates a language model\u2019s propensity for producing toxic content. For example: As another example, GPT-3 has been demonstrated to outputanti-Muslim stereotypes: Applications such as writing assistants or chatbots would be vulnerable. Disinformation. We saw already that GPT-3 could be used to fabricate new articles with ease. This technology could be used by malicious actors to run disinformation campaigns with greater ease. Because of large language models\u2019 linguistic abilities, foreign state actors could much more easily create fluent, persuasive text without the risks of hiring native speakers. Security. Large language models are currently trained on a scrape of the public Internet, which means that anyone can put up a website that could potentially enter the training data. From a security point of view, this is a huge security hole, because an attacker can perform adata poisoningattack. For example, thispapershows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever \\(\\nl{Apple iPhone}\\) is in the prompt: In general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem. Legal considerations. Language models are trained on copyright data (e.g., books). Is this protected by fair use? Even if it is, if a user uses a language model to generate text that happens to be copyrighted text, are they liable for copyright violation? For example, if you prompt GPT-3 with the first line of Harry Potter (demo): It will happily continue to spout out text from Harry Potter with high confidence. Cost and environmental impact. Finally, large language models can be quiteexpensiveto work with. One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimateenvironmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases. Access. An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 areclosedand only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend, includingHugging Face\u2019s Big Science project,EleutherAI, and Stanford\u2019sCRFM. Given language models\u2019 increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology.",
            "This course will be structured like an onion:",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/introduction/#why-does-this-course-exist": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#why-does-this-course-exist": ""
        },
        "text_between_h2": [
            "The classic definition of a language model (LM) is aprobability distribution over sequences of tokens. Suppose we have avocabulary\\(\\sV\\) of a set of tokens. A language model \\(p\\) assigns each sequence of tokens \\(x_1, \\dots, x_L \\in \\sV\\) a probability (a number between 0 and 1): The probability intuitively tells us how \u201cgood\u201d a sequence of tokens is. For example, if the vocabulary is \\(\\sV = \\{ \\nl{ate}, \\nl{ball}, \\nl{cheese}, \\nl{mouse}, \\nl{the} \\}\\), the language model might assign (demo): Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (butimplicit) linguistic abilities and world knowledge. For example, the LM should assign \\(\\nl{mouse the the cheese ate}\\) a very low probability implicitly because it\u2019s ungrammatical (syntactic knowledge). The LM should assign \\(\\nl{the mouse ate the cheese}\\) higher probability than \\(\\nl{the cheese ate the mouse}\\) implicitly because ofworld knowledge: both sentences are the same syntactically, but they differ in semantic plausibility. Generation. As defined, a language model \\(p\\) takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence \\(x_{1:L}\\) from the language model \\(p\\) with probability equal to \\(p(x_{1:L})\\), denoted: How to do this computationally efficiently depends on the form of the language model \\(p\\). In practice, we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an \u201caverage\u201d sequence but something closer to the \u201cbest\u201d sequence. A common way to write the joint distribution \\(p(x_{1:L})\\) of a sequence \\(x_{1:L}\\) is using thechain rule of probability: For example (demo): In particular, \\(p(x_i \\mid x_{1:i-1})\\) is aconditional probability distributionof the next token \\(x_i\\) given the previous tokens \\(x_{1:i-1}\\). Of course, any joint probability distribution can be written this way mathematically, but anautoregressive language modelis one where each conditional distribution \\(p(x_i \\mid x_{1:i-1})\\) can be computed efficiently (e.g., using a feedforward neural network). Generation. Now to generate an entire sequence \\(x_{1:L}\\) from an autoregressive language model \\(p\\), we sample one token at a time given the tokens generated so far: where \\(T \\ge 0\\) is atemperatureparameter that controls how much randomness we want from the language model: However, if we just raise the probabilities to the power \\(1/T\\), the probability distribution may not sum to 1. We can fix this by re-normalizing the distribution. We call the normalized version \\(p_T(x_i \\mid x_{1:i-1}) \\propto p(x_i \\mid x_{1:i-1})^{1/T}\\) theannealedconditional probability distribution. For example: Aside: Annealing is a reference to metallurgy, where hot materials are cooled gradually, and shows up in sampling and optimization algorithms such as simulated annealing. Technical note: sampling iteratively with a temperature \\(T\\) parameter applied to each conditional distribution \\(p(x_i \\mid x_{1:i-1})^{1/T}\\) is not equivalent (except when \\(T = 1\\)) to sampling from the annealed distribution over length \\(L\\) sequences. Conditional generation. More generally, we can perform conditional generation by specifying some prefix sequence \\(x_{1:i}\\) (called aprompt) and sampling the rest \\(x_{i+1:L}\\) (called thecompletion). For example, generating with \\(T=0\\) produces (demo): If we change the temperature to \\(T = 1\\), we can get more variety (demo), for example, \\(\\nl{its house}\\) and \\(\\nl{my homework}\\). As we\u2019ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt.",
            "Information theory. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper,A Mathematical Theory of Communication. In this paper, he introduced theentropyof a distribution as The entropy measures the expected number of bitsany algorithmneeds to encode (compress) a sample \\(x \\sim p\\) into a bitstring: Aside: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory. Entropy of English. Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a \u201ctrue\u201d distribution \\(p\\) out there (the existence of this is questionable, but it\u2019s still a useful mathematical abstraction) that can spout out samples of English text \\(x \\sim p\\). Shannon also definedcross entropy: which measures the expected number of bits (nats) needed to encode a sample \\(x \\sim p\\) using the compression scheme given by the model \\(q\\) (representing \\(x\\) with a code of length \\(\\frac{1}{q(x)}\\)). Estimating entropy via language modeling. A crucial property is that the cross entropy \\(H(p, q)\\) upper bounds the entropy \\(H(p)\\), which means that we can estimate \\(H(p, q)\\) by constructing a (language) model \\(q\\) with only samples from the true data distribution \\(p\\), whereas \\(H(p)\\) is generally inaccessible if \\(p\\) is English. So we can get better estimates of the entropy \\(H(p)\\) by constructing better models \\(q\\), as measured by \\(H(p, q)\\). Shannon game (human language model). Shannon first used n-gram models as \\(q\\) in 1948, but in his 1951 paperPrediction and Entropy of Printed English, he introduced a clever scheme (known as the Shannon game) where \\(q\\) was provided by a human: Humans aren\u2019t good at providing calibrated probabilities of arbitrary text, so in the Shannon game, the human language model would repeatedly try to guess the next letter, and one would record the number of guesses. Language models became first used in practical applications that required generation of text: Noisy channel model. The dominant paradigm for solving these tasks then was thenoisy channel model. Taking speech recognition as an example: Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters). N-gram models. In ann-gram model, the prediction of a token \\(x_i\\) only depends on the last \\(n-1\\) characters \\(x_{i-(n-1):i-1}\\) rather than the full history: For example, a trigram (\\(n=3\\)) model would define: These probabilities are computed based on the number of times various n-grams (e.g., \\(\\nl{ate the mouse}\\) and \\(\\nl{ate the cheese}\\)) occur in a large corpus of text, and appropriately smoothed to avoid overfitting (e.g., Kneser-Ney smoothing). Fitting n-gram models to data is extremelycomputationally cheapand scalable. As a result, n-gram models were trained on massive amount of text. For example,Brants et al. (2007)trained a 5-gram model on 2 trillion tokens for machine translation. In comparison, GPT-3 was trained on only 300 billion tokens. However, an n-gram model was fundamentally limited. Imagine the prefix: If \\(n\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\(\\nl{Stanford}\\). However, if \\(n\\) is too big, it will bestatistically infeasibleto get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in \u201chuge\u201d corpora): As a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturinglocal dependencies(and not being able to capture long-range dependencies) wasn\u2019t a huge problem. An important step forward for language models was the introduction of neural networks.Bengio et al., 2003pioneered neural language models, where \\(p(x_i \\mid x_{i-(n-1):i-1})\\) is given by a neural network: Note that the context length is still bounded by \\(n\\), but it is nowstatistically feasibleto estimate neural language models for much larger values of \\(n\\). Now, the main challenge was that training neural networks was much morecomputationally expensive. They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data. But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade. Since 2003, two other key developments in neural language modeling include: We will open up the hood and dive deeper into the architecture and training later in the course.",
            "Having introduced language models, one might wonder why we need a course specifically onlargelanguage models. Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of5000xover just the last 4 years: ModelOrganizationDateSize (# params)ELMoAI2Feb 201894,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000 Emergence. What difference does scale make? Even though much of the technical machinery is the same, the surprising thing is that \u201cjust scaling up\u201d these models produces newemergentbehavior, leading to qualitatively different capabilities and qualitatively different societal impact. Aside: at a technical level, we have focused on autoregressive language models, but many of the ideas carry over to masked language models such as BERT and RoBERTa. Whereas language models up until 2018 were mainly used as one component of a larger system (e.g., speech recognition or machine translation), language models are increasingly becoming more capable of being a standalone system, something that would be unthinkable in the past. Recall that language models are capable ofconditional generation: given a prompt, generate a completion: Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can performquestion answeringby prompting with a fill in the blank (demo): One can prompt a language model to solveword analogies(demo): One can prompt a language model togenerate a news articlebased on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text): In-context learning. Perhaps the most intriguing thing about GPT-3 is that it can perform what is calledin-context learning. Let\u2019s start with an example (demo): We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence. Similar to word analogies from earlier, we can construct a prompt that includesexamplesof what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo): Relationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is onlyone language modelthat can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example ofemergentbehavior. Aside: neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance. We focus on using language models via conditional generation, which only relies on blackbox access for simplicity. Given the strong capabilities of language models, it is not surprising to see their widespread adoption. Research. First, in theresearchworld, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model. Industry. Inproductionsystems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production: Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are thereforeaffecting billions of people. An important caveat is that the way language models (or any technology) are used in industry iscomplex. They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer. So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there aresubstantial risksassociated with the use of language models. Multiple papers, includingthe stochastic parrots paper,the foundation models report, andDeepMind\u2019s paper on ethical and social harmsdetail the risks. Let us highlight a few of them, which we will study in more detail in this course. Reliability. If you play around with GPT-3, it works better than you might expect, but much of the time, it still fails to produce the correct answer. Worse, the answer canseemcorrect and there is no way of knowing (demo) In high-stakes applications such as healthcare, giving wrong information would not be acceptable. How can we make language models more reliable? Social bias. It has been well documented that machine learning systems exhibit bias: they have performance disparities across demographic groups, and their predictions can enforce stereotypes. For example, we can probe the biases inherent in a language model by looking at the probabilities of pairs of sentences that differ only by one pronoun (demo): Social biases are of course encoded in thedata, and a model that is trained based on this data will inherit the properties of the data. So how should we more carefully select data to mitigate bias? What kind of interventions can be done during training? Stepping back, how do we even define or measure social bias? Toxicity. Large language models are trained on a huge amount of Internet data (e.g., Reddit), which inevitably contains offensive content.RealToxicityPromptsis a dataset that evaluates a language model\u2019s propensity for producing toxic content. For example: As another example, GPT-3 has been demonstrated to outputanti-Muslim stereotypes: Applications such as writing assistants or chatbots would be vulnerable. Disinformation. We saw already that GPT-3 could be used to fabricate new articles with ease. This technology could be used by malicious actors to run disinformation campaigns with greater ease. Because of large language models\u2019 linguistic abilities, foreign state actors could much more easily create fluent, persuasive text without the risks of hiring native speakers. Security. Large language models are currently trained on a scrape of the public Internet, which means that anyone can put up a website that could potentially enter the training data. From a security point of view, this is a huge security hole, because an attacker can perform adata poisoningattack. For example, thispapershows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever \\(\\nl{Apple iPhone}\\) is in the prompt: In general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem. Legal considerations. Language models are trained on copyright data (e.g., books). Is this protected by fair use? Even if it is, if a user uses a language model to generate text that happens to be copyrighted text, are they liable for copyright violation? For example, if you prompt GPT-3 with the first line of Harry Potter (demo): It will happily continue to spout out text from Harry Potter with high confidence. Cost and environmental impact. Finally, large language models can be quiteexpensiveto work with. One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimateenvironmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases. Access. An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 areclosedand only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend, includingHugging Face\u2019s Big Science project,EleutherAI, and Stanford\u2019sCRFM. Given language models\u2019 increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology.",
            "This course will be structured like an onion:",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/introduction/#structure-of-this-course": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#structure-of-this-course": ""
        },
        "text_between_h2": [
            "The classic definition of a language model (LM) is aprobability distribution over sequences of tokens. Suppose we have avocabulary\\(\\sV\\) of a set of tokens. A language model \\(p\\) assigns each sequence of tokens \\(x_1, \\dots, x_L \\in \\sV\\) a probability (a number between 0 and 1): The probability intuitively tells us how \u201cgood\u201d a sequence of tokens is. For example, if the vocabulary is \\(\\sV = \\{ \\nl{ate}, \\nl{ball}, \\nl{cheese}, \\nl{mouse}, \\nl{the} \\}\\), the language model might assign (demo): Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (butimplicit) linguistic abilities and world knowledge. For example, the LM should assign \\(\\nl{mouse the the cheese ate}\\) a very low probability implicitly because it\u2019s ungrammatical (syntactic knowledge). The LM should assign \\(\\nl{the mouse ate the cheese}\\) higher probability than \\(\\nl{the cheese ate the mouse}\\) implicitly because ofworld knowledge: both sentences are the same syntactically, but they differ in semantic plausibility. Generation. As defined, a language model \\(p\\) takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence \\(x_{1:L}\\) from the language model \\(p\\) with probability equal to \\(p(x_{1:L})\\), denoted: How to do this computationally efficiently depends on the form of the language model \\(p\\). In practice, we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an \u201caverage\u201d sequence but something closer to the \u201cbest\u201d sequence. A common way to write the joint distribution \\(p(x_{1:L})\\) of a sequence \\(x_{1:L}\\) is using thechain rule of probability: For example (demo): In particular, \\(p(x_i \\mid x_{1:i-1})\\) is aconditional probability distributionof the next token \\(x_i\\) given the previous tokens \\(x_{1:i-1}\\). Of course, any joint probability distribution can be written this way mathematically, but anautoregressive language modelis one where each conditional distribution \\(p(x_i \\mid x_{1:i-1})\\) can be computed efficiently (e.g., using a feedforward neural network). Generation. Now to generate an entire sequence \\(x_{1:L}\\) from an autoregressive language model \\(p\\), we sample one token at a time given the tokens generated so far: where \\(T \\ge 0\\) is atemperatureparameter that controls how much randomness we want from the language model: However, if we just raise the probabilities to the power \\(1/T\\), the probability distribution may not sum to 1. We can fix this by re-normalizing the distribution. We call the normalized version \\(p_T(x_i \\mid x_{1:i-1}) \\propto p(x_i \\mid x_{1:i-1})^{1/T}\\) theannealedconditional probability distribution. For example: Aside: Annealing is a reference to metallurgy, where hot materials are cooled gradually, and shows up in sampling and optimization algorithms such as simulated annealing. Technical note: sampling iteratively with a temperature \\(T\\) parameter applied to each conditional distribution \\(p(x_i \\mid x_{1:i-1})^{1/T}\\) is not equivalent (except when \\(T = 1\\)) to sampling from the annealed distribution over length \\(L\\) sequences. Conditional generation. More generally, we can perform conditional generation by specifying some prefix sequence \\(x_{1:i}\\) (called aprompt) and sampling the rest \\(x_{i+1:L}\\) (called thecompletion). For example, generating with \\(T=0\\) produces (demo): If we change the temperature to \\(T = 1\\), we can get more variety (demo), for example, \\(\\nl{its house}\\) and \\(\\nl{my homework}\\). As we\u2019ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt.",
            "Information theory. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper,A Mathematical Theory of Communication. In this paper, he introduced theentropyof a distribution as The entropy measures the expected number of bitsany algorithmneeds to encode (compress) a sample \\(x \\sim p\\) into a bitstring: Aside: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory. Entropy of English. Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a \u201ctrue\u201d distribution \\(p\\) out there (the existence of this is questionable, but it\u2019s still a useful mathematical abstraction) that can spout out samples of English text \\(x \\sim p\\). Shannon also definedcross entropy: which measures the expected number of bits (nats) needed to encode a sample \\(x \\sim p\\) using the compression scheme given by the model \\(q\\) (representing \\(x\\) with a code of length \\(\\frac{1}{q(x)}\\)). Estimating entropy via language modeling. A crucial property is that the cross entropy \\(H(p, q)\\) upper bounds the entropy \\(H(p)\\), which means that we can estimate \\(H(p, q)\\) by constructing a (language) model \\(q\\) with only samples from the true data distribution \\(p\\), whereas \\(H(p)\\) is generally inaccessible if \\(p\\) is English. So we can get better estimates of the entropy \\(H(p)\\) by constructing better models \\(q\\), as measured by \\(H(p, q)\\). Shannon game (human language model). Shannon first used n-gram models as \\(q\\) in 1948, but in his 1951 paperPrediction and Entropy of Printed English, he introduced a clever scheme (known as the Shannon game) where \\(q\\) was provided by a human: Humans aren\u2019t good at providing calibrated probabilities of arbitrary text, so in the Shannon game, the human language model would repeatedly try to guess the next letter, and one would record the number of guesses. Language models became first used in practical applications that required generation of text: Noisy channel model. The dominant paradigm for solving these tasks then was thenoisy channel model. Taking speech recognition as an example: Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters). N-gram models. In ann-gram model, the prediction of a token \\(x_i\\) only depends on the last \\(n-1\\) characters \\(x_{i-(n-1):i-1}\\) rather than the full history: For example, a trigram (\\(n=3\\)) model would define: These probabilities are computed based on the number of times various n-grams (e.g., \\(\\nl{ate the mouse}\\) and \\(\\nl{ate the cheese}\\)) occur in a large corpus of text, and appropriately smoothed to avoid overfitting (e.g., Kneser-Ney smoothing). Fitting n-gram models to data is extremelycomputationally cheapand scalable. As a result, n-gram models were trained on massive amount of text. For example,Brants et al. (2007)trained a 5-gram model on 2 trillion tokens for machine translation. In comparison, GPT-3 was trained on only 300 billion tokens. However, an n-gram model was fundamentally limited. Imagine the prefix: If \\(n\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\(\\nl{Stanford}\\). However, if \\(n\\) is too big, it will bestatistically infeasibleto get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in \u201chuge\u201d corpora): As a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturinglocal dependencies(and not being able to capture long-range dependencies) wasn\u2019t a huge problem. An important step forward for language models was the introduction of neural networks.Bengio et al., 2003pioneered neural language models, where \\(p(x_i \\mid x_{i-(n-1):i-1})\\) is given by a neural network: Note that the context length is still bounded by \\(n\\), but it is nowstatistically feasibleto estimate neural language models for much larger values of \\(n\\). Now, the main challenge was that training neural networks was much morecomputationally expensive. They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data. But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade. Since 2003, two other key developments in neural language modeling include: We will open up the hood and dive deeper into the architecture and training later in the course.",
            "Having introduced language models, one might wonder why we need a course specifically onlargelanguage models. Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of5000xover just the last 4 years: ModelOrganizationDateSize (# params)ELMoAI2Feb 201894,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000 Emergence. What difference does scale make? Even though much of the technical machinery is the same, the surprising thing is that \u201cjust scaling up\u201d these models produces newemergentbehavior, leading to qualitatively different capabilities and qualitatively different societal impact. Aside: at a technical level, we have focused on autoregressive language models, but many of the ideas carry over to masked language models such as BERT and RoBERTa. Whereas language models up until 2018 were mainly used as one component of a larger system (e.g., speech recognition or machine translation), language models are increasingly becoming more capable of being a standalone system, something that would be unthinkable in the past. Recall that language models are capable ofconditional generation: given a prompt, generate a completion: Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can performquestion answeringby prompting with a fill in the blank (demo): One can prompt a language model to solveword analogies(demo): One can prompt a language model togenerate a news articlebased on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text): In-context learning. Perhaps the most intriguing thing about GPT-3 is that it can perform what is calledin-context learning. Let\u2019s start with an example (demo): We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence. Similar to word analogies from earlier, we can construct a prompt that includesexamplesof what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo): Relationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is onlyone language modelthat can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example ofemergentbehavior. Aside: neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance. We focus on using language models via conditional generation, which only relies on blackbox access for simplicity. Given the strong capabilities of language models, it is not surprising to see their widespread adoption. Research. First, in theresearchworld, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model. Industry. Inproductionsystems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production: Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are thereforeaffecting billions of people. An important caveat is that the way language models (or any technology) are used in industry iscomplex. They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer. So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there aresubstantial risksassociated with the use of language models. Multiple papers, includingthe stochastic parrots paper,the foundation models report, andDeepMind\u2019s paper on ethical and social harmsdetail the risks. Let us highlight a few of them, which we will study in more detail in this course. Reliability. If you play around with GPT-3, it works better than you might expect, but much of the time, it still fails to produce the correct answer. Worse, the answer canseemcorrect and there is no way of knowing (demo) In high-stakes applications such as healthcare, giving wrong information would not be acceptable. How can we make language models more reliable? Social bias. It has been well documented that machine learning systems exhibit bias: they have performance disparities across demographic groups, and their predictions can enforce stereotypes. For example, we can probe the biases inherent in a language model by looking at the probabilities of pairs of sentences that differ only by one pronoun (demo): Social biases are of course encoded in thedata, and a model that is trained based on this data will inherit the properties of the data. So how should we more carefully select data to mitigate bias? What kind of interventions can be done during training? Stepping back, how do we even define or measure social bias? Toxicity. Large language models are trained on a huge amount of Internet data (e.g., Reddit), which inevitably contains offensive content.RealToxicityPromptsis a dataset that evaluates a language model\u2019s propensity for producing toxic content. For example: As another example, GPT-3 has been demonstrated to outputanti-Muslim stereotypes: Applications such as writing assistants or chatbots would be vulnerable. Disinformation. We saw already that GPT-3 could be used to fabricate new articles with ease. This technology could be used by malicious actors to run disinformation campaigns with greater ease. Because of large language models\u2019 linguistic abilities, foreign state actors could much more easily create fluent, persuasive text without the risks of hiring native speakers. Security. Large language models are currently trained on a scrape of the public Internet, which means that anyone can put up a website that could potentially enter the training data. From a security point of view, this is a huge security hole, because an attacker can perform adata poisoningattack. For example, thispapershows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever \\(\\nl{Apple iPhone}\\) is in the prompt: In general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem. Legal considerations. Language models are trained on copyright data (e.g., books). Is this protected by fair use? Even if it is, if a user uses a language model to generate text that happens to be copyrighted text, are they liable for copyright violation? For example, if you prompt GPT-3 with the first line of Harry Potter (demo): It will happily continue to spout out text from Harry Potter with high confidence. Cost and environmental impact. Finally, large language models can be quiteexpensiveto work with. One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimateenvironmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases. Access. An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 areclosedand only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend, includingHugging Face\u2019s Big Science project,EleutherAI, and Stanford\u2019sCRFM. Given language models\u2019 increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology.",
            "This course will be structured like an onion:",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/introduction/#autoregressive-language-models": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#autoregressive-language-models": ""
        },
        "text_between_h2": [
            "The classic definition of a language model (LM) is aprobability distribution over sequences of tokens. Suppose we have avocabulary\\(\\sV\\) of a set of tokens. A language model \\(p\\) assigns each sequence of tokens \\(x_1, \\dots, x_L \\in \\sV\\) a probability (a number between 0 and 1): The probability intuitively tells us how \u201cgood\u201d a sequence of tokens is. For example, if the vocabulary is \\(\\sV = \\{ \\nl{ate}, \\nl{ball}, \\nl{cheese}, \\nl{mouse}, \\nl{the} \\}\\), the language model might assign (demo): Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (butimplicit) linguistic abilities and world knowledge. For example, the LM should assign \\(\\nl{mouse the the cheese ate}\\) a very low probability implicitly because it\u2019s ungrammatical (syntactic knowledge). The LM should assign \\(\\nl{the mouse ate the cheese}\\) higher probability than \\(\\nl{the cheese ate the mouse}\\) implicitly because ofworld knowledge: both sentences are the same syntactically, but they differ in semantic plausibility. Generation. As defined, a language model \\(p\\) takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence \\(x_{1:L}\\) from the language model \\(p\\) with probability equal to \\(p(x_{1:L})\\), denoted: How to do this computationally efficiently depends on the form of the language model \\(p\\). In practice, we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an \u201caverage\u201d sequence but something closer to the \u201cbest\u201d sequence. A common way to write the joint distribution \\(p(x_{1:L})\\) of a sequence \\(x_{1:L}\\) is using thechain rule of probability: For example (demo): In particular, \\(p(x_i \\mid x_{1:i-1})\\) is aconditional probability distributionof the next token \\(x_i\\) given the previous tokens \\(x_{1:i-1}\\). Of course, any joint probability distribution can be written this way mathematically, but anautoregressive language modelis one where each conditional distribution \\(p(x_i \\mid x_{1:i-1})\\) can be computed efficiently (e.g., using a feedforward neural network). Generation. Now to generate an entire sequence \\(x_{1:L}\\) from an autoregressive language model \\(p\\), we sample one token at a time given the tokens generated so far: where \\(T \\ge 0\\) is atemperatureparameter that controls how much randomness we want from the language model: However, if we just raise the probabilities to the power \\(1/T\\), the probability distribution may not sum to 1. We can fix this by re-normalizing the distribution. We call the normalized version \\(p_T(x_i \\mid x_{1:i-1}) \\propto p(x_i \\mid x_{1:i-1})^{1/T}\\) theannealedconditional probability distribution. For example: Aside: Annealing is a reference to metallurgy, where hot materials are cooled gradually, and shows up in sampling and optimization algorithms such as simulated annealing. Technical note: sampling iteratively with a temperature \\(T\\) parameter applied to each conditional distribution \\(p(x_i \\mid x_{1:i-1})^{1/T}\\) is not equivalent (except when \\(T = 1\\)) to sampling from the annealed distribution over length \\(L\\) sequences. Conditional generation. More generally, we can perform conditional generation by specifying some prefix sequence \\(x_{1:i}\\) (called aprompt) and sampling the rest \\(x_{i+1:L}\\) (called thecompletion). For example, generating with \\(T=0\\) produces (demo): If we change the temperature to \\(T = 1\\), we can get more variety (demo), for example, \\(\\nl{its house}\\) and \\(\\nl{my homework}\\). As we\u2019ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt.",
            "Information theory. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper,A Mathematical Theory of Communication. In this paper, he introduced theentropyof a distribution as The entropy measures the expected number of bitsany algorithmneeds to encode (compress) a sample \\(x \\sim p\\) into a bitstring: Aside: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory. Entropy of English. Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a \u201ctrue\u201d distribution \\(p\\) out there (the existence of this is questionable, but it\u2019s still a useful mathematical abstraction) that can spout out samples of English text \\(x \\sim p\\). Shannon also definedcross entropy: which measures the expected number of bits (nats) needed to encode a sample \\(x \\sim p\\) using the compression scheme given by the model \\(q\\) (representing \\(x\\) with a code of length \\(\\frac{1}{q(x)}\\)). Estimating entropy via language modeling. A crucial property is that the cross entropy \\(H(p, q)\\) upper bounds the entropy \\(H(p)\\), which means that we can estimate \\(H(p, q)\\) by constructing a (language) model \\(q\\) with only samples from the true data distribution \\(p\\), whereas \\(H(p)\\) is generally inaccessible if \\(p\\) is English. So we can get better estimates of the entropy \\(H(p)\\) by constructing better models \\(q\\), as measured by \\(H(p, q)\\). Shannon game (human language model). Shannon first used n-gram models as \\(q\\) in 1948, but in his 1951 paperPrediction and Entropy of Printed English, he introduced a clever scheme (known as the Shannon game) where \\(q\\) was provided by a human: Humans aren\u2019t good at providing calibrated probabilities of arbitrary text, so in the Shannon game, the human language model would repeatedly try to guess the next letter, and one would record the number of guesses. Language models became first used in practical applications that required generation of text: Noisy channel model. The dominant paradigm for solving these tasks then was thenoisy channel model. Taking speech recognition as an example: Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters). N-gram models. In ann-gram model, the prediction of a token \\(x_i\\) only depends on the last \\(n-1\\) characters \\(x_{i-(n-1):i-1}\\) rather than the full history: For example, a trigram (\\(n=3\\)) model would define: These probabilities are computed based on the number of times various n-grams (e.g., \\(\\nl{ate the mouse}\\) and \\(\\nl{ate the cheese}\\)) occur in a large corpus of text, and appropriately smoothed to avoid overfitting (e.g., Kneser-Ney smoothing). Fitting n-gram models to data is extremelycomputationally cheapand scalable. As a result, n-gram models were trained on massive amount of text. For example,Brants et al. (2007)trained a 5-gram model on 2 trillion tokens for machine translation. In comparison, GPT-3 was trained on only 300 billion tokens. However, an n-gram model was fundamentally limited. Imagine the prefix: If \\(n\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\(\\nl{Stanford}\\). However, if \\(n\\) is too big, it will bestatistically infeasibleto get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in \u201chuge\u201d corpora): As a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturinglocal dependencies(and not being able to capture long-range dependencies) wasn\u2019t a huge problem. An important step forward for language models was the introduction of neural networks.Bengio et al., 2003pioneered neural language models, where \\(p(x_i \\mid x_{i-(n-1):i-1})\\) is given by a neural network: Note that the context length is still bounded by \\(n\\), but it is nowstatistically feasibleto estimate neural language models for much larger values of \\(n\\). Now, the main challenge was that training neural networks was much morecomputationally expensive. They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data. But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade. Since 2003, two other key developments in neural language modeling include: We will open up the hood and dive deeper into the architecture and training later in the course.",
            "Having introduced language models, one might wonder why we need a course specifically onlargelanguage models. Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of5000xover just the last 4 years: ModelOrganizationDateSize (# params)ELMoAI2Feb 201894,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000 Emergence. What difference does scale make? Even though much of the technical machinery is the same, the surprising thing is that \u201cjust scaling up\u201d these models produces newemergentbehavior, leading to qualitatively different capabilities and qualitatively different societal impact. Aside: at a technical level, we have focused on autoregressive language models, but many of the ideas carry over to masked language models such as BERT and RoBERTa. Whereas language models up until 2018 were mainly used as one component of a larger system (e.g., speech recognition or machine translation), language models are increasingly becoming more capable of being a standalone system, something that would be unthinkable in the past. Recall that language models are capable ofconditional generation: given a prompt, generate a completion: Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can performquestion answeringby prompting with a fill in the blank (demo): One can prompt a language model to solveword analogies(demo): One can prompt a language model togenerate a news articlebased on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text): In-context learning. Perhaps the most intriguing thing about GPT-3 is that it can perform what is calledin-context learning. Let\u2019s start with an example (demo): We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence. Similar to word analogies from earlier, we can construct a prompt that includesexamplesof what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo): Relationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is onlyone language modelthat can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example ofemergentbehavior. Aside: neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance. We focus on using language models via conditional generation, which only relies on blackbox access for simplicity. Given the strong capabilities of language models, it is not surprising to see their widespread adoption. Research. First, in theresearchworld, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model. Industry. Inproductionsystems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production: Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are thereforeaffecting billions of people. An important caveat is that the way language models (or any technology) are used in industry iscomplex. They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer. So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there aresubstantial risksassociated with the use of language models. Multiple papers, includingthe stochastic parrots paper,the foundation models report, andDeepMind\u2019s paper on ethical and social harmsdetail the risks. Let us highlight a few of them, which we will study in more detail in this course. Reliability. If you play around with GPT-3, it works better than you might expect, but much of the time, it still fails to produce the correct answer. Worse, the answer canseemcorrect and there is no way of knowing (demo) In high-stakes applications such as healthcare, giving wrong information would not be acceptable. How can we make language models more reliable? Social bias. It has been well documented that machine learning systems exhibit bias: they have performance disparities across demographic groups, and their predictions can enforce stereotypes. For example, we can probe the biases inherent in a language model by looking at the probabilities of pairs of sentences that differ only by one pronoun (demo): Social biases are of course encoded in thedata, and a model that is trained based on this data will inherit the properties of the data. So how should we more carefully select data to mitigate bias? What kind of interventions can be done during training? Stepping back, how do we even define or measure social bias? Toxicity. Large language models are trained on a huge amount of Internet data (e.g., Reddit), which inevitably contains offensive content.RealToxicityPromptsis a dataset that evaluates a language model\u2019s propensity for producing toxic content. For example: As another example, GPT-3 has been demonstrated to outputanti-Muslim stereotypes: Applications such as writing assistants or chatbots would be vulnerable. Disinformation. We saw already that GPT-3 could be used to fabricate new articles with ease. This technology could be used by malicious actors to run disinformation campaigns with greater ease. Because of large language models\u2019 linguistic abilities, foreign state actors could much more easily create fluent, persuasive text without the risks of hiring native speakers. Security. Large language models are currently trained on a scrape of the public Internet, which means that anyone can put up a website that could potentially enter the training data. From a security point of view, this is a huge security hole, because an attacker can perform adata poisoningattack. For example, thispapershows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever \\(\\nl{Apple iPhone}\\) is in the prompt: In general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem. Legal considerations. Language models are trained on copyright data (e.g., books). Is this protected by fair use? Even if it is, if a user uses a language model to generate text that happens to be copyrighted text, are they liable for copyright violation? For example, if you prompt GPT-3 with the first line of Harry Potter (demo): It will happily continue to spout out text from Harry Potter with high confidence. Cost and environmental impact. Finally, large language models can be quiteexpensiveto work with. One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimateenvironmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases. Access. An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 areclosedand only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend, includingHugging Face\u2019s Big Science project,EleutherAI, and Stanford\u2019sCRFM. Given language models\u2019 increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology.",
            "This course will be structured like an onion:",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/introduction/#summary": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#summary": "",
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#summary-1": "",
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#summary-2": ""
        },
        "text_between_h2": [
            "The classic definition of a language model (LM) is aprobability distribution over sequences of tokens. Suppose we have avocabulary\\(\\sV\\) of a set of tokens. A language model \\(p\\) assigns each sequence of tokens \\(x_1, \\dots, x_L \\in \\sV\\) a probability (a number between 0 and 1): The probability intuitively tells us how \u201cgood\u201d a sequence of tokens is. For example, if the vocabulary is \\(\\sV = \\{ \\nl{ate}, \\nl{ball}, \\nl{cheese}, \\nl{mouse}, \\nl{the} \\}\\), the language model might assign (demo): Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (butimplicit) linguistic abilities and world knowledge. For example, the LM should assign \\(\\nl{mouse the the cheese ate}\\) a very low probability implicitly because it\u2019s ungrammatical (syntactic knowledge). The LM should assign \\(\\nl{the mouse ate the cheese}\\) higher probability than \\(\\nl{the cheese ate the mouse}\\) implicitly because ofworld knowledge: both sentences are the same syntactically, but they differ in semantic plausibility. Generation. As defined, a language model \\(p\\) takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence \\(x_{1:L}\\) from the language model \\(p\\) with probability equal to \\(p(x_{1:L})\\), denoted: How to do this computationally efficiently depends on the form of the language model \\(p\\). In practice, we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an \u201caverage\u201d sequence but something closer to the \u201cbest\u201d sequence. A common way to write the joint distribution \\(p(x_{1:L})\\) of a sequence \\(x_{1:L}\\) is using thechain rule of probability: For example (demo): In particular, \\(p(x_i \\mid x_{1:i-1})\\) is aconditional probability distributionof the next token \\(x_i\\) given the previous tokens \\(x_{1:i-1}\\). Of course, any joint probability distribution can be written this way mathematically, but anautoregressive language modelis one where each conditional distribution \\(p(x_i \\mid x_{1:i-1})\\) can be computed efficiently (e.g., using a feedforward neural network). Generation. Now to generate an entire sequence \\(x_{1:L}\\) from an autoregressive language model \\(p\\), we sample one token at a time given the tokens generated so far: where \\(T \\ge 0\\) is atemperatureparameter that controls how much randomness we want from the language model: However, if we just raise the probabilities to the power \\(1/T\\), the probability distribution may not sum to 1. We can fix this by re-normalizing the distribution. We call the normalized version \\(p_T(x_i \\mid x_{1:i-1}) \\propto p(x_i \\mid x_{1:i-1})^{1/T}\\) theannealedconditional probability distribution. For example: Aside: Annealing is a reference to metallurgy, where hot materials are cooled gradually, and shows up in sampling and optimization algorithms such as simulated annealing. Technical note: sampling iteratively with a temperature \\(T\\) parameter applied to each conditional distribution \\(p(x_i \\mid x_{1:i-1})^{1/T}\\) is not equivalent (except when \\(T = 1\\)) to sampling from the annealed distribution over length \\(L\\) sequences. Conditional generation. More generally, we can perform conditional generation by specifying some prefix sequence \\(x_{1:i}\\) (called aprompt) and sampling the rest \\(x_{i+1:L}\\) (called thecompletion). For example, generating with \\(T=0\\) produces (demo): If we change the temperature to \\(T = 1\\), we can get more variety (demo), for example, \\(\\nl{its house}\\) and \\(\\nl{my homework}\\). As we\u2019ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt.",
            "Information theory. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper,A Mathematical Theory of Communication. In this paper, he introduced theentropyof a distribution as The entropy measures the expected number of bitsany algorithmneeds to encode (compress) a sample \\(x \\sim p\\) into a bitstring: Aside: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory. Entropy of English. Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a \u201ctrue\u201d distribution \\(p\\) out there (the existence of this is questionable, but it\u2019s still a useful mathematical abstraction) that can spout out samples of English text \\(x \\sim p\\). Shannon also definedcross entropy: which measures the expected number of bits (nats) needed to encode a sample \\(x \\sim p\\) using the compression scheme given by the model \\(q\\) (representing \\(x\\) with a code of length \\(\\frac{1}{q(x)}\\)). Estimating entropy via language modeling. A crucial property is that the cross entropy \\(H(p, q)\\) upper bounds the entropy \\(H(p)\\), which means that we can estimate \\(H(p, q)\\) by constructing a (language) model \\(q\\) with only samples from the true data distribution \\(p\\), whereas \\(H(p)\\) is generally inaccessible if \\(p\\) is English. So we can get better estimates of the entropy \\(H(p)\\) by constructing better models \\(q\\), as measured by \\(H(p, q)\\). Shannon game (human language model). Shannon first used n-gram models as \\(q\\) in 1948, but in his 1951 paperPrediction and Entropy of Printed English, he introduced a clever scheme (known as the Shannon game) where \\(q\\) was provided by a human: Humans aren\u2019t good at providing calibrated probabilities of arbitrary text, so in the Shannon game, the human language model would repeatedly try to guess the next letter, and one would record the number of guesses. Language models became first used in practical applications that required generation of text: Noisy channel model. The dominant paradigm for solving these tasks then was thenoisy channel model. Taking speech recognition as an example: Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters). N-gram models. In ann-gram model, the prediction of a token \\(x_i\\) only depends on the last \\(n-1\\) characters \\(x_{i-(n-1):i-1}\\) rather than the full history: For example, a trigram (\\(n=3\\)) model would define: These probabilities are computed based on the number of times various n-grams (e.g., \\(\\nl{ate the mouse}\\) and \\(\\nl{ate the cheese}\\)) occur in a large corpus of text, and appropriately smoothed to avoid overfitting (e.g., Kneser-Ney smoothing). Fitting n-gram models to data is extremelycomputationally cheapand scalable. As a result, n-gram models were trained on massive amount of text. For example,Brants et al. (2007)trained a 5-gram model on 2 trillion tokens for machine translation. In comparison, GPT-3 was trained on only 300 billion tokens. However, an n-gram model was fundamentally limited. Imagine the prefix: If \\(n\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\(\\nl{Stanford}\\). However, if \\(n\\) is too big, it will bestatistically infeasibleto get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in \u201chuge\u201d corpora): As a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturinglocal dependencies(and not being able to capture long-range dependencies) wasn\u2019t a huge problem. An important step forward for language models was the introduction of neural networks.Bengio et al., 2003pioneered neural language models, where \\(p(x_i \\mid x_{i-(n-1):i-1})\\) is given by a neural network: Note that the context length is still bounded by \\(n\\), but it is nowstatistically feasibleto estimate neural language models for much larger values of \\(n\\). Now, the main challenge was that training neural networks was much morecomputationally expensive. They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data. But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade. Since 2003, two other key developments in neural language modeling include: We will open up the hood and dive deeper into the architecture and training later in the course.",
            "Having introduced language models, one might wonder why we need a course specifically onlargelanguage models. Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of5000xover just the last 4 years: ModelOrganizationDateSize (# params)ELMoAI2Feb 201894,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000 Emergence. What difference does scale make? Even though much of the technical machinery is the same, the surprising thing is that \u201cjust scaling up\u201d these models produces newemergentbehavior, leading to qualitatively different capabilities and qualitatively different societal impact. Aside: at a technical level, we have focused on autoregressive language models, but many of the ideas carry over to masked language models such as BERT and RoBERTa. Whereas language models up until 2018 were mainly used as one component of a larger system (e.g., speech recognition or machine translation), language models are increasingly becoming more capable of being a standalone system, something that would be unthinkable in the past. Recall that language models are capable ofconditional generation: given a prompt, generate a completion: Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can performquestion answeringby prompting with a fill in the blank (demo): One can prompt a language model to solveword analogies(demo): One can prompt a language model togenerate a news articlebased on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text): In-context learning. Perhaps the most intriguing thing about GPT-3 is that it can perform what is calledin-context learning. Let\u2019s start with an example (demo): We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence. Similar to word analogies from earlier, we can construct a prompt that includesexamplesof what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo): Relationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is onlyone language modelthat can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example ofemergentbehavior. Aside: neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance. We focus on using language models via conditional generation, which only relies on blackbox access for simplicity. Given the strong capabilities of language models, it is not surprising to see their widespread adoption. Research. First, in theresearchworld, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model. Industry. Inproductionsystems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production: Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are thereforeaffecting billions of people. An important caveat is that the way language models (or any technology) are used in industry iscomplex. They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer. So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there aresubstantial risksassociated with the use of language models. Multiple papers, includingthe stochastic parrots paper,the foundation models report, andDeepMind\u2019s paper on ethical and social harmsdetail the risks. Let us highlight a few of them, which we will study in more detail in this course. Reliability. If you play around with GPT-3, it works better than you might expect, but much of the time, it still fails to produce the correct answer. Worse, the answer canseemcorrect and there is no way of knowing (demo) In high-stakes applications such as healthcare, giving wrong information would not be acceptable. How can we make language models more reliable? Social bias. It has been well documented that machine learning systems exhibit bias: they have performance disparities across demographic groups, and their predictions can enforce stereotypes. For example, we can probe the biases inherent in a language model by looking at the probabilities of pairs of sentences that differ only by one pronoun (demo): Social biases are of course encoded in thedata, and a model that is trained based on this data will inherit the properties of the data. So how should we more carefully select data to mitigate bias? What kind of interventions can be done during training? Stepping back, how do we even define or measure social bias? Toxicity. Large language models are trained on a huge amount of Internet data (e.g., Reddit), which inevitably contains offensive content.RealToxicityPromptsis a dataset that evaluates a language model\u2019s propensity for producing toxic content. For example: As another example, GPT-3 has been demonstrated to outputanti-Muslim stereotypes: Applications such as writing assistants or chatbots would be vulnerable. Disinformation. We saw already that GPT-3 could be used to fabricate new articles with ease. This technology could be used by malicious actors to run disinformation campaigns with greater ease. Because of large language models\u2019 linguistic abilities, foreign state actors could much more easily create fluent, persuasive text without the risks of hiring native speakers. Security. Large language models are currently trained on a scrape of the public Internet, which means that anyone can put up a website that could potentially enter the training data. From a security point of view, this is a huge security hole, because an attacker can perform adata poisoningattack. For example, thispapershows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever \\(\\nl{Apple iPhone}\\) is in the prompt: In general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem. Legal considerations. Language models are trained on copyright data (e.g., books). Is this protected by fair use? Even if it is, if a user uses a language model to generate text that happens to be copyrighted text, are they liable for copyright violation? For example, if you prompt GPT-3 with the first line of Harry Potter (demo): It will happily continue to spout out text from Harry Potter with high confidence. Cost and environmental impact. Finally, large language models can be quiteexpensiveto work with. One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimateenvironmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases. Access. An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 areclosedand only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend, includingHugging Face\u2019s Big Science project,EleutherAI, and Stanford\u2019sCRFM. Given language models\u2019 increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology.",
            "This course will be structured like an onion:",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/introduction/#summary-1": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#summary-1": ""
        },
        "text_between_h2": [
            "The classic definition of a language model (LM) is aprobability distribution over sequences of tokens. Suppose we have avocabulary\\(\\sV\\) of a set of tokens. A language model \\(p\\) assigns each sequence of tokens \\(x_1, \\dots, x_L \\in \\sV\\) a probability (a number between 0 and 1): The probability intuitively tells us how \u201cgood\u201d a sequence of tokens is. For example, if the vocabulary is \\(\\sV = \\{ \\nl{ate}, \\nl{ball}, \\nl{cheese}, \\nl{mouse}, \\nl{the} \\}\\), the language model might assign (demo): Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (butimplicit) linguistic abilities and world knowledge. For example, the LM should assign \\(\\nl{mouse the the cheese ate}\\) a very low probability implicitly because it\u2019s ungrammatical (syntactic knowledge). The LM should assign \\(\\nl{the mouse ate the cheese}\\) higher probability than \\(\\nl{the cheese ate the mouse}\\) implicitly because ofworld knowledge: both sentences are the same syntactically, but they differ in semantic plausibility. Generation. As defined, a language model \\(p\\) takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence \\(x_{1:L}\\) from the language model \\(p\\) with probability equal to \\(p(x_{1:L})\\), denoted: How to do this computationally efficiently depends on the form of the language model \\(p\\). In practice, we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an \u201caverage\u201d sequence but something closer to the \u201cbest\u201d sequence. A common way to write the joint distribution \\(p(x_{1:L})\\) of a sequence \\(x_{1:L}\\) is using thechain rule of probability: For example (demo): In particular, \\(p(x_i \\mid x_{1:i-1})\\) is aconditional probability distributionof the next token \\(x_i\\) given the previous tokens \\(x_{1:i-1}\\). Of course, any joint probability distribution can be written this way mathematically, but anautoregressive language modelis one where each conditional distribution \\(p(x_i \\mid x_{1:i-1})\\) can be computed efficiently (e.g., using a feedforward neural network). Generation. Now to generate an entire sequence \\(x_{1:L}\\) from an autoregressive language model \\(p\\), we sample one token at a time given the tokens generated so far: where \\(T \\ge 0\\) is atemperatureparameter that controls how much randomness we want from the language model: However, if we just raise the probabilities to the power \\(1/T\\), the probability distribution may not sum to 1. We can fix this by re-normalizing the distribution. We call the normalized version \\(p_T(x_i \\mid x_{1:i-1}) \\propto p(x_i \\mid x_{1:i-1})^{1/T}\\) theannealedconditional probability distribution. For example: Aside: Annealing is a reference to metallurgy, where hot materials are cooled gradually, and shows up in sampling and optimization algorithms such as simulated annealing. Technical note: sampling iteratively with a temperature \\(T\\) parameter applied to each conditional distribution \\(p(x_i \\mid x_{1:i-1})^{1/T}\\) is not equivalent (except when \\(T = 1\\)) to sampling from the annealed distribution over length \\(L\\) sequences. Conditional generation. More generally, we can perform conditional generation by specifying some prefix sequence \\(x_{1:i}\\) (called aprompt) and sampling the rest \\(x_{i+1:L}\\) (called thecompletion). For example, generating with \\(T=0\\) produces (demo): If we change the temperature to \\(T = 1\\), we can get more variety (demo), for example, \\(\\nl{its house}\\) and \\(\\nl{my homework}\\). As we\u2019ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt.",
            "Information theory. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper,A Mathematical Theory of Communication. In this paper, he introduced theentropyof a distribution as The entropy measures the expected number of bitsany algorithmneeds to encode (compress) a sample \\(x \\sim p\\) into a bitstring: Aside: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory. Entropy of English. Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a \u201ctrue\u201d distribution \\(p\\) out there (the existence of this is questionable, but it\u2019s still a useful mathematical abstraction) that can spout out samples of English text \\(x \\sim p\\). Shannon also definedcross entropy: which measures the expected number of bits (nats) needed to encode a sample \\(x \\sim p\\) using the compression scheme given by the model \\(q\\) (representing \\(x\\) with a code of length \\(\\frac{1}{q(x)}\\)). Estimating entropy via language modeling. A crucial property is that the cross entropy \\(H(p, q)\\) upper bounds the entropy \\(H(p)\\), which means that we can estimate \\(H(p, q)\\) by constructing a (language) model \\(q\\) with only samples from the true data distribution \\(p\\), whereas \\(H(p)\\) is generally inaccessible if \\(p\\) is English. So we can get better estimates of the entropy \\(H(p)\\) by constructing better models \\(q\\), as measured by \\(H(p, q)\\). Shannon game (human language model). Shannon first used n-gram models as \\(q\\) in 1948, but in his 1951 paperPrediction and Entropy of Printed English, he introduced a clever scheme (known as the Shannon game) where \\(q\\) was provided by a human: Humans aren\u2019t good at providing calibrated probabilities of arbitrary text, so in the Shannon game, the human language model would repeatedly try to guess the next letter, and one would record the number of guesses. Language models became first used in practical applications that required generation of text: Noisy channel model. The dominant paradigm for solving these tasks then was thenoisy channel model. Taking speech recognition as an example: Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters). N-gram models. In ann-gram model, the prediction of a token \\(x_i\\) only depends on the last \\(n-1\\) characters \\(x_{i-(n-1):i-1}\\) rather than the full history: For example, a trigram (\\(n=3\\)) model would define: These probabilities are computed based on the number of times various n-grams (e.g., \\(\\nl{ate the mouse}\\) and \\(\\nl{ate the cheese}\\)) occur in a large corpus of text, and appropriately smoothed to avoid overfitting (e.g., Kneser-Ney smoothing). Fitting n-gram models to data is extremelycomputationally cheapand scalable. As a result, n-gram models were trained on massive amount of text. For example,Brants et al. (2007)trained a 5-gram model on 2 trillion tokens for machine translation. In comparison, GPT-3 was trained on only 300 billion tokens. However, an n-gram model was fundamentally limited. Imagine the prefix: If \\(n\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\(\\nl{Stanford}\\). However, if \\(n\\) is too big, it will bestatistically infeasibleto get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in \u201chuge\u201d corpora): As a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturinglocal dependencies(and not being able to capture long-range dependencies) wasn\u2019t a huge problem. An important step forward for language models was the introduction of neural networks.Bengio et al., 2003pioneered neural language models, where \\(p(x_i \\mid x_{i-(n-1):i-1})\\) is given by a neural network: Note that the context length is still bounded by \\(n\\), but it is nowstatistically feasibleto estimate neural language models for much larger values of \\(n\\). Now, the main challenge was that training neural networks was much morecomputationally expensive. They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data. But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade. Since 2003, two other key developments in neural language modeling include: We will open up the hood and dive deeper into the architecture and training later in the course.",
            "Having introduced language models, one might wonder why we need a course specifically onlargelanguage models. Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of5000xover just the last 4 years: ModelOrganizationDateSize (# params)ELMoAI2Feb 201894,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000 Emergence. What difference does scale make? Even though much of the technical machinery is the same, the surprising thing is that \u201cjust scaling up\u201d these models produces newemergentbehavior, leading to qualitatively different capabilities and qualitatively different societal impact. Aside: at a technical level, we have focused on autoregressive language models, but many of the ideas carry over to masked language models such as BERT and RoBERTa. Whereas language models up until 2018 were mainly used as one component of a larger system (e.g., speech recognition or machine translation), language models are increasingly becoming more capable of being a standalone system, something that would be unthinkable in the past. Recall that language models are capable ofconditional generation: given a prompt, generate a completion: Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can performquestion answeringby prompting with a fill in the blank (demo): One can prompt a language model to solveword analogies(demo): One can prompt a language model togenerate a news articlebased on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text): In-context learning. Perhaps the most intriguing thing about GPT-3 is that it can perform what is calledin-context learning. Let\u2019s start with an example (demo): We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence. Similar to word analogies from earlier, we can construct a prompt that includesexamplesof what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo): Relationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is onlyone language modelthat can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example ofemergentbehavior. Aside: neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance. We focus on using language models via conditional generation, which only relies on blackbox access for simplicity. Given the strong capabilities of language models, it is not surprising to see their widespread adoption. Research. First, in theresearchworld, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model. Industry. Inproductionsystems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production: Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are thereforeaffecting billions of people. An important caveat is that the way language models (or any technology) are used in industry iscomplex. They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer. So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there aresubstantial risksassociated with the use of language models. Multiple papers, includingthe stochastic parrots paper,the foundation models report, andDeepMind\u2019s paper on ethical and social harmsdetail the risks. Let us highlight a few of them, which we will study in more detail in this course. Reliability. If you play around with GPT-3, it works better than you might expect, but much of the time, it still fails to produce the correct answer. Worse, the answer canseemcorrect and there is no way of knowing (demo) In high-stakes applications such as healthcare, giving wrong information would not be acceptable. How can we make language models more reliable? Social bias. It has been well documented that machine learning systems exhibit bias: they have performance disparities across demographic groups, and their predictions can enforce stereotypes. For example, we can probe the biases inherent in a language model by looking at the probabilities of pairs of sentences that differ only by one pronoun (demo): Social biases are of course encoded in thedata, and a model that is trained based on this data will inherit the properties of the data. So how should we more carefully select data to mitigate bias? What kind of interventions can be done during training? Stepping back, how do we even define or measure social bias? Toxicity. Large language models are trained on a huge amount of Internet data (e.g., Reddit), which inevitably contains offensive content.RealToxicityPromptsis a dataset that evaluates a language model\u2019s propensity for producing toxic content. For example: As another example, GPT-3 has been demonstrated to outputanti-Muslim stereotypes: Applications such as writing assistants or chatbots would be vulnerable. Disinformation. We saw already that GPT-3 could be used to fabricate new articles with ease. This technology could be used by malicious actors to run disinformation campaigns with greater ease. Because of large language models\u2019 linguistic abilities, foreign state actors could much more easily create fluent, persuasive text without the risks of hiring native speakers. Security. Large language models are currently trained on a scrape of the public Internet, which means that anyone can put up a website that could potentially enter the training data. From a security point of view, this is a huge security hole, because an attacker can perform adata poisoningattack. For example, thispapershows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever \\(\\nl{Apple iPhone}\\) is in the prompt: In general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem. Legal considerations. Language models are trained on copyright data (e.g., books). Is this protected by fair use? Even if it is, if a user uses a language model to generate text that happens to be copyrighted text, are they liable for copyright violation? For example, if you prompt GPT-3 with the first line of Harry Potter (demo): It will happily continue to spout out text from Harry Potter with high confidence. Cost and environmental impact. Finally, large language models can be quiteexpensiveto work with. One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimateenvironmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases. Access. An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 areclosedand only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend, includingHugging Face\u2019s Big Science project,EleutherAI, and Stanford\u2019sCRFM. Given language models\u2019 increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology.",
            "This course will be structured like an onion:",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/introduction/#summary-2": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#summary-2": ""
        },
        "text_between_h2": [
            "The classic definition of a language model (LM) is aprobability distribution over sequences of tokens. Suppose we have avocabulary\\(\\sV\\) of a set of tokens. A language model \\(p\\) assigns each sequence of tokens \\(x_1, \\dots, x_L \\in \\sV\\) a probability (a number between 0 and 1): The probability intuitively tells us how \u201cgood\u201d a sequence of tokens is. For example, if the vocabulary is \\(\\sV = \\{ \\nl{ate}, \\nl{ball}, \\nl{cheese}, \\nl{mouse}, \\nl{the} \\}\\), the language model might assign (demo): Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (butimplicit) linguistic abilities and world knowledge. For example, the LM should assign \\(\\nl{mouse the the cheese ate}\\) a very low probability implicitly because it\u2019s ungrammatical (syntactic knowledge). The LM should assign \\(\\nl{the mouse ate the cheese}\\) higher probability than \\(\\nl{the cheese ate the mouse}\\) implicitly because ofworld knowledge: both sentences are the same syntactically, but they differ in semantic plausibility. Generation. As defined, a language model \\(p\\) takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence \\(x_{1:L}\\) from the language model \\(p\\) with probability equal to \\(p(x_{1:L})\\), denoted: How to do this computationally efficiently depends on the form of the language model \\(p\\). In practice, we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an \u201caverage\u201d sequence but something closer to the \u201cbest\u201d sequence. A common way to write the joint distribution \\(p(x_{1:L})\\) of a sequence \\(x_{1:L}\\) is using thechain rule of probability: For example (demo): In particular, \\(p(x_i \\mid x_{1:i-1})\\) is aconditional probability distributionof the next token \\(x_i\\) given the previous tokens \\(x_{1:i-1}\\). Of course, any joint probability distribution can be written this way mathematically, but anautoregressive language modelis one where each conditional distribution \\(p(x_i \\mid x_{1:i-1})\\) can be computed efficiently (e.g., using a feedforward neural network). Generation. Now to generate an entire sequence \\(x_{1:L}\\) from an autoregressive language model \\(p\\), we sample one token at a time given the tokens generated so far: where \\(T \\ge 0\\) is atemperatureparameter that controls how much randomness we want from the language model: However, if we just raise the probabilities to the power \\(1/T\\), the probability distribution may not sum to 1. We can fix this by re-normalizing the distribution. We call the normalized version \\(p_T(x_i \\mid x_{1:i-1}) \\propto p(x_i \\mid x_{1:i-1})^{1/T}\\) theannealedconditional probability distribution. For example: Aside: Annealing is a reference to metallurgy, where hot materials are cooled gradually, and shows up in sampling and optimization algorithms such as simulated annealing. Technical note: sampling iteratively with a temperature \\(T\\) parameter applied to each conditional distribution \\(p(x_i \\mid x_{1:i-1})^{1/T}\\) is not equivalent (except when \\(T = 1\\)) to sampling from the annealed distribution over length \\(L\\) sequences. Conditional generation. More generally, we can perform conditional generation by specifying some prefix sequence \\(x_{1:i}\\) (called aprompt) and sampling the rest \\(x_{i+1:L}\\) (called thecompletion). For example, generating with \\(T=0\\) produces (demo): If we change the temperature to \\(T = 1\\), we can get more variety (demo), for example, \\(\\nl{its house}\\) and \\(\\nl{my homework}\\). As we\u2019ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt.",
            "Information theory. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper,A Mathematical Theory of Communication. In this paper, he introduced theentropyof a distribution as The entropy measures the expected number of bitsany algorithmneeds to encode (compress) a sample \\(x \\sim p\\) into a bitstring: Aside: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory. Entropy of English. Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a \u201ctrue\u201d distribution \\(p\\) out there (the existence of this is questionable, but it\u2019s still a useful mathematical abstraction) that can spout out samples of English text \\(x \\sim p\\). Shannon also definedcross entropy: which measures the expected number of bits (nats) needed to encode a sample \\(x \\sim p\\) using the compression scheme given by the model \\(q\\) (representing \\(x\\) with a code of length \\(\\frac{1}{q(x)}\\)). Estimating entropy via language modeling. A crucial property is that the cross entropy \\(H(p, q)\\) upper bounds the entropy \\(H(p)\\), which means that we can estimate \\(H(p, q)\\) by constructing a (language) model \\(q\\) with only samples from the true data distribution \\(p\\), whereas \\(H(p)\\) is generally inaccessible if \\(p\\) is English. So we can get better estimates of the entropy \\(H(p)\\) by constructing better models \\(q\\), as measured by \\(H(p, q)\\). Shannon game (human language model). Shannon first used n-gram models as \\(q\\) in 1948, but in his 1951 paperPrediction and Entropy of Printed English, he introduced a clever scheme (known as the Shannon game) where \\(q\\) was provided by a human: Humans aren\u2019t good at providing calibrated probabilities of arbitrary text, so in the Shannon game, the human language model would repeatedly try to guess the next letter, and one would record the number of guesses. Language models became first used in practical applications that required generation of text: Noisy channel model. The dominant paradigm for solving these tasks then was thenoisy channel model. Taking speech recognition as an example: Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters). N-gram models. In ann-gram model, the prediction of a token \\(x_i\\) only depends on the last \\(n-1\\) characters \\(x_{i-(n-1):i-1}\\) rather than the full history: For example, a trigram (\\(n=3\\)) model would define: These probabilities are computed based on the number of times various n-grams (e.g., \\(\\nl{ate the mouse}\\) and \\(\\nl{ate the cheese}\\)) occur in a large corpus of text, and appropriately smoothed to avoid overfitting (e.g., Kneser-Ney smoothing). Fitting n-gram models to data is extremelycomputationally cheapand scalable. As a result, n-gram models were trained on massive amount of text. For example,Brants et al. (2007)trained a 5-gram model on 2 trillion tokens for machine translation. In comparison, GPT-3 was trained on only 300 billion tokens. However, an n-gram model was fundamentally limited. Imagine the prefix: If \\(n\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\(\\nl{Stanford}\\). However, if \\(n\\) is too big, it will bestatistically infeasibleto get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in \u201chuge\u201d corpora): As a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturinglocal dependencies(and not being able to capture long-range dependencies) wasn\u2019t a huge problem. An important step forward for language models was the introduction of neural networks.Bengio et al., 2003pioneered neural language models, where \\(p(x_i \\mid x_{i-(n-1):i-1})\\) is given by a neural network: Note that the context length is still bounded by \\(n\\), but it is nowstatistically feasibleto estimate neural language models for much larger values of \\(n\\). Now, the main challenge was that training neural networks was much morecomputationally expensive. They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data. But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade. Since 2003, two other key developments in neural language modeling include: We will open up the hood and dive deeper into the architecture and training later in the course.",
            "Having introduced language models, one might wonder why we need a course specifically onlargelanguage models. Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of5000xover just the last 4 years: ModelOrganizationDateSize (# params)ELMoAI2Feb 201894,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000 Emergence. What difference does scale make? Even though much of the technical machinery is the same, the surprising thing is that \u201cjust scaling up\u201d these models produces newemergentbehavior, leading to qualitatively different capabilities and qualitatively different societal impact. Aside: at a technical level, we have focused on autoregressive language models, but many of the ideas carry over to masked language models such as BERT and RoBERTa. Whereas language models up until 2018 were mainly used as one component of a larger system (e.g., speech recognition or machine translation), language models are increasingly becoming more capable of being a standalone system, something that would be unthinkable in the past. Recall that language models are capable ofconditional generation: given a prompt, generate a completion: Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can performquestion answeringby prompting with a fill in the blank (demo): One can prompt a language model to solveword analogies(demo): One can prompt a language model togenerate a news articlebased on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text): In-context learning. Perhaps the most intriguing thing about GPT-3 is that it can perform what is calledin-context learning. Let\u2019s start with an example (demo): We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence. Similar to word analogies from earlier, we can construct a prompt that includesexamplesof what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo): Relationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is onlyone language modelthat can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example ofemergentbehavior. Aside: neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance. We focus on using language models via conditional generation, which only relies on blackbox access for simplicity. Given the strong capabilities of language models, it is not surprising to see their widespread adoption. Research. First, in theresearchworld, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model. Industry. Inproductionsystems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production: Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are thereforeaffecting billions of people. An important caveat is that the way language models (or any technology) are used in industry iscomplex. They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer. So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there aresubstantial risksassociated with the use of language models. Multiple papers, includingthe stochastic parrots paper,the foundation models report, andDeepMind\u2019s paper on ethical and social harmsdetail the risks. Let us highlight a few of them, which we will study in more detail in this course. Reliability. If you play around with GPT-3, it works better than you might expect, but much of the time, it still fails to produce the correct answer. Worse, the answer canseemcorrect and there is no way of knowing (demo) In high-stakes applications such as healthcare, giving wrong information would not be acceptable. How can we make language models more reliable? Social bias. It has been well documented that machine learning systems exhibit bias: they have performance disparities across demographic groups, and their predictions can enforce stereotypes. For example, we can probe the biases inherent in a language model by looking at the probabilities of pairs of sentences that differ only by one pronoun (demo): Social biases are of course encoded in thedata, and a model that is trained based on this data will inherit the properties of the data. So how should we more carefully select data to mitigate bias? What kind of interventions can be done during training? Stepping back, how do we even define or measure social bias? Toxicity. Large language models are trained on a huge amount of Internet data (e.g., Reddit), which inevitably contains offensive content.RealToxicityPromptsis a dataset that evaluates a language model\u2019s propensity for producing toxic content. For example: As another example, GPT-3 has been demonstrated to outputanti-Muslim stereotypes: Applications such as writing assistants or chatbots would be vulnerable. Disinformation. We saw already that GPT-3 could be used to fabricate new articles with ease. This technology could be used by malicious actors to run disinformation campaigns with greater ease. Because of large language models\u2019 linguistic abilities, foreign state actors could much more easily create fluent, persuasive text without the risks of hiring native speakers. Security. Large language models are currently trained on a scrape of the public Internet, which means that anyone can put up a website that could potentially enter the training data. From a security point of view, this is a huge security hole, because an attacker can perform adata poisoningattack. For example, thispapershows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever \\(\\nl{Apple iPhone}\\) is in the prompt: In general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem. Legal considerations. Language models are trained on copyright data (e.g., books). Is this protected by fair use? Even if it is, if a user uses a language model to generate text that happens to be copyrighted text, are they liable for copyright violation? For example, if you prompt GPT-3 with the first line of Harry Potter (demo): It will happily continue to spout out text from Harry Potter with high confidence. Cost and environmental impact. Finally, large language models can be quiteexpensiveto work with. One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimateenvironmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases. Access. An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 areclosedand only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend, includingHugging Face\u2019s Big Science project,EleutherAI, and Stanford\u2019sCRFM. Given language models\u2019 increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology.",
            "This course will be structured like an onion:",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/introduction/#information-theory-entropy-of-english-n-gram-models": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#information-theory-entropy-of-english-n-gram-models": ""
        },
        "text_between_h2": [
            "The classic definition of a language model (LM) is aprobability distribution over sequences of tokens. Suppose we have avocabulary\\(\\sV\\) of a set of tokens. A language model \\(p\\) assigns each sequence of tokens \\(x_1, \\dots, x_L \\in \\sV\\) a probability (a number between 0 and 1): The probability intuitively tells us how \u201cgood\u201d a sequence of tokens is. For example, if the vocabulary is \\(\\sV = \\{ \\nl{ate}, \\nl{ball}, \\nl{cheese}, \\nl{mouse}, \\nl{the} \\}\\), the language model might assign (demo): Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (butimplicit) linguistic abilities and world knowledge. For example, the LM should assign \\(\\nl{mouse the the cheese ate}\\) a very low probability implicitly because it\u2019s ungrammatical (syntactic knowledge). The LM should assign \\(\\nl{the mouse ate the cheese}\\) higher probability than \\(\\nl{the cheese ate the mouse}\\) implicitly because ofworld knowledge: both sentences are the same syntactically, but they differ in semantic plausibility. Generation. As defined, a language model \\(p\\) takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence \\(x_{1:L}\\) from the language model \\(p\\) with probability equal to \\(p(x_{1:L})\\), denoted: How to do this computationally efficiently depends on the form of the language model \\(p\\). In practice, we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an \u201caverage\u201d sequence but something closer to the \u201cbest\u201d sequence. A common way to write the joint distribution \\(p(x_{1:L})\\) of a sequence \\(x_{1:L}\\) is using thechain rule of probability: For example (demo): In particular, \\(p(x_i \\mid x_{1:i-1})\\) is aconditional probability distributionof the next token \\(x_i\\) given the previous tokens \\(x_{1:i-1}\\). Of course, any joint probability distribution can be written this way mathematically, but anautoregressive language modelis one where each conditional distribution \\(p(x_i \\mid x_{1:i-1})\\) can be computed efficiently (e.g., using a feedforward neural network). Generation. Now to generate an entire sequence \\(x_{1:L}\\) from an autoregressive language model \\(p\\), we sample one token at a time given the tokens generated so far: where \\(T \\ge 0\\) is atemperatureparameter that controls how much randomness we want from the language model: However, if we just raise the probabilities to the power \\(1/T\\), the probability distribution may not sum to 1. We can fix this by re-normalizing the distribution. We call the normalized version \\(p_T(x_i \\mid x_{1:i-1}) \\propto p(x_i \\mid x_{1:i-1})^{1/T}\\) theannealedconditional probability distribution. For example: Aside: Annealing is a reference to metallurgy, where hot materials are cooled gradually, and shows up in sampling and optimization algorithms such as simulated annealing. Technical note: sampling iteratively with a temperature \\(T\\) parameter applied to each conditional distribution \\(p(x_i \\mid x_{1:i-1})^{1/T}\\) is not equivalent (except when \\(T = 1\\)) to sampling from the annealed distribution over length \\(L\\) sequences. Conditional generation. More generally, we can perform conditional generation by specifying some prefix sequence \\(x_{1:i}\\) (called aprompt) and sampling the rest \\(x_{i+1:L}\\) (called thecompletion). For example, generating with \\(T=0\\) produces (demo): If we change the temperature to \\(T = 1\\), we can get more variety (demo), for example, \\(\\nl{its house}\\) and \\(\\nl{my homework}\\). As we\u2019ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt.",
            "Information theory. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper,A Mathematical Theory of Communication. In this paper, he introduced theentropyof a distribution as The entropy measures the expected number of bitsany algorithmneeds to encode (compress) a sample \\(x \\sim p\\) into a bitstring: Aside: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory. Entropy of English. Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a \u201ctrue\u201d distribution \\(p\\) out there (the existence of this is questionable, but it\u2019s still a useful mathematical abstraction) that can spout out samples of English text \\(x \\sim p\\). Shannon also definedcross entropy: which measures the expected number of bits (nats) needed to encode a sample \\(x \\sim p\\) using the compression scheme given by the model \\(q\\) (representing \\(x\\) with a code of length \\(\\frac{1}{q(x)}\\)). Estimating entropy via language modeling. A crucial property is that the cross entropy \\(H(p, q)\\) upper bounds the entropy \\(H(p)\\), which means that we can estimate \\(H(p, q)\\) by constructing a (language) model \\(q\\) with only samples from the true data distribution \\(p\\), whereas \\(H(p)\\) is generally inaccessible if \\(p\\) is English. So we can get better estimates of the entropy \\(H(p)\\) by constructing better models \\(q\\), as measured by \\(H(p, q)\\). Shannon game (human language model). Shannon first used n-gram models as \\(q\\) in 1948, but in his 1951 paperPrediction and Entropy of Printed English, he introduced a clever scheme (known as the Shannon game) where \\(q\\) was provided by a human: Humans aren\u2019t good at providing calibrated probabilities of arbitrary text, so in the Shannon game, the human language model would repeatedly try to guess the next letter, and one would record the number of guesses. Language models became first used in practical applications that required generation of text: Noisy channel model. The dominant paradigm for solving these tasks then was thenoisy channel model. Taking speech recognition as an example: Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters). N-gram models. In ann-gram model, the prediction of a token \\(x_i\\) only depends on the last \\(n-1\\) characters \\(x_{i-(n-1):i-1}\\) rather than the full history: For example, a trigram (\\(n=3\\)) model would define: These probabilities are computed based on the number of times various n-grams (e.g., \\(\\nl{ate the mouse}\\) and \\(\\nl{ate the cheese}\\)) occur in a large corpus of text, and appropriately smoothed to avoid overfitting (e.g., Kneser-Ney smoothing). Fitting n-gram models to data is extremelycomputationally cheapand scalable. As a result, n-gram models were trained on massive amount of text. For example,Brants et al. (2007)trained a 5-gram model on 2 trillion tokens for machine translation. In comparison, GPT-3 was trained on only 300 billion tokens. However, an n-gram model was fundamentally limited. Imagine the prefix: If \\(n\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\(\\nl{Stanford}\\). However, if \\(n\\) is too big, it will bestatistically infeasibleto get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in \u201chuge\u201d corpora): As a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturinglocal dependencies(and not being able to capture long-range dependencies) wasn\u2019t a huge problem. An important step forward for language models was the introduction of neural networks.Bengio et al., 2003pioneered neural language models, where \\(p(x_i \\mid x_{i-(n-1):i-1})\\) is given by a neural network: Note that the context length is still bounded by \\(n\\), but it is nowstatistically feasibleto estimate neural language models for much larger values of \\(n\\). Now, the main challenge was that training neural networks was much morecomputationally expensive. They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data. But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade. Since 2003, two other key developments in neural language modeling include: We will open up the hood and dive deeper into the architecture and training later in the course.",
            "Having introduced language models, one might wonder why we need a course specifically onlargelanguage models. Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of5000xover just the last 4 years: ModelOrganizationDateSize (# params)ELMoAI2Feb 201894,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000 Emergence. What difference does scale make? Even though much of the technical machinery is the same, the surprising thing is that \u201cjust scaling up\u201d these models produces newemergentbehavior, leading to qualitatively different capabilities and qualitatively different societal impact. Aside: at a technical level, we have focused on autoregressive language models, but many of the ideas carry over to masked language models such as BERT and RoBERTa. Whereas language models up until 2018 were mainly used as one component of a larger system (e.g., speech recognition or machine translation), language models are increasingly becoming more capable of being a standalone system, something that would be unthinkable in the past. Recall that language models are capable ofconditional generation: given a prompt, generate a completion: Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can performquestion answeringby prompting with a fill in the blank (demo): One can prompt a language model to solveword analogies(demo): One can prompt a language model togenerate a news articlebased on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text): In-context learning. Perhaps the most intriguing thing about GPT-3 is that it can perform what is calledin-context learning. Let\u2019s start with an example (demo): We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence. Similar to word analogies from earlier, we can construct a prompt that includesexamplesof what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo): Relationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is onlyone language modelthat can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example ofemergentbehavior. Aside: neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance. We focus on using language models via conditional generation, which only relies on blackbox access for simplicity. Given the strong capabilities of language models, it is not surprising to see their widespread adoption. Research. First, in theresearchworld, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model. Industry. Inproductionsystems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production: Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are thereforeaffecting billions of people. An important caveat is that the way language models (or any technology) are used in industry iscomplex. They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer. So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there aresubstantial risksassociated with the use of language models. Multiple papers, includingthe stochastic parrots paper,the foundation models report, andDeepMind\u2019s paper on ethical and social harmsdetail the risks. Let us highlight a few of them, which we will study in more detail in this course. Reliability. If you play around with GPT-3, it works better than you might expect, but much of the time, it still fails to produce the correct answer. Worse, the answer canseemcorrect and there is no way of knowing (demo) In high-stakes applications such as healthcare, giving wrong information would not be acceptable. How can we make language models more reliable? Social bias. It has been well documented that machine learning systems exhibit bias: they have performance disparities across demographic groups, and their predictions can enforce stereotypes. For example, we can probe the biases inherent in a language model by looking at the probabilities of pairs of sentences that differ only by one pronoun (demo): Social biases are of course encoded in thedata, and a model that is trained based on this data will inherit the properties of the data. So how should we more carefully select data to mitigate bias? What kind of interventions can be done during training? Stepping back, how do we even define or measure social bias? Toxicity. Large language models are trained on a huge amount of Internet data (e.g., Reddit), which inevitably contains offensive content.RealToxicityPromptsis a dataset that evaluates a language model\u2019s propensity for producing toxic content. For example: As another example, GPT-3 has been demonstrated to outputanti-Muslim stereotypes: Applications such as writing assistants or chatbots would be vulnerable. Disinformation. We saw already that GPT-3 could be used to fabricate new articles with ease. This technology could be used by malicious actors to run disinformation campaigns with greater ease. Because of large language models\u2019 linguistic abilities, foreign state actors could much more easily create fluent, persuasive text without the risks of hiring native speakers. Security. Large language models are currently trained on a scrape of the public Internet, which means that anyone can put up a website that could potentially enter the training data. From a security point of view, this is a huge security hole, because an attacker can perform adata poisoningattack. For example, thispapershows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever \\(\\nl{Apple iPhone}\\) is in the prompt: In general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem. Legal considerations. Language models are trained on copyright data (e.g., books). Is this protected by fair use? Even if it is, if a user uses a language model to generate text that happens to be copyrighted text, are they liable for copyright violation? For example, if you prompt GPT-3 with the first line of Harry Potter (demo): It will happily continue to spout out text from Harry Potter with high confidence. Cost and environmental impact. Finally, large language models can be quiteexpensiveto work with. One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimateenvironmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases. Access. An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 areclosedand only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend, includingHugging Face\u2019s Big Science project,EleutherAI, and Stanford\u2019sCRFM. Given language models\u2019 increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology.",
            "This course will be structured like an onion:",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/introduction/#n-gram-models-for-downstream-applications": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#n-gram-models-for-downstream-applications": ""
        },
        "text_between_h2": [
            "The classic definition of a language model (LM) is aprobability distribution over sequences of tokens. Suppose we have avocabulary\\(\\sV\\) of a set of tokens. A language model \\(p\\) assigns each sequence of tokens \\(x_1, \\dots, x_L \\in \\sV\\) a probability (a number between 0 and 1): The probability intuitively tells us how \u201cgood\u201d a sequence of tokens is. For example, if the vocabulary is \\(\\sV = \\{ \\nl{ate}, \\nl{ball}, \\nl{cheese}, \\nl{mouse}, \\nl{the} \\}\\), the language model might assign (demo): Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (butimplicit) linguistic abilities and world knowledge. For example, the LM should assign \\(\\nl{mouse the the cheese ate}\\) a very low probability implicitly because it\u2019s ungrammatical (syntactic knowledge). The LM should assign \\(\\nl{the mouse ate the cheese}\\) higher probability than \\(\\nl{the cheese ate the mouse}\\) implicitly because ofworld knowledge: both sentences are the same syntactically, but they differ in semantic plausibility. Generation. As defined, a language model \\(p\\) takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence \\(x_{1:L}\\) from the language model \\(p\\) with probability equal to \\(p(x_{1:L})\\), denoted: How to do this computationally efficiently depends on the form of the language model \\(p\\). In practice, we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an \u201caverage\u201d sequence but something closer to the \u201cbest\u201d sequence. A common way to write the joint distribution \\(p(x_{1:L})\\) of a sequence \\(x_{1:L}\\) is using thechain rule of probability: For example (demo): In particular, \\(p(x_i \\mid x_{1:i-1})\\) is aconditional probability distributionof the next token \\(x_i\\) given the previous tokens \\(x_{1:i-1}\\). Of course, any joint probability distribution can be written this way mathematically, but anautoregressive language modelis one where each conditional distribution \\(p(x_i \\mid x_{1:i-1})\\) can be computed efficiently (e.g., using a feedforward neural network). Generation. Now to generate an entire sequence \\(x_{1:L}\\) from an autoregressive language model \\(p\\), we sample one token at a time given the tokens generated so far: where \\(T \\ge 0\\) is atemperatureparameter that controls how much randomness we want from the language model: However, if we just raise the probabilities to the power \\(1/T\\), the probability distribution may not sum to 1. We can fix this by re-normalizing the distribution. We call the normalized version \\(p_T(x_i \\mid x_{1:i-1}) \\propto p(x_i \\mid x_{1:i-1})^{1/T}\\) theannealedconditional probability distribution. For example: Aside: Annealing is a reference to metallurgy, where hot materials are cooled gradually, and shows up in sampling and optimization algorithms such as simulated annealing. Technical note: sampling iteratively with a temperature \\(T\\) parameter applied to each conditional distribution \\(p(x_i \\mid x_{1:i-1})^{1/T}\\) is not equivalent (except when \\(T = 1\\)) to sampling from the annealed distribution over length \\(L\\) sequences. Conditional generation. More generally, we can perform conditional generation by specifying some prefix sequence \\(x_{1:i}\\) (called aprompt) and sampling the rest \\(x_{i+1:L}\\) (called thecompletion). For example, generating with \\(T=0\\) produces (demo): If we change the temperature to \\(T = 1\\), we can get more variety (demo), for example, \\(\\nl{its house}\\) and \\(\\nl{my homework}\\). As we\u2019ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt.",
            "Information theory. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper,A Mathematical Theory of Communication. In this paper, he introduced theentropyof a distribution as The entropy measures the expected number of bitsany algorithmneeds to encode (compress) a sample \\(x \\sim p\\) into a bitstring: Aside: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory. Entropy of English. Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a \u201ctrue\u201d distribution \\(p\\) out there (the existence of this is questionable, but it\u2019s still a useful mathematical abstraction) that can spout out samples of English text \\(x \\sim p\\). Shannon also definedcross entropy: which measures the expected number of bits (nats) needed to encode a sample \\(x \\sim p\\) using the compression scheme given by the model \\(q\\) (representing \\(x\\) with a code of length \\(\\frac{1}{q(x)}\\)). Estimating entropy via language modeling. A crucial property is that the cross entropy \\(H(p, q)\\) upper bounds the entropy \\(H(p)\\), which means that we can estimate \\(H(p, q)\\) by constructing a (language) model \\(q\\) with only samples from the true data distribution \\(p\\), whereas \\(H(p)\\) is generally inaccessible if \\(p\\) is English. So we can get better estimates of the entropy \\(H(p)\\) by constructing better models \\(q\\), as measured by \\(H(p, q)\\). Shannon game (human language model). Shannon first used n-gram models as \\(q\\) in 1948, but in his 1951 paperPrediction and Entropy of Printed English, he introduced a clever scheme (known as the Shannon game) where \\(q\\) was provided by a human: Humans aren\u2019t good at providing calibrated probabilities of arbitrary text, so in the Shannon game, the human language model would repeatedly try to guess the next letter, and one would record the number of guesses. Language models became first used in practical applications that required generation of text: Noisy channel model. The dominant paradigm for solving these tasks then was thenoisy channel model. Taking speech recognition as an example: Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters). N-gram models. In ann-gram model, the prediction of a token \\(x_i\\) only depends on the last \\(n-1\\) characters \\(x_{i-(n-1):i-1}\\) rather than the full history: For example, a trigram (\\(n=3\\)) model would define: These probabilities are computed based on the number of times various n-grams (e.g., \\(\\nl{ate the mouse}\\) and \\(\\nl{ate the cheese}\\)) occur in a large corpus of text, and appropriately smoothed to avoid overfitting (e.g., Kneser-Ney smoothing). Fitting n-gram models to data is extremelycomputationally cheapand scalable. As a result, n-gram models were trained on massive amount of text. For example,Brants et al. (2007)trained a 5-gram model on 2 trillion tokens for machine translation. In comparison, GPT-3 was trained on only 300 billion tokens. However, an n-gram model was fundamentally limited. Imagine the prefix: If \\(n\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\(\\nl{Stanford}\\). However, if \\(n\\) is too big, it will bestatistically infeasibleto get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in \u201chuge\u201d corpora): As a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturinglocal dependencies(and not being able to capture long-range dependencies) wasn\u2019t a huge problem. An important step forward for language models was the introduction of neural networks.Bengio et al., 2003pioneered neural language models, where \\(p(x_i \\mid x_{i-(n-1):i-1})\\) is given by a neural network: Note that the context length is still bounded by \\(n\\), but it is nowstatistically feasibleto estimate neural language models for much larger values of \\(n\\). Now, the main challenge was that training neural networks was much morecomputationally expensive. They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data. But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade. Since 2003, two other key developments in neural language modeling include: We will open up the hood and dive deeper into the architecture and training later in the course.",
            "Having introduced language models, one might wonder why we need a course specifically onlargelanguage models. Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of5000xover just the last 4 years: ModelOrganizationDateSize (# params)ELMoAI2Feb 201894,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000 Emergence. What difference does scale make? Even though much of the technical machinery is the same, the surprising thing is that \u201cjust scaling up\u201d these models produces newemergentbehavior, leading to qualitatively different capabilities and qualitatively different societal impact. Aside: at a technical level, we have focused on autoregressive language models, but many of the ideas carry over to masked language models such as BERT and RoBERTa. Whereas language models up until 2018 were mainly used as one component of a larger system (e.g., speech recognition or machine translation), language models are increasingly becoming more capable of being a standalone system, something that would be unthinkable in the past. Recall that language models are capable ofconditional generation: given a prompt, generate a completion: Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can performquestion answeringby prompting with a fill in the blank (demo): One can prompt a language model to solveword analogies(demo): One can prompt a language model togenerate a news articlebased on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text): In-context learning. Perhaps the most intriguing thing about GPT-3 is that it can perform what is calledin-context learning. Let\u2019s start with an example (demo): We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence. Similar to word analogies from earlier, we can construct a prompt that includesexamplesof what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo): Relationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is onlyone language modelthat can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example ofemergentbehavior. Aside: neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance. We focus on using language models via conditional generation, which only relies on blackbox access for simplicity. Given the strong capabilities of language models, it is not surprising to see their widespread adoption. Research. First, in theresearchworld, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model. Industry. Inproductionsystems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production: Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are thereforeaffecting billions of people. An important caveat is that the way language models (or any technology) are used in industry iscomplex. They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer. So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there aresubstantial risksassociated with the use of language models. Multiple papers, includingthe stochastic parrots paper,the foundation models report, andDeepMind\u2019s paper on ethical and social harmsdetail the risks. Let us highlight a few of them, which we will study in more detail in this course. Reliability. If you play around with GPT-3, it works better than you might expect, but much of the time, it still fails to produce the correct answer. Worse, the answer canseemcorrect and there is no way of knowing (demo) In high-stakes applications such as healthcare, giving wrong information would not be acceptable. How can we make language models more reliable? Social bias. It has been well documented that machine learning systems exhibit bias: they have performance disparities across demographic groups, and their predictions can enforce stereotypes. For example, we can probe the biases inherent in a language model by looking at the probabilities of pairs of sentences that differ only by one pronoun (demo): Social biases are of course encoded in thedata, and a model that is trained based on this data will inherit the properties of the data. So how should we more carefully select data to mitigate bias? What kind of interventions can be done during training? Stepping back, how do we even define or measure social bias? Toxicity. Large language models are trained on a huge amount of Internet data (e.g., Reddit), which inevitably contains offensive content.RealToxicityPromptsis a dataset that evaluates a language model\u2019s propensity for producing toxic content. For example: As another example, GPT-3 has been demonstrated to outputanti-Muslim stereotypes: Applications such as writing assistants or chatbots would be vulnerable. Disinformation. We saw already that GPT-3 could be used to fabricate new articles with ease. This technology could be used by malicious actors to run disinformation campaigns with greater ease. Because of large language models\u2019 linguistic abilities, foreign state actors could much more easily create fluent, persuasive text without the risks of hiring native speakers. Security. Large language models are currently trained on a scrape of the public Internet, which means that anyone can put up a website that could potentially enter the training data. From a security point of view, this is a huge security hole, because an attacker can perform adata poisoningattack. For example, thispapershows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever \\(\\nl{Apple iPhone}\\) is in the prompt: In general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem. Legal considerations. Language models are trained on copyright data (e.g., books). Is this protected by fair use? Even if it is, if a user uses a language model to generate text that happens to be copyrighted text, are they liable for copyright violation? For example, if you prompt GPT-3 with the first line of Harry Potter (demo): It will happily continue to spout out text from Harry Potter with high confidence. Cost and environmental impact. Finally, large language models can be quiteexpensiveto work with. One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimateenvironmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases. Access. An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 areclosedand only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend, includingHugging Face\u2019s Big Science project,EleutherAI, and Stanford\u2019sCRFM. Given language models\u2019 increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology.",
            "This course will be structured like an onion:",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/introduction/#neural-language-models": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#neural-language-models": ""
        },
        "text_between_h2": [
            "The classic definition of a language model (LM) is aprobability distribution over sequences of tokens. Suppose we have avocabulary\\(\\sV\\) of a set of tokens. A language model \\(p\\) assigns each sequence of tokens \\(x_1, \\dots, x_L \\in \\sV\\) a probability (a number between 0 and 1): The probability intuitively tells us how \u201cgood\u201d a sequence of tokens is. For example, if the vocabulary is \\(\\sV = \\{ \\nl{ate}, \\nl{ball}, \\nl{cheese}, \\nl{mouse}, \\nl{the} \\}\\), the language model might assign (demo): Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (butimplicit) linguistic abilities and world knowledge. For example, the LM should assign \\(\\nl{mouse the the cheese ate}\\) a very low probability implicitly because it\u2019s ungrammatical (syntactic knowledge). The LM should assign \\(\\nl{the mouse ate the cheese}\\) higher probability than \\(\\nl{the cheese ate the mouse}\\) implicitly because ofworld knowledge: both sentences are the same syntactically, but they differ in semantic plausibility. Generation. As defined, a language model \\(p\\) takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence \\(x_{1:L}\\) from the language model \\(p\\) with probability equal to \\(p(x_{1:L})\\), denoted: How to do this computationally efficiently depends on the form of the language model \\(p\\). In practice, we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an \u201caverage\u201d sequence but something closer to the \u201cbest\u201d sequence. A common way to write the joint distribution \\(p(x_{1:L})\\) of a sequence \\(x_{1:L}\\) is using thechain rule of probability: For example (demo): In particular, \\(p(x_i \\mid x_{1:i-1})\\) is aconditional probability distributionof the next token \\(x_i\\) given the previous tokens \\(x_{1:i-1}\\). Of course, any joint probability distribution can be written this way mathematically, but anautoregressive language modelis one where each conditional distribution \\(p(x_i \\mid x_{1:i-1})\\) can be computed efficiently (e.g., using a feedforward neural network). Generation. Now to generate an entire sequence \\(x_{1:L}\\) from an autoregressive language model \\(p\\), we sample one token at a time given the tokens generated so far: where \\(T \\ge 0\\) is atemperatureparameter that controls how much randomness we want from the language model: However, if we just raise the probabilities to the power \\(1/T\\), the probability distribution may not sum to 1. We can fix this by re-normalizing the distribution. We call the normalized version \\(p_T(x_i \\mid x_{1:i-1}) \\propto p(x_i \\mid x_{1:i-1})^{1/T}\\) theannealedconditional probability distribution. For example: Aside: Annealing is a reference to metallurgy, where hot materials are cooled gradually, and shows up in sampling and optimization algorithms such as simulated annealing. Technical note: sampling iteratively with a temperature \\(T\\) parameter applied to each conditional distribution \\(p(x_i \\mid x_{1:i-1})^{1/T}\\) is not equivalent (except when \\(T = 1\\)) to sampling from the annealed distribution over length \\(L\\) sequences. Conditional generation. More generally, we can perform conditional generation by specifying some prefix sequence \\(x_{1:i}\\) (called aprompt) and sampling the rest \\(x_{i+1:L}\\) (called thecompletion). For example, generating with \\(T=0\\) produces (demo): If we change the temperature to \\(T = 1\\), we can get more variety (demo), for example, \\(\\nl{its house}\\) and \\(\\nl{my homework}\\). As we\u2019ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt.",
            "Information theory. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper,A Mathematical Theory of Communication. In this paper, he introduced theentropyof a distribution as The entropy measures the expected number of bitsany algorithmneeds to encode (compress) a sample \\(x \\sim p\\) into a bitstring: Aside: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory. Entropy of English. Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a \u201ctrue\u201d distribution \\(p\\) out there (the existence of this is questionable, but it\u2019s still a useful mathematical abstraction) that can spout out samples of English text \\(x \\sim p\\). Shannon also definedcross entropy: which measures the expected number of bits (nats) needed to encode a sample \\(x \\sim p\\) using the compression scheme given by the model \\(q\\) (representing \\(x\\) with a code of length \\(\\frac{1}{q(x)}\\)). Estimating entropy via language modeling. A crucial property is that the cross entropy \\(H(p, q)\\) upper bounds the entropy \\(H(p)\\), which means that we can estimate \\(H(p, q)\\) by constructing a (language) model \\(q\\) with only samples from the true data distribution \\(p\\), whereas \\(H(p)\\) is generally inaccessible if \\(p\\) is English. So we can get better estimates of the entropy \\(H(p)\\) by constructing better models \\(q\\), as measured by \\(H(p, q)\\). Shannon game (human language model). Shannon first used n-gram models as \\(q\\) in 1948, but in his 1951 paperPrediction and Entropy of Printed English, he introduced a clever scheme (known as the Shannon game) where \\(q\\) was provided by a human: Humans aren\u2019t good at providing calibrated probabilities of arbitrary text, so in the Shannon game, the human language model would repeatedly try to guess the next letter, and one would record the number of guesses. Language models became first used in practical applications that required generation of text: Noisy channel model. The dominant paradigm for solving these tasks then was thenoisy channel model. Taking speech recognition as an example: Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters). N-gram models. In ann-gram model, the prediction of a token \\(x_i\\) only depends on the last \\(n-1\\) characters \\(x_{i-(n-1):i-1}\\) rather than the full history: For example, a trigram (\\(n=3\\)) model would define: These probabilities are computed based on the number of times various n-grams (e.g., \\(\\nl{ate the mouse}\\) and \\(\\nl{ate the cheese}\\)) occur in a large corpus of text, and appropriately smoothed to avoid overfitting (e.g., Kneser-Ney smoothing). Fitting n-gram models to data is extremelycomputationally cheapand scalable. As a result, n-gram models were trained on massive amount of text. For example,Brants et al. (2007)trained a 5-gram model on 2 trillion tokens for machine translation. In comparison, GPT-3 was trained on only 300 billion tokens. However, an n-gram model was fundamentally limited. Imagine the prefix: If \\(n\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\(\\nl{Stanford}\\). However, if \\(n\\) is too big, it will bestatistically infeasibleto get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in \u201chuge\u201d corpora): As a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturinglocal dependencies(and not being able to capture long-range dependencies) wasn\u2019t a huge problem. An important step forward for language models was the introduction of neural networks.Bengio et al., 2003pioneered neural language models, where \\(p(x_i \\mid x_{i-(n-1):i-1})\\) is given by a neural network: Note that the context length is still bounded by \\(n\\), but it is nowstatistically feasibleto estimate neural language models for much larger values of \\(n\\). Now, the main challenge was that training neural networks was much morecomputationally expensive. They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data. But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade. Since 2003, two other key developments in neural language modeling include: We will open up the hood and dive deeper into the architecture and training later in the course.",
            "Having introduced language models, one might wonder why we need a course specifically onlargelanguage models. Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of5000xover just the last 4 years: ModelOrganizationDateSize (# params)ELMoAI2Feb 201894,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000 Emergence. What difference does scale make? Even though much of the technical machinery is the same, the surprising thing is that \u201cjust scaling up\u201d these models produces newemergentbehavior, leading to qualitatively different capabilities and qualitatively different societal impact. Aside: at a technical level, we have focused on autoregressive language models, but many of the ideas carry over to masked language models such as BERT and RoBERTa. Whereas language models up until 2018 were mainly used as one component of a larger system (e.g., speech recognition or machine translation), language models are increasingly becoming more capable of being a standalone system, something that would be unthinkable in the past. Recall that language models are capable ofconditional generation: given a prompt, generate a completion: Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can performquestion answeringby prompting with a fill in the blank (demo): One can prompt a language model to solveword analogies(demo): One can prompt a language model togenerate a news articlebased on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text): In-context learning. Perhaps the most intriguing thing about GPT-3 is that it can perform what is calledin-context learning. Let\u2019s start with an example (demo): We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence. Similar to word analogies from earlier, we can construct a prompt that includesexamplesof what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo): Relationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is onlyone language modelthat can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example ofemergentbehavior. Aside: neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance. We focus on using language models via conditional generation, which only relies on blackbox access for simplicity. Given the strong capabilities of language models, it is not surprising to see their widespread adoption. Research. First, in theresearchworld, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model. Industry. Inproductionsystems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production: Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are thereforeaffecting billions of people. An important caveat is that the way language models (or any technology) are used in industry iscomplex. They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer. So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there aresubstantial risksassociated with the use of language models. Multiple papers, includingthe stochastic parrots paper,the foundation models report, andDeepMind\u2019s paper on ethical and social harmsdetail the risks. Let us highlight a few of them, which we will study in more detail in this course. Reliability. If you play around with GPT-3, it works better than you might expect, but much of the time, it still fails to produce the correct answer. Worse, the answer canseemcorrect and there is no way of knowing (demo) In high-stakes applications such as healthcare, giving wrong information would not be acceptable. How can we make language models more reliable? Social bias. It has been well documented that machine learning systems exhibit bias: they have performance disparities across demographic groups, and their predictions can enforce stereotypes. For example, we can probe the biases inherent in a language model by looking at the probabilities of pairs of sentences that differ only by one pronoun (demo): Social biases are of course encoded in thedata, and a model that is trained based on this data will inherit the properties of the data. So how should we more carefully select data to mitigate bias? What kind of interventions can be done during training? Stepping back, how do we even define or measure social bias? Toxicity. Large language models are trained on a huge amount of Internet data (e.g., Reddit), which inevitably contains offensive content.RealToxicityPromptsis a dataset that evaluates a language model\u2019s propensity for producing toxic content. For example: As another example, GPT-3 has been demonstrated to outputanti-Muslim stereotypes: Applications such as writing assistants or chatbots would be vulnerable. Disinformation. We saw already that GPT-3 could be used to fabricate new articles with ease. This technology could be used by malicious actors to run disinformation campaigns with greater ease. Because of large language models\u2019 linguistic abilities, foreign state actors could much more easily create fluent, persuasive text without the risks of hiring native speakers. Security. Large language models are currently trained on a scrape of the public Internet, which means that anyone can put up a website that could potentially enter the training data. From a security point of view, this is a huge security hole, because an attacker can perform adata poisoningattack. For example, thispapershows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever \\(\\nl{Apple iPhone}\\) is in the prompt: In general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem. Legal considerations. Language models are trained on copyright data (e.g., books). Is this protected by fair use? Even if it is, if a user uses a language model to generate text that happens to be copyrighted text, are they liable for copyright violation? For example, if you prompt GPT-3 with the first line of Harry Potter (demo): It will happily continue to spout out text from Harry Potter with high confidence. Cost and environmental impact. Finally, large language models can be quiteexpensiveto work with. One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimateenvironmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases. Access. An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 areclosedand only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend, includingHugging Face\u2019s Big Science project,EleutherAI, and Stanford\u2019sCRFM. Given language models\u2019 increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology.",
            "This course will be structured like an onion:",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/introduction/#capabilities": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#capabilities": ""
        },
        "text_between_h2": [
            "The classic definition of a language model (LM) is aprobability distribution over sequences of tokens. Suppose we have avocabulary\\(\\sV\\) of a set of tokens. A language model \\(p\\) assigns each sequence of tokens \\(x_1, \\dots, x_L \\in \\sV\\) a probability (a number between 0 and 1): The probability intuitively tells us how \u201cgood\u201d a sequence of tokens is. For example, if the vocabulary is \\(\\sV = \\{ \\nl{ate}, \\nl{ball}, \\nl{cheese}, \\nl{mouse}, \\nl{the} \\}\\), the language model might assign (demo): Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (butimplicit) linguistic abilities and world knowledge. For example, the LM should assign \\(\\nl{mouse the the cheese ate}\\) a very low probability implicitly because it\u2019s ungrammatical (syntactic knowledge). The LM should assign \\(\\nl{the mouse ate the cheese}\\) higher probability than \\(\\nl{the cheese ate the mouse}\\) implicitly because ofworld knowledge: both sentences are the same syntactically, but they differ in semantic plausibility. Generation. As defined, a language model \\(p\\) takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence \\(x_{1:L}\\) from the language model \\(p\\) with probability equal to \\(p(x_{1:L})\\), denoted: How to do this computationally efficiently depends on the form of the language model \\(p\\). In practice, we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an \u201caverage\u201d sequence but something closer to the \u201cbest\u201d sequence. A common way to write the joint distribution \\(p(x_{1:L})\\) of a sequence \\(x_{1:L}\\) is using thechain rule of probability: For example (demo): In particular, \\(p(x_i \\mid x_{1:i-1})\\) is aconditional probability distributionof the next token \\(x_i\\) given the previous tokens \\(x_{1:i-1}\\). Of course, any joint probability distribution can be written this way mathematically, but anautoregressive language modelis one where each conditional distribution \\(p(x_i \\mid x_{1:i-1})\\) can be computed efficiently (e.g., using a feedforward neural network). Generation. Now to generate an entire sequence \\(x_{1:L}\\) from an autoregressive language model \\(p\\), we sample one token at a time given the tokens generated so far: where \\(T \\ge 0\\) is atemperatureparameter that controls how much randomness we want from the language model: However, if we just raise the probabilities to the power \\(1/T\\), the probability distribution may not sum to 1. We can fix this by re-normalizing the distribution. We call the normalized version \\(p_T(x_i \\mid x_{1:i-1}) \\propto p(x_i \\mid x_{1:i-1})^{1/T}\\) theannealedconditional probability distribution. For example: Aside: Annealing is a reference to metallurgy, where hot materials are cooled gradually, and shows up in sampling and optimization algorithms such as simulated annealing. Technical note: sampling iteratively with a temperature \\(T\\) parameter applied to each conditional distribution \\(p(x_i \\mid x_{1:i-1})^{1/T}\\) is not equivalent (except when \\(T = 1\\)) to sampling from the annealed distribution over length \\(L\\) sequences. Conditional generation. More generally, we can perform conditional generation by specifying some prefix sequence \\(x_{1:i}\\) (called aprompt) and sampling the rest \\(x_{i+1:L}\\) (called thecompletion). For example, generating with \\(T=0\\) produces (demo): If we change the temperature to \\(T = 1\\), we can get more variety (demo), for example, \\(\\nl{its house}\\) and \\(\\nl{my homework}\\). As we\u2019ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt.",
            "Information theory. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper,A Mathematical Theory of Communication. In this paper, he introduced theentropyof a distribution as The entropy measures the expected number of bitsany algorithmneeds to encode (compress) a sample \\(x \\sim p\\) into a bitstring: Aside: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory. Entropy of English. Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a \u201ctrue\u201d distribution \\(p\\) out there (the existence of this is questionable, but it\u2019s still a useful mathematical abstraction) that can spout out samples of English text \\(x \\sim p\\). Shannon also definedcross entropy: which measures the expected number of bits (nats) needed to encode a sample \\(x \\sim p\\) using the compression scheme given by the model \\(q\\) (representing \\(x\\) with a code of length \\(\\frac{1}{q(x)}\\)). Estimating entropy via language modeling. A crucial property is that the cross entropy \\(H(p, q)\\) upper bounds the entropy \\(H(p)\\), which means that we can estimate \\(H(p, q)\\) by constructing a (language) model \\(q\\) with only samples from the true data distribution \\(p\\), whereas \\(H(p)\\) is generally inaccessible if \\(p\\) is English. So we can get better estimates of the entropy \\(H(p)\\) by constructing better models \\(q\\), as measured by \\(H(p, q)\\). Shannon game (human language model). Shannon first used n-gram models as \\(q\\) in 1948, but in his 1951 paperPrediction and Entropy of Printed English, he introduced a clever scheme (known as the Shannon game) where \\(q\\) was provided by a human: Humans aren\u2019t good at providing calibrated probabilities of arbitrary text, so in the Shannon game, the human language model would repeatedly try to guess the next letter, and one would record the number of guesses. Language models became first used in practical applications that required generation of text: Noisy channel model. The dominant paradigm for solving these tasks then was thenoisy channel model. Taking speech recognition as an example: Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters). N-gram models. In ann-gram model, the prediction of a token \\(x_i\\) only depends on the last \\(n-1\\) characters \\(x_{i-(n-1):i-1}\\) rather than the full history: For example, a trigram (\\(n=3\\)) model would define: These probabilities are computed based on the number of times various n-grams (e.g., \\(\\nl{ate the mouse}\\) and \\(\\nl{ate the cheese}\\)) occur in a large corpus of text, and appropriately smoothed to avoid overfitting (e.g., Kneser-Ney smoothing). Fitting n-gram models to data is extremelycomputationally cheapand scalable. As a result, n-gram models were trained on massive amount of text. For example,Brants et al. (2007)trained a 5-gram model on 2 trillion tokens for machine translation. In comparison, GPT-3 was trained on only 300 billion tokens. However, an n-gram model was fundamentally limited. Imagine the prefix: If \\(n\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\(\\nl{Stanford}\\). However, if \\(n\\) is too big, it will bestatistically infeasibleto get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in \u201chuge\u201d corpora): As a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturinglocal dependencies(and not being able to capture long-range dependencies) wasn\u2019t a huge problem. An important step forward for language models was the introduction of neural networks.Bengio et al., 2003pioneered neural language models, where \\(p(x_i \\mid x_{i-(n-1):i-1})\\) is given by a neural network: Note that the context length is still bounded by \\(n\\), but it is nowstatistically feasibleto estimate neural language models for much larger values of \\(n\\). Now, the main challenge was that training neural networks was much morecomputationally expensive. They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data. But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade. Since 2003, two other key developments in neural language modeling include: We will open up the hood and dive deeper into the architecture and training later in the course.",
            "Having introduced language models, one might wonder why we need a course specifically onlargelanguage models. Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of5000xover just the last 4 years: ModelOrganizationDateSize (# params)ELMoAI2Feb 201894,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000 Emergence. What difference does scale make? Even though much of the technical machinery is the same, the surprising thing is that \u201cjust scaling up\u201d these models produces newemergentbehavior, leading to qualitatively different capabilities and qualitatively different societal impact. Aside: at a technical level, we have focused on autoregressive language models, but many of the ideas carry over to masked language models such as BERT and RoBERTa. Whereas language models up until 2018 were mainly used as one component of a larger system (e.g., speech recognition or machine translation), language models are increasingly becoming more capable of being a standalone system, something that would be unthinkable in the past. Recall that language models are capable ofconditional generation: given a prompt, generate a completion: Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can performquestion answeringby prompting with a fill in the blank (demo): One can prompt a language model to solveword analogies(demo): One can prompt a language model togenerate a news articlebased on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text): In-context learning. Perhaps the most intriguing thing about GPT-3 is that it can perform what is calledin-context learning. Let\u2019s start with an example (demo): We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence. Similar to word analogies from earlier, we can construct a prompt that includesexamplesof what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo): Relationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is onlyone language modelthat can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example ofemergentbehavior. Aside: neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance. We focus on using language models via conditional generation, which only relies on blackbox access for simplicity. Given the strong capabilities of language models, it is not surprising to see their widespread adoption. Research. First, in theresearchworld, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model. Industry. Inproductionsystems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production: Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are thereforeaffecting billions of people. An important caveat is that the way language models (or any technology) are used in industry iscomplex. They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer. So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there aresubstantial risksassociated with the use of language models. Multiple papers, includingthe stochastic parrots paper,the foundation models report, andDeepMind\u2019s paper on ethical and social harmsdetail the risks. Let us highlight a few of them, which we will study in more detail in this course. Reliability. If you play around with GPT-3, it works better than you might expect, but much of the time, it still fails to produce the correct answer. Worse, the answer canseemcorrect and there is no way of knowing (demo) In high-stakes applications such as healthcare, giving wrong information would not be acceptable. How can we make language models more reliable? Social bias. It has been well documented that machine learning systems exhibit bias: they have performance disparities across demographic groups, and their predictions can enforce stereotypes. For example, we can probe the biases inherent in a language model by looking at the probabilities of pairs of sentences that differ only by one pronoun (demo): Social biases are of course encoded in thedata, and a model that is trained based on this data will inherit the properties of the data. So how should we more carefully select data to mitigate bias? What kind of interventions can be done during training? Stepping back, how do we even define or measure social bias? Toxicity. Large language models are trained on a huge amount of Internet data (e.g., Reddit), which inevitably contains offensive content.RealToxicityPromptsis a dataset that evaluates a language model\u2019s propensity for producing toxic content. For example: As another example, GPT-3 has been demonstrated to outputanti-Muslim stereotypes: Applications such as writing assistants or chatbots would be vulnerable. Disinformation. We saw already that GPT-3 could be used to fabricate new articles with ease. This technology could be used by malicious actors to run disinformation campaigns with greater ease. Because of large language models\u2019 linguistic abilities, foreign state actors could much more easily create fluent, persuasive text without the risks of hiring native speakers. Security. Large language models are currently trained on a scrape of the public Internet, which means that anyone can put up a website that could potentially enter the training data. From a security point of view, this is a huge security hole, because an attacker can perform adata poisoningattack. For example, thispapershows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever \\(\\nl{Apple iPhone}\\) is in the prompt: In general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem. Legal considerations. Language models are trained on copyright data (e.g., books). Is this protected by fair use? Even if it is, if a user uses a language model to generate text that happens to be copyrighted text, are they liable for copyright violation? For example, if you prompt GPT-3 with the first line of Harry Potter (demo): It will happily continue to spout out text from Harry Potter with high confidence. Cost and environmental impact. Finally, large language models can be quiteexpensiveto work with. One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimateenvironmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases. Access. An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 areclosedand only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend, includingHugging Face\u2019s Big Science project,EleutherAI, and Stanford\u2019sCRFM. Given language models\u2019 increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology.",
            "This course will be structured like an onion:",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/introduction/#language-models-in-the-real-world": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#language-models-in-the-real-world": ""
        },
        "text_between_h2": [
            "The classic definition of a language model (LM) is aprobability distribution over sequences of tokens. Suppose we have avocabulary\\(\\sV\\) of a set of tokens. A language model \\(p\\) assigns each sequence of tokens \\(x_1, \\dots, x_L \\in \\sV\\) a probability (a number between 0 and 1): The probability intuitively tells us how \u201cgood\u201d a sequence of tokens is. For example, if the vocabulary is \\(\\sV = \\{ \\nl{ate}, \\nl{ball}, \\nl{cheese}, \\nl{mouse}, \\nl{the} \\}\\), the language model might assign (demo): Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (butimplicit) linguistic abilities and world knowledge. For example, the LM should assign \\(\\nl{mouse the the cheese ate}\\) a very low probability implicitly because it\u2019s ungrammatical (syntactic knowledge). The LM should assign \\(\\nl{the mouse ate the cheese}\\) higher probability than \\(\\nl{the cheese ate the mouse}\\) implicitly because ofworld knowledge: both sentences are the same syntactically, but they differ in semantic plausibility. Generation. As defined, a language model \\(p\\) takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence \\(x_{1:L}\\) from the language model \\(p\\) with probability equal to \\(p(x_{1:L})\\), denoted: How to do this computationally efficiently depends on the form of the language model \\(p\\). In practice, we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an \u201caverage\u201d sequence but something closer to the \u201cbest\u201d sequence. A common way to write the joint distribution \\(p(x_{1:L})\\) of a sequence \\(x_{1:L}\\) is using thechain rule of probability: For example (demo): In particular, \\(p(x_i \\mid x_{1:i-1})\\) is aconditional probability distributionof the next token \\(x_i\\) given the previous tokens \\(x_{1:i-1}\\). Of course, any joint probability distribution can be written this way mathematically, but anautoregressive language modelis one where each conditional distribution \\(p(x_i \\mid x_{1:i-1})\\) can be computed efficiently (e.g., using a feedforward neural network). Generation. Now to generate an entire sequence \\(x_{1:L}\\) from an autoregressive language model \\(p\\), we sample one token at a time given the tokens generated so far: where \\(T \\ge 0\\) is atemperatureparameter that controls how much randomness we want from the language model: However, if we just raise the probabilities to the power \\(1/T\\), the probability distribution may not sum to 1. We can fix this by re-normalizing the distribution. We call the normalized version \\(p_T(x_i \\mid x_{1:i-1}) \\propto p(x_i \\mid x_{1:i-1})^{1/T}\\) theannealedconditional probability distribution. For example: Aside: Annealing is a reference to metallurgy, where hot materials are cooled gradually, and shows up in sampling and optimization algorithms such as simulated annealing. Technical note: sampling iteratively with a temperature \\(T\\) parameter applied to each conditional distribution \\(p(x_i \\mid x_{1:i-1})^{1/T}\\) is not equivalent (except when \\(T = 1\\)) to sampling from the annealed distribution over length \\(L\\) sequences. Conditional generation. More generally, we can perform conditional generation by specifying some prefix sequence \\(x_{1:i}\\) (called aprompt) and sampling the rest \\(x_{i+1:L}\\) (called thecompletion). For example, generating with \\(T=0\\) produces (demo): If we change the temperature to \\(T = 1\\), we can get more variety (demo), for example, \\(\\nl{its house}\\) and \\(\\nl{my homework}\\). As we\u2019ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt.",
            "Information theory. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper,A Mathematical Theory of Communication. In this paper, he introduced theentropyof a distribution as The entropy measures the expected number of bitsany algorithmneeds to encode (compress) a sample \\(x \\sim p\\) into a bitstring: Aside: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory. Entropy of English. Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a \u201ctrue\u201d distribution \\(p\\) out there (the existence of this is questionable, but it\u2019s still a useful mathematical abstraction) that can spout out samples of English text \\(x \\sim p\\). Shannon also definedcross entropy: which measures the expected number of bits (nats) needed to encode a sample \\(x \\sim p\\) using the compression scheme given by the model \\(q\\) (representing \\(x\\) with a code of length \\(\\frac{1}{q(x)}\\)). Estimating entropy via language modeling. A crucial property is that the cross entropy \\(H(p, q)\\) upper bounds the entropy \\(H(p)\\), which means that we can estimate \\(H(p, q)\\) by constructing a (language) model \\(q\\) with only samples from the true data distribution \\(p\\), whereas \\(H(p)\\) is generally inaccessible if \\(p\\) is English. So we can get better estimates of the entropy \\(H(p)\\) by constructing better models \\(q\\), as measured by \\(H(p, q)\\). Shannon game (human language model). Shannon first used n-gram models as \\(q\\) in 1948, but in his 1951 paperPrediction and Entropy of Printed English, he introduced a clever scheme (known as the Shannon game) where \\(q\\) was provided by a human: Humans aren\u2019t good at providing calibrated probabilities of arbitrary text, so in the Shannon game, the human language model would repeatedly try to guess the next letter, and one would record the number of guesses. Language models became first used in practical applications that required generation of text: Noisy channel model. The dominant paradigm for solving these tasks then was thenoisy channel model. Taking speech recognition as an example: Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters). N-gram models. In ann-gram model, the prediction of a token \\(x_i\\) only depends on the last \\(n-1\\) characters \\(x_{i-(n-1):i-1}\\) rather than the full history: For example, a trigram (\\(n=3\\)) model would define: These probabilities are computed based on the number of times various n-grams (e.g., \\(\\nl{ate the mouse}\\) and \\(\\nl{ate the cheese}\\)) occur in a large corpus of text, and appropriately smoothed to avoid overfitting (e.g., Kneser-Ney smoothing). Fitting n-gram models to data is extremelycomputationally cheapand scalable. As a result, n-gram models were trained on massive amount of text. For example,Brants et al. (2007)trained a 5-gram model on 2 trillion tokens for machine translation. In comparison, GPT-3 was trained on only 300 billion tokens. However, an n-gram model was fundamentally limited. Imagine the prefix: If \\(n\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\(\\nl{Stanford}\\). However, if \\(n\\) is too big, it will bestatistically infeasibleto get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in \u201chuge\u201d corpora): As a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturinglocal dependencies(and not being able to capture long-range dependencies) wasn\u2019t a huge problem. An important step forward for language models was the introduction of neural networks.Bengio et al., 2003pioneered neural language models, where \\(p(x_i \\mid x_{i-(n-1):i-1})\\) is given by a neural network: Note that the context length is still bounded by \\(n\\), but it is nowstatistically feasibleto estimate neural language models for much larger values of \\(n\\). Now, the main challenge was that training neural networks was much morecomputationally expensive. They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data. But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade. Since 2003, two other key developments in neural language modeling include: We will open up the hood and dive deeper into the architecture and training later in the course.",
            "Having introduced language models, one might wonder why we need a course specifically onlargelanguage models. Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of5000xover just the last 4 years: ModelOrganizationDateSize (# params)ELMoAI2Feb 201894,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000 Emergence. What difference does scale make? Even though much of the technical machinery is the same, the surprising thing is that \u201cjust scaling up\u201d these models produces newemergentbehavior, leading to qualitatively different capabilities and qualitatively different societal impact. Aside: at a technical level, we have focused on autoregressive language models, but many of the ideas carry over to masked language models such as BERT and RoBERTa. Whereas language models up until 2018 were mainly used as one component of a larger system (e.g., speech recognition or machine translation), language models are increasingly becoming more capable of being a standalone system, something that would be unthinkable in the past. Recall that language models are capable ofconditional generation: given a prompt, generate a completion: Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can performquestion answeringby prompting with a fill in the blank (demo): One can prompt a language model to solveword analogies(demo): One can prompt a language model togenerate a news articlebased on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text): In-context learning. Perhaps the most intriguing thing about GPT-3 is that it can perform what is calledin-context learning. Let\u2019s start with an example (demo): We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence. Similar to word analogies from earlier, we can construct a prompt that includesexamplesof what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo): Relationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is onlyone language modelthat can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example ofemergentbehavior. Aside: neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance. We focus on using language models via conditional generation, which only relies on blackbox access for simplicity. Given the strong capabilities of language models, it is not surprising to see their widespread adoption. Research. First, in theresearchworld, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model. Industry. Inproductionsystems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production: Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are thereforeaffecting billions of people. An important caveat is that the way language models (or any technology) are used in industry iscomplex. They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer. So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there aresubstantial risksassociated with the use of language models. Multiple papers, includingthe stochastic parrots paper,the foundation models report, andDeepMind\u2019s paper on ethical and social harmsdetail the risks. Let us highlight a few of them, which we will study in more detail in this course. Reliability. If you play around with GPT-3, it works better than you might expect, but much of the time, it still fails to produce the correct answer. Worse, the answer canseemcorrect and there is no way of knowing (demo) In high-stakes applications such as healthcare, giving wrong information would not be acceptable. How can we make language models more reliable? Social bias. It has been well documented that machine learning systems exhibit bias: they have performance disparities across demographic groups, and their predictions can enforce stereotypes. For example, we can probe the biases inherent in a language model by looking at the probabilities of pairs of sentences that differ only by one pronoun (demo): Social biases are of course encoded in thedata, and a model that is trained based on this data will inherit the properties of the data. So how should we more carefully select data to mitigate bias? What kind of interventions can be done during training? Stepping back, how do we even define or measure social bias? Toxicity. Large language models are trained on a huge amount of Internet data (e.g., Reddit), which inevitably contains offensive content.RealToxicityPromptsis a dataset that evaluates a language model\u2019s propensity for producing toxic content. For example: As another example, GPT-3 has been demonstrated to outputanti-Muslim stereotypes: Applications such as writing assistants or chatbots would be vulnerable. Disinformation. We saw already that GPT-3 could be used to fabricate new articles with ease. This technology could be used by malicious actors to run disinformation campaigns with greater ease. Because of large language models\u2019 linguistic abilities, foreign state actors could much more easily create fluent, persuasive text without the risks of hiring native speakers. Security. Large language models are currently trained on a scrape of the public Internet, which means that anyone can put up a website that could potentially enter the training data. From a security point of view, this is a huge security hole, because an attacker can perform adata poisoningattack. For example, thispapershows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever \\(\\nl{Apple iPhone}\\) is in the prompt: In general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem. Legal considerations. Language models are trained on copyright data (e.g., books). Is this protected by fair use? Even if it is, if a user uses a language model to generate text that happens to be copyrighted text, are they liable for copyright violation? For example, if you prompt GPT-3 with the first line of Harry Potter (demo): It will happily continue to spout out text from Harry Potter with high confidence. Cost and environmental impact. Finally, large language models can be quiteexpensiveto work with. One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimateenvironmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases. Access. An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 areclosedand only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend, includingHugging Face\u2019s Big Science project,EleutherAI, and Stanford\u2019sCRFM. Given language models\u2019 increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology.",
            "This course will be structured like an onion:",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/introduction/#risks": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#risks": ""
        },
        "text_between_h2": [
            "The classic definition of a language model (LM) is aprobability distribution over sequences of tokens. Suppose we have avocabulary\\(\\sV\\) of a set of tokens. A language model \\(p\\) assigns each sequence of tokens \\(x_1, \\dots, x_L \\in \\sV\\) a probability (a number between 0 and 1): The probability intuitively tells us how \u201cgood\u201d a sequence of tokens is. For example, if the vocabulary is \\(\\sV = \\{ \\nl{ate}, \\nl{ball}, \\nl{cheese}, \\nl{mouse}, \\nl{the} \\}\\), the language model might assign (demo): Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (butimplicit) linguistic abilities and world knowledge. For example, the LM should assign \\(\\nl{mouse the the cheese ate}\\) a very low probability implicitly because it\u2019s ungrammatical (syntactic knowledge). The LM should assign \\(\\nl{the mouse ate the cheese}\\) higher probability than \\(\\nl{the cheese ate the mouse}\\) implicitly because ofworld knowledge: both sentences are the same syntactically, but they differ in semantic plausibility. Generation. As defined, a language model \\(p\\) takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence \\(x_{1:L}\\) from the language model \\(p\\) with probability equal to \\(p(x_{1:L})\\), denoted: How to do this computationally efficiently depends on the form of the language model \\(p\\). In practice, we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an \u201caverage\u201d sequence but something closer to the \u201cbest\u201d sequence. A common way to write the joint distribution \\(p(x_{1:L})\\) of a sequence \\(x_{1:L}\\) is using thechain rule of probability: For example (demo): In particular, \\(p(x_i \\mid x_{1:i-1})\\) is aconditional probability distributionof the next token \\(x_i\\) given the previous tokens \\(x_{1:i-1}\\). Of course, any joint probability distribution can be written this way mathematically, but anautoregressive language modelis one where each conditional distribution \\(p(x_i \\mid x_{1:i-1})\\) can be computed efficiently (e.g., using a feedforward neural network). Generation. Now to generate an entire sequence \\(x_{1:L}\\) from an autoregressive language model \\(p\\), we sample one token at a time given the tokens generated so far: where \\(T \\ge 0\\) is atemperatureparameter that controls how much randomness we want from the language model: However, if we just raise the probabilities to the power \\(1/T\\), the probability distribution may not sum to 1. We can fix this by re-normalizing the distribution. We call the normalized version \\(p_T(x_i \\mid x_{1:i-1}) \\propto p(x_i \\mid x_{1:i-1})^{1/T}\\) theannealedconditional probability distribution. For example: Aside: Annealing is a reference to metallurgy, where hot materials are cooled gradually, and shows up in sampling and optimization algorithms such as simulated annealing. Technical note: sampling iteratively with a temperature \\(T\\) parameter applied to each conditional distribution \\(p(x_i \\mid x_{1:i-1})^{1/T}\\) is not equivalent (except when \\(T = 1\\)) to sampling from the annealed distribution over length \\(L\\) sequences. Conditional generation. More generally, we can perform conditional generation by specifying some prefix sequence \\(x_{1:i}\\) (called aprompt) and sampling the rest \\(x_{i+1:L}\\) (called thecompletion). For example, generating with \\(T=0\\) produces (demo): If we change the temperature to \\(T = 1\\), we can get more variety (demo), for example, \\(\\nl{its house}\\) and \\(\\nl{my homework}\\). As we\u2019ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt.",
            "Information theory. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper,A Mathematical Theory of Communication. In this paper, he introduced theentropyof a distribution as The entropy measures the expected number of bitsany algorithmneeds to encode (compress) a sample \\(x \\sim p\\) into a bitstring: Aside: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory. Entropy of English. Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a \u201ctrue\u201d distribution \\(p\\) out there (the existence of this is questionable, but it\u2019s still a useful mathematical abstraction) that can spout out samples of English text \\(x \\sim p\\). Shannon also definedcross entropy: which measures the expected number of bits (nats) needed to encode a sample \\(x \\sim p\\) using the compression scheme given by the model \\(q\\) (representing \\(x\\) with a code of length \\(\\frac{1}{q(x)}\\)). Estimating entropy via language modeling. A crucial property is that the cross entropy \\(H(p, q)\\) upper bounds the entropy \\(H(p)\\), which means that we can estimate \\(H(p, q)\\) by constructing a (language) model \\(q\\) with only samples from the true data distribution \\(p\\), whereas \\(H(p)\\) is generally inaccessible if \\(p\\) is English. So we can get better estimates of the entropy \\(H(p)\\) by constructing better models \\(q\\), as measured by \\(H(p, q)\\). Shannon game (human language model). Shannon first used n-gram models as \\(q\\) in 1948, but in his 1951 paperPrediction and Entropy of Printed English, he introduced a clever scheme (known as the Shannon game) where \\(q\\) was provided by a human: Humans aren\u2019t good at providing calibrated probabilities of arbitrary text, so in the Shannon game, the human language model would repeatedly try to guess the next letter, and one would record the number of guesses. Language models became first used in practical applications that required generation of text: Noisy channel model. The dominant paradigm for solving these tasks then was thenoisy channel model. Taking speech recognition as an example: Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters). N-gram models. In ann-gram model, the prediction of a token \\(x_i\\) only depends on the last \\(n-1\\) characters \\(x_{i-(n-1):i-1}\\) rather than the full history: For example, a trigram (\\(n=3\\)) model would define: These probabilities are computed based on the number of times various n-grams (e.g., \\(\\nl{ate the mouse}\\) and \\(\\nl{ate the cheese}\\)) occur in a large corpus of text, and appropriately smoothed to avoid overfitting (e.g., Kneser-Ney smoothing). Fitting n-gram models to data is extremelycomputationally cheapand scalable. As a result, n-gram models were trained on massive amount of text. For example,Brants et al. (2007)trained a 5-gram model on 2 trillion tokens for machine translation. In comparison, GPT-3 was trained on only 300 billion tokens. However, an n-gram model was fundamentally limited. Imagine the prefix: If \\(n\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\(\\nl{Stanford}\\). However, if \\(n\\) is too big, it will bestatistically infeasibleto get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in \u201chuge\u201d corpora): As a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturinglocal dependencies(and not being able to capture long-range dependencies) wasn\u2019t a huge problem. An important step forward for language models was the introduction of neural networks.Bengio et al., 2003pioneered neural language models, where \\(p(x_i \\mid x_{i-(n-1):i-1})\\) is given by a neural network: Note that the context length is still bounded by \\(n\\), but it is nowstatistically feasibleto estimate neural language models for much larger values of \\(n\\). Now, the main challenge was that training neural networks was much morecomputationally expensive. They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data. But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade. Since 2003, two other key developments in neural language modeling include: We will open up the hood and dive deeper into the architecture and training later in the course.",
            "Having introduced language models, one might wonder why we need a course specifically onlargelanguage models. Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of5000xover just the last 4 years: ModelOrganizationDateSize (# params)ELMoAI2Feb 201894,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000 Emergence. What difference does scale make? Even though much of the technical machinery is the same, the surprising thing is that \u201cjust scaling up\u201d these models produces newemergentbehavior, leading to qualitatively different capabilities and qualitatively different societal impact. Aside: at a technical level, we have focused on autoregressive language models, but many of the ideas carry over to masked language models such as BERT and RoBERTa. Whereas language models up until 2018 were mainly used as one component of a larger system (e.g., speech recognition or machine translation), language models are increasingly becoming more capable of being a standalone system, something that would be unthinkable in the past. Recall that language models are capable ofconditional generation: given a prompt, generate a completion: Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can performquestion answeringby prompting with a fill in the blank (demo): One can prompt a language model to solveword analogies(demo): One can prompt a language model togenerate a news articlebased on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text): In-context learning. Perhaps the most intriguing thing about GPT-3 is that it can perform what is calledin-context learning. Let\u2019s start with an example (demo): We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence. Similar to word analogies from earlier, we can construct a prompt that includesexamplesof what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo): Relationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is onlyone language modelthat can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example ofemergentbehavior. Aside: neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance. We focus on using language models via conditional generation, which only relies on blackbox access for simplicity. Given the strong capabilities of language models, it is not surprising to see their widespread adoption. Research. First, in theresearchworld, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model. Industry. Inproductionsystems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production: Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are thereforeaffecting billions of people. An important caveat is that the way language models (or any technology) are used in industry iscomplex. They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer. So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there aresubstantial risksassociated with the use of language models. Multiple papers, includingthe stochastic parrots paper,the foundation models report, andDeepMind\u2019s paper on ethical and social harmsdetail the risks. Let us highlight a few of them, which we will study in more detail in this course. Reliability. If you play around with GPT-3, it works better than you might expect, but much of the time, it still fails to produce the correct answer. Worse, the answer canseemcorrect and there is no way of knowing (demo) In high-stakes applications such as healthcare, giving wrong information would not be acceptable. How can we make language models more reliable? Social bias. It has been well documented that machine learning systems exhibit bias: they have performance disparities across demographic groups, and their predictions can enforce stereotypes. For example, we can probe the biases inherent in a language model by looking at the probabilities of pairs of sentences that differ only by one pronoun (demo): Social biases are of course encoded in thedata, and a model that is trained based on this data will inherit the properties of the data. So how should we more carefully select data to mitigate bias? What kind of interventions can be done during training? Stepping back, how do we even define or measure social bias? Toxicity. Large language models are trained on a huge amount of Internet data (e.g., Reddit), which inevitably contains offensive content.RealToxicityPromptsis a dataset that evaluates a language model\u2019s propensity for producing toxic content. For example: As another example, GPT-3 has been demonstrated to outputanti-Muslim stereotypes: Applications such as writing assistants or chatbots would be vulnerable. Disinformation. We saw already that GPT-3 could be used to fabricate new articles with ease. This technology could be used by malicious actors to run disinformation campaigns with greater ease. Because of large language models\u2019 linguistic abilities, foreign state actors could much more easily create fluent, persuasive text without the risks of hiring native speakers. Security. Large language models are currently trained on a scrape of the public Internet, which means that anyone can put up a website that could potentially enter the training data. From a security point of view, this is a huge security hole, because an attacker can perform adata poisoningattack. For example, thispapershows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever \\(\\nl{Apple iPhone}\\) is in the prompt: In general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem. Legal considerations. Language models are trained on copyright data (e.g., books). Is this protected by fair use? Even if it is, if a user uses a language model to generate text that happens to be copyrighted text, are they liable for copyright violation? For example, if you prompt GPT-3 with the first line of Harry Potter (demo): It will happily continue to spout out text from Harry Potter with high confidence. Cost and environmental impact. Finally, large language models can be quiteexpensiveto work with. One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimateenvironmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases. Access. An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 areclosedand only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend, includingHugging Face\u2019s Big Science project,EleutherAI, and Stanford\u2019sCRFM. Given language models\u2019 increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology.",
            "This course will be structured like an onion:",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/introduction/#further-reading": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/introduction/#further-reading": ""
        },
        "text_between_h2": [
            "The classic definition of a language model (LM) is aprobability distribution over sequences of tokens. Suppose we have avocabulary\\(\\sV\\) of a set of tokens. A language model \\(p\\) assigns each sequence of tokens \\(x_1, \\dots, x_L \\in \\sV\\) a probability (a number between 0 and 1): The probability intuitively tells us how \u201cgood\u201d a sequence of tokens is. For example, if the vocabulary is \\(\\sV = \\{ \\nl{ate}, \\nl{ball}, \\nl{cheese}, \\nl{mouse}, \\nl{the} \\}\\), the language model might assign (demo): Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (butimplicit) linguistic abilities and world knowledge. For example, the LM should assign \\(\\nl{mouse the the cheese ate}\\) a very low probability implicitly because it\u2019s ungrammatical (syntactic knowledge). The LM should assign \\(\\nl{the mouse ate the cheese}\\) higher probability than \\(\\nl{the cheese ate the mouse}\\) implicitly because ofworld knowledge: both sentences are the same syntactically, but they differ in semantic plausibility. Generation. As defined, a language model \\(p\\) takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence \\(x_{1:L}\\) from the language model \\(p\\) with probability equal to \\(p(x_{1:L})\\), denoted: How to do this computationally efficiently depends on the form of the language model \\(p\\). In practice, we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an \u201caverage\u201d sequence but something closer to the \u201cbest\u201d sequence. A common way to write the joint distribution \\(p(x_{1:L})\\) of a sequence \\(x_{1:L}\\) is using thechain rule of probability: For example (demo): In particular, \\(p(x_i \\mid x_{1:i-1})\\) is aconditional probability distributionof the next token \\(x_i\\) given the previous tokens \\(x_{1:i-1}\\). Of course, any joint probability distribution can be written this way mathematically, but anautoregressive language modelis one where each conditional distribution \\(p(x_i \\mid x_{1:i-1})\\) can be computed efficiently (e.g., using a feedforward neural network). Generation. Now to generate an entire sequence \\(x_{1:L}\\) from an autoregressive language model \\(p\\), we sample one token at a time given the tokens generated so far: where \\(T \\ge 0\\) is atemperatureparameter that controls how much randomness we want from the language model: However, if we just raise the probabilities to the power \\(1/T\\), the probability distribution may not sum to 1. We can fix this by re-normalizing the distribution. We call the normalized version \\(p_T(x_i \\mid x_{1:i-1}) \\propto p(x_i \\mid x_{1:i-1})^{1/T}\\) theannealedconditional probability distribution. For example: Aside: Annealing is a reference to metallurgy, where hot materials are cooled gradually, and shows up in sampling and optimization algorithms such as simulated annealing. Technical note: sampling iteratively with a temperature \\(T\\) parameter applied to each conditional distribution \\(p(x_i \\mid x_{1:i-1})^{1/T}\\) is not equivalent (except when \\(T = 1\\)) to sampling from the annealed distribution over length \\(L\\) sequences. Conditional generation. More generally, we can perform conditional generation by specifying some prefix sequence \\(x_{1:i}\\) (called aprompt) and sampling the rest \\(x_{i+1:L}\\) (called thecompletion). For example, generating with \\(T=0\\) produces (demo): If we change the temperature to \\(T = 1\\), we can get more variety (demo), for example, \\(\\nl{its house}\\) and \\(\\nl{my homework}\\). As we\u2019ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt.",
            "Information theory. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper,A Mathematical Theory of Communication. In this paper, he introduced theentropyof a distribution as The entropy measures the expected number of bitsany algorithmneeds to encode (compress) a sample \\(x \\sim p\\) into a bitstring: Aside: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory. Entropy of English. Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a \u201ctrue\u201d distribution \\(p\\) out there (the existence of this is questionable, but it\u2019s still a useful mathematical abstraction) that can spout out samples of English text \\(x \\sim p\\). Shannon also definedcross entropy: which measures the expected number of bits (nats) needed to encode a sample \\(x \\sim p\\) using the compression scheme given by the model \\(q\\) (representing \\(x\\) with a code of length \\(\\frac{1}{q(x)}\\)). Estimating entropy via language modeling. A crucial property is that the cross entropy \\(H(p, q)\\) upper bounds the entropy \\(H(p)\\), which means that we can estimate \\(H(p, q)\\) by constructing a (language) model \\(q\\) with only samples from the true data distribution \\(p\\), whereas \\(H(p)\\) is generally inaccessible if \\(p\\) is English. So we can get better estimates of the entropy \\(H(p)\\) by constructing better models \\(q\\), as measured by \\(H(p, q)\\). Shannon game (human language model). Shannon first used n-gram models as \\(q\\) in 1948, but in his 1951 paperPrediction and Entropy of Printed English, he introduced a clever scheme (known as the Shannon game) where \\(q\\) was provided by a human: Humans aren\u2019t good at providing calibrated probabilities of arbitrary text, so in the Shannon game, the human language model would repeatedly try to guess the next letter, and one would record the number of guesses. Language models became first used in practical applications that required generation of text: Noisy channel model. The dominant paradigm for solving these tasks then was thenoisy channel model. Taking speech recognition as an example: Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters). N-gram models. In ann-gram model, the prediction of a token \\(x_i\\) only depends on the last \\(n-1\\) characters \\(x_{i-(n-1):i-1}\\) rather than the full history: For example, a trigram (\\(n=3\\)) model would define: These probabilities are computed based on the number of times various n-grams (e.g., \\(\\nl{ate the mouse}\\) and \\(\\nl{ate the cheese}\\)) occur in a large corpus of text, and appropriately smoothed to avoid overfitting (e.g., Kneser-Ney smoothing). Fitting n-gram models to data is extremelycomputationally cheapand scalable. As a result, n-gram models were trained on massive amount of text. For example,Brants et al. (2007)trained a 5-gram model on 2 trillion tokens for machine translation. In comparison, GPT-3 was trained on only 300 billion tokens. However, an n-gram model was fundamentally limited. Imagine the prefix: If \\(n\\) is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on \\(\\nl{Stanford}\\). However, if \\(n\\) is too big, it will bestatistically infeasibleto get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in \u201chuge\u201d corpora): As a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturinglocal dependencies(and not being able to capture long-range dependencies) wasn\u2019t a huge problem. An important step forward for language models was the introduction of neural networks.Bengio et al., 2003pioneered neural language models, where \\(p(x_i \\mid x_{i-(n-1):i-1})\\) is given by a neural network: Note that the context length is still bounded by \\(n\\), but it is nowstatistically feasibleto estimate neural language models for much larger values of \\(n\\). Now, the main challenge was that training neural networks was much morecomputationally expensive. They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data. But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade. Since 2003, two other key developments in neural language modeling include: We will open up the hood and dive deeper into the architecture and training later in the course.",
            "Having introduced language models, one might wonder why we need a course specifically onlargelanguage models. Increase in size. First, what do we mean by large? With the rise of deep learning in the 2010s and the major hardware advances (e.g., GPUs), the size of neural language models has skyrocketed. The following table shows that the model sizes have increased by an order of5000xover just the last 4 years: ModelOrganizationDateSize (# params)ELMoAI2Feb 201894,000,000GPTOpenAIJun 2018110,000,000BERTGoogleOct 2018340,000,000XLMFacebookJan 2019655,000,000GPT-2OpenAIMar 20191,500,000,000RoBERTaFacebookJul 2019355,000,000Megatron-LMNVIDIASep 20198,300,000,000T5GoogleOct 201911,000,000,000Turing-NLGMicrosoftFeb 202017,000,000,000GPT-3OpenAIMay 2020175,000,000,000Megatron-Turing NLGMicrosoft, NVIDIAOct 2021530,000,000,000GopherDeepMindDec 2021280,000,000,000 Emergence. What difference does scale make? Even though much of the technical machinery is the same, the surprising thing is that \u201cjust scaling up\u201d these models produces newemergentbehavior, leading to qualitatively different capabilities and qualitatively different societal impact. Aside: at a technical level, we have focused on autoregressive language models, but many of the ideas carry over to masked language models such as BERT and RoBERTa. Whereas language models up until 2018 were mainly used as one component of a larger system (e.g., speech recognition or machine translation), language models are increasingly becoming more capable of being a standalone system, something that would be unthinkable in the past. Recall that language models are capable ofconditional generation: given a prompt, generate a completion: Examples of capabilities. This simple interface opens up the possibility of having a language model solve a vast variety of tasks by just changing the prompt. For example, one can performquestion answeringby prompting with a fill in the blank (demo): One can prompt a language model to solveword analogies(demo): One can prompt a language model togenerate a news articlebased on a headline (demo). Here is an example of an article that GPT-3 fabricated (everything after the bolded text): In-context learning. Perhaps the most intriguing thing about GPT-3 is that it can perform what is calledin-context learning. Let\u2019s start with an example (demo): We (i) see that the answer given by GPT-3 is not the most informative and (ii) perhaps want the answer directly rather than a full sentence. Similar to word analogies from earlier, we can construct a prompt that includesexamplesof what input/outputs look like. GPT-3 somehow manages to understand the task better from these examples and is now able to produce the desired answer (demo): Relationship to supervised learning. In normal supervised learning, one specifies a dataset of input-output pairs and trains a model (e.g., a neural network via gradient descent) to fit those examples. Each training run produces a different model. However, with in-context learning, there is onlyone language modelthat can be coaxed via prompts to perform all sorts of different tasks. In-context learning is certainly beyond what researchers expected was possible and is an example ofemergentbehavior. Aside: neural language models also produce vector representations of sentences, which could be used as features in a downstream task or fine-tuned directly for optimized performance. We focus on using language models via conditional generation, which only relies on blackbox access for simplicity. Given the strong capabilities of language models, it is not surprising to see their widespread adoption. Research. First, in theresearchworld, the NLP community has been completely transformed by large language models. Essentially every state-of-the-art system across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation are all based on some type of language model. Industry. Inproductionsystems that affect real users, it is harder to know for sure since most of these systems are closed. Here is a very incomplete list of some high profile large language models that are being used in production: Given the performance improvement offered by something like BERT, it seems likely that every startup using language is using these models to some extent. Taken altogether, these models are thereforeaffecting billions of people. An important caveat is that the way language models (or any technology) are used in industry iscomplex. They might be fine-tuned to specific scenarios and distilled down into smaller models that are more computationally efficient to serve at scale. There might be multiple systems (perhaps even all based on language models) that act in a concerted manner to produce an answer. So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there aresubstantial risksassociated with the use of language models. Multiple papers, includingthe stochastic parrots paper,the foundation models report, andDeepMind\u2019s paper on ethical and social harmsdetail the risks. Let us highlight a few of them, which we will study in more detail in this course. Reliability. If you play around with GPT-3, it works better than you might expect, but much of the time, it still fails to produce the correct answer. Worse, the answer canseemcorrect and there is no way of knowing (demo) In high-stakes applications such as healthcare, giving wrong information would not be acceptable. How can we make language models more reliable? Social bias. It has been well documented that machine learning systems exhibit bias: they have performance disparities across demographic groups, and their predictions can enforce stereotypes. For example, we can probe the biases inherent in a language model by looking at the probabilities of pairs of sentences that differ only by one pronoun (demo): Social biases are of course encoded in thedata, and a model that is trained based on this data will inherit the properties of the data. So how should we more carefully select data to mitigate bias? What kind of interventions can be done during training? Stepping back, how do we even define or measure social bias? Toxicity. Large language models are trained on a huge amount of Internet data (e.g., Reddit), which inevitably contains offensive content.RealToxicityPromptsis a dataset that evaluates a language model\u2019s propensity for producing toxic content. For example: As another example, GPT-3 has been demonstrated to outputanti-Muslim stereotypes: Applications such as writing assistants or chatbots would be vulnerable. Disinformation. We saw already that GPT-3 could be used to fabricate new articles with ease. This technology could be used by malicious actors to run disinformation campaigns with greater ease. Because of large language models\u2019 linguistic abilities, foreign state actors could much more easily create fluent, persuasive text without the risks of hiring native speakers. Security. Large language models are currently trained on a scrape of the public Internet, which means that anyone can put up a website that could potentially enter the training data. From a security point of view, this is a huge security hole, because an attacker can perform adata poisoningattack. For example, thispapershows that poison documents can be injected into the training set such that the model generates negative sentiment text whenever \\(\\nl{Apple iPhone}\\) is in the prompt: In general, the poison documents can be inconspicuous and, given the lack of careful curation that happens with existing training sets, this is a huge problem. Legal considerations. Language models are trained on copyright data (e.g., books). Is this protected by fair use? Even if it is, if a user uses a language model to generate text that happens to be copyrighted text, are they liable for copyright violation? For example, if you prompt GPT-3 with the first line of Harry Potter (demo): It will happily continue to spout out text from Harry Potter with high confidence. Cost and environmental impact. Finally, large language models can be quiteexpensiveto work with. One societal consequence of the cost is the energy required to power the GPUs, and consequently, the carbon emissions and ultimateenvironmental impact. However, determining the cost-benefit tradeoffs is tricky. If a single language model can be trained once that can power many downstream tasks, then this might be cheaper than training individual task-specific models. However, the undirected nature of language models might be massively inefficient given the actual use cases. Access. An accompanying concern with rising costs is access. Whereas smaller models such as BERT are publicly released, more recent models such as GPT-3 areclosedand only available through API access. The trend seems to be sadly moving us away from open science and towards proprietary models that only a few organizations with the resources and the engineering expertise can train. There are a few efforts that are trying to reverse this trend, includingHugging Face\u2019s Big Science project,EleutherAI, and Stanford\u2019sCRFM. Given language models\u2019 increasing social impact, it is imperative that we as a community find a way to allow as many scholars as possible to study, critique, and improve this technology.",
            "This course will be structured like an onion:",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/capabilities/": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/": "Capabilities",
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#language-modeling": "",
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#question-answering": "",
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#translation": "",
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#arithmetic": "",
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#news-article-generation": "",
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#novel-tasks": "",
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#penn-tree-bank": "",
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#lambada-paperno-et-al-2016": "",
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#hellaswag-zellers-et-al-2019": "",
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#triviaqa-joshi-et-al-2017": "",
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#webquestions-berant-et-al-2013": "",
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#naturalquestions": "",
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/newser.com": "newser.com",
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#using-new-words": "",
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#correcting-english-grammar": "",
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#other-tasks": "",
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#summary": "",
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#further-reading": ""
        },
        "text_between_h2": [
            "The most natural starting point for thinking about what a language model can do is to ask if it can do the thing that language models are supposed to do: model language. Recall that a language model \\(p\\) is a probability distribution over sequences of tokens. Suppose we take a corpus of text \\(x_{1:L}\\), for example: We can ask: what is the probability the language model assigns to it? Recall that we can break down the the joint probability into the product of the conditional probabilities for each token by the chain rule: Perplexity. The joint probability of a sequence depends on its length and thusgoes to zeroas the length grows, which makes it hard to track. (Just think about trying to get a better estimate of perplexity on newswire by getting more newswire.) Intuitively we want to average the per token probabilities \\(p(x_i \\mid x_{1:i-1})\\). We don\u2019t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn\u2019t penalize you for that. Instead, we want thegeometric average, which is exactly what perplexity does: Perplexity can be interpreted as theaverage \u201cbranching factor\u201dper token. Recall that \\(\\log \\frac{1}{p(x_i \\mid x_{1:i-1})}\\) is the code length. We are taking the average code length; exponentiating provides the number of possibilities. For intuition, take uniform distribution: a bitstring of length of 3 can encode \\(2^3\\) possible strings. Tale of two errors. There are two types of errors a language model can make, and perplexity treats them asymmetrically: Then we can compute the perplexity of \\(x_{1:L}\\) under \\(q\\): where the last approximate equality holds for small values of \\(\\epsilon\\). If we mix in 5% junk, then perplexity only by 5%. Note that the resulting language is horrible for generation, since every 20 tokens on average it\u2019s just going to generate a gibberish token. Now let\u2019s get on with evaluating perplexity on an actual dataset. ThePenn Tree Bankis a classic dataset in NLP, originally annotated for syntactic parsing. Beginning withEmami and Jelinek (2004)andMikolov and Zweig (2012), a version of the dataset that only contained Wall Street Journal articles was used as a language modeling evaluation. Note that the PTB language modeling benchmark involved some significant preprocessing of the original dataset (h/t toJohn Hewittfor pointing this out). Adaptation. Feed the entire text as a prompt into GPT-3 and evaluate the perplexity (demo): Results. GPT-3 vastly outperforms the existing state-of-the-art: ModelPerplexityGPT-320.5BERT-Large-CAs131.3 See theleaderboardfor the latest results. Train/test leakage. The authors did not evaluate on some datasets such asWikiText-103because GPT-3 was trained on Wikipedia. PTB had the advance of predating the Internet, and is only available through a paid license. This is another complication with large datasets: it is difficult to check that your test data did not appear in your training data and was memorized. Adaptation. Results. GPT-3 doesmuch betteron this task than the previous state-of-the-art (based on GPT-2): ModelPerplexityGPT-3 (few-shot)1.92SOTA8.63 See theleaderboardfor the latest results. Adaptation. This is amultiple-choice task, so the most natural thing to do is toscoreeach candidate answer with the language model and predict the \u201cbest\u201d one (demo): where ${answer} is one of: How do you score a candidate answer \\(y\\) given a question \\(x\\)? There\u2019s no principled answer, but here are someheuristics: Results. GPT-3 got close but did not exceed the state-of-the-art: ModelAccuracySOTA85.6GPT-379.3 However, the SOTA used fine-tuning on the HellaSwag training set, so it is pretty impressive that GPT-3 can get close without any task-specific training data! See theleaderboardfor the latest results.",
            "Now we consider (closed-book) question answering, where the input is a question and the output is an answer. Thelanguage model has to somehow \u201cknow\u201d the answerwithout looking up information in a database or a set of documents (we\u2019ll consider reading comprehension later, where the information is provided). Adaptation. We define a prompt based on the training instances (if any) and the question, and take the completion as the predicted answer (demo): Results. ModelAccuracyRAG68.0GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2 We also see that both increasing the model size and the number of in-context training instances helps:  Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG45.5GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5 Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG44.5GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9",
            "Adaptation. For the few-shot setting, we construct a prompt containing input-output training instances along with the input (demo): Results. Here are the results from German to English: ModelAccuracySOTA (supervised)40.2GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6",
            "GPT-3 is a language model (primarily on English), but we can evaluate it on a range of more \u201cabstract reasoning\u201d tasks, to evaluate GPT-3 as more of a general-purpose model. Adaptation. Pose the problem as question answering (demo): Results.  It doesn\u2019t work perfectly and can hardly be said to \u201cunderstand arithmetic\u201d fully, but it works surprisingly well.",
            "Adaptation. Note: in-context learning was needed to give the model an idea of what a prompt looks like. Results. Humans were able to able to detect classify \u201chuman\u201d versus \u201cmachine\u201d only 52% of the time (barely above random chance). For the article above, humans guessed \u201cmachine\u201d correctly only 12% of the time.",
            "Adaptation. Just describe the task in the prompt (demo): Adaptation. The prompt consists of input-output pairs (demo):",
            "Since the original paper, GPT-3 has been applied to many more tasks, including benchmark datasets and one-off demos. Here is an non-exhaustive list. Benchmarks. The performance on these benchmarks is still mediocre, but it\u2019s perhaps not bad given that we\u2019re doing few-shot learning! Demos. The demos are creative and interesting, but it\u2019s hard to tell how reliably they work.",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#language-modeling": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#language-modeling": ""
        },
        "text_between_h2": [
            "The most natural starting point for thinking about what a language model can do is to ask if it can do the thing that language models are supposed to do: model language. Recall that a language model \\(p\\) is a probability distribution over sequences of tokens. Suppose we take a corpus of text \\(x_{1:L}\\), for example: We can ask: what is the probability the language model assigns to it? Recall that we can break down the the joint probability into the product of the conditional probabilities for each token by the chain rule: Perplexity. The joint probability of a sequence depends on its length and thusgoes to zeroas the length grows, which makes it hard to track. (Just think about trying to get a better estimate of perplexity on newswire by getting more newswire.) Intuitively we want to average the per token probabilities \\(p(x_i \\mid x_{1:i-1})\\). We don\u2019t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn\u2019t penalize you for that. Instead, we want thegeometric average, which is exactly what perplexity does: Perplexity can be interpreted as theaverage \u201cbranching factor\u201dper token. Recall that \\(\\log \\frac{1}{p(x_i \\mid x_{1:i-1})}\\) is the code length. We are taking the average code length; exponentiating provides the number of possibilities. For intuition, take uniform distribution: a bitstring of length of 3 can encode \\(2^3\\) possible strings. Tale of two errors. There are two types of errors a language model can make, and perplexity treats them asymmetrically: Then we can compute the perplexity of \\(x_{1:L}\\) under \\(q\\): where the last approximate equality holds for small values of \\(\\epsilon\\). If we mix in 5% junk, then perplexity only by 5%. Note that the resulting language is horrible for generation, since every 20 tokens on average it\u2019s just going to generate a gibberish token. Now let\u2019s get on with evaluating perplexity on an actual dataset. ThePenn Tree Bankis a classic dataset in NLP, originally annotated for syntactic parsing. Beginning withEmami and Jelinek (2004)andMikolov and Zweig (2012), a version of the dataset that only contained Wall Street Journal articles was used as a language modeling evaluation. Note that the PTB language modeling benchmark involved some significant preprocessing of the original dataset (h/t toJohn Hewittfor pointing this out). Adaptation. Feed the entire text as a prompt into GPT-3 and evaluate the perplexity (demo): Results. GPT-3 vastly outperforms the existing state-of-the-art: ModelPerplexityGPT-320.5BERT-Large-CAs131.3 See theleaderboardfor the latest results. Train/test leakage. The authors did not evaluate on some datasets such asWikiText-103because GPT-3 was trained on Wikipedia. PTB had the advance of predating the Internet, and is only available through a paid license. This is another complication with large datasets: it is difficult to check that your test data did not appear in your training data and was memorized. Adaptation. Results. GPT-3 doesmuch betteron this task than the previous state-of-the-art (based on GPT-2): ModelPerplexityGPT-3 (few-shot)1.92SOTA8.63 See theleaderboardfor the latest results. Adaptation. This is amultiple-choice task, so the most natural thing to do is toscoreeach candidate answer with the language model and predict the \u201cbest\u201d one (demo): where ${answer} is one of: How do you score a candidate answer \\(y\\) given a question \\(x\\)? There\u2019s no principled answer, but here are someheuristics: Results. GPT-3 got close but did not exceed the state-of-the-art: ModelAccuracySOTA85.6GPT-379.3 However, the SOTA used fine-tuning on the HellaSwag training set, so it is pretty impressive that GPT-3 can get close without any task-specific training data! See theleaderboardfor the latest results.",
            "Now we consider (closed-book) question answering, where the input is a question and the output is an answer. Thelanguage model has to somehow \u201cknow\u201d the answerwithout looking up information in a database or a set of documents (we\u2019ll consider reading comprehension later, where the information is provided). Adaptation. We define a prompt based on the training instances (if any) and the question, and take the completion as the predicted answer (demo): Results. ModelAccuracyRAG68.0GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2 We also see that both increasing the model size and the number of in-context training instances helps:  Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG45.5GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5 Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG44.5GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9",
            "Adaptation. For the few-shot setting, we construct a prompt containing input-output training instances along with the input (demo): Results. Here are the results from German to English: ModelAccuracySOTA (supervised)40.2GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6",
            "GPT-3 is a language model (primarily on English), but we can evaluate it on a range of more \u201cabstract reasoning\u201d tasks, to evaluate GPT-3 as more of a general-purpose model. Adaptation. Pose the problem as question answering (demo): Results.  It doesn\u2019t work perfectly and can hardly be said to \u201cunderstand arithmetic\u201d fully, but it works surprisingly well.",
            "Adaptation. Note: in-context learning was needed to give the model an idea of what a prompt looks like. Results. Humans were able to able to detect classify \u201chuman\u201d versus \u201cmachine\u201d only 52% of the time (barely above random chance). For the article above, humans guessed \u201cmachine\u201d correctly only 12% of the time.",
            "Adaptation. Just describe the task in the prompt (demo): Adaptation. The prompt consists of input-output pairs (demo):",
            "Since the original paper, GPT-3 has been applied to many more tasks, including benchmark datasets and one-off demos. Here is an non-exhaustive list. Benchmarks. The performance on these benchmarks is still mediocre, but it\u2019s perhaps not bad given that we\u2019re doing few-shot learning! Demos. The demos are creative and interesting, but it\u2019s hard to tell how reliably they work.",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#question-answering": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#question-answering": ""
        },
        "text_between_h2": [
            "The most natural starting point for thinking about what a language model can do is to ask if it can do the thing that language models are supposed to do: model language. Recall that a language model \\(p\\) is a probability distribution over sequences of tokens. Suppose we take a corpus of text \\(x_{1:L}\\), for example: We can ask: what is the probability the language model assigns to it? Recall that we can break down the the joint probability into the product of the conditional probabilities for each token by the chain rule: Perplexity. The joint probability of a sequence depends on its length and thusgoes to zeroas the length grows, which makes it hard to track. (Just think about trying to get a better estimate of perplexity on newswire by getting more newswire.) Intuitively we want to average the per token probabilities \\(p(x_i \\mid x_{1:i-1})\\). We don\u2019t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn\u2019t penalize you for that. Instead, we want thegeometric average, which is exactly what perplexity does: Perplexity can be interpreted as theaverage \u201cbranching factor\u201dper token. Recall that \\(\\log \\frac{1}{p(x_i \\mid x_{1:i-1})}\\) is the code length. We are taking the average code length; exponentiating provides the number of possibilities. For intuition, take uniform distribution: a bitstring of length of 3 can encode \\(2^3\\) possible strings. Tale of two errors. There are two types of errors a language model can make, and perplexity treats them asymmetrically: Then we can compute the perplexity of \\(x_{1:L}\\) under \\(q\\): where the last approximate equality holds for small values of \\(\\epsilon\\). If we mix in 5% junk, then perplexity only by 5%. Note that the resulting language is horrible for generation, since every 20 tokens on average it\u2019s just going to generate a gibberish token. Now let\u2019s get on with evaluating perplexity on an actual dataset. ThePenn Tree Bankis a classic dataset in NLP, originally annotated for syntactic parsing. Beginning withEmami and Jelinek (2004)andMikolov and Zweig (2012), a version of the dataset that only contained Wall Street Journal articles was used as a language modeling evaluation. Note that the PTB language modeling benchmark involved some significant preprocessing of the original dataset (h/t toJohn Hewittfor pointing this out). Adaptation. Feed the entire text as a prompt into GPT-3 and evaluate the perplexity (demo): Results. GPT-3 vastly outperforms the existing state-of-the-art: ModelPerplexityGPT-320.5BERT-Large-CAs131.3 See theleaderboardfor the latest results. Train/test leakage. The authors did not evaluate on some datasets such asWikiText-103because GPT-3 was trained on Wikipedia. PTB had the advance of predating the Internet, and is only available through a paid license. This is another complication with large datasets: it is difficult to check that your test data did not appear in your training data and was memorized. Adaptation. Results. GPT-3 doesmuch betteron this task than the previous state-of-the-art (based on GPT-2): ModelPerplexityGPT-3 (few-shot)1.92SOTA8.63 See theleaderboardfor the latest results. Adaptation. This is amultiple-choice task, so the most natural thing to do is toscoreeach candidate answer with the language model and predict the \u201cbest\u201d one (demo): where ${answer} is one of: How do you score a candidate answer \\(y\\) given a question \\(x\\)? There\u2019s no principled answer, but here are someheuristics: Results. GPT-3 got close but did not exceed the state-of-the-art: ModelAccuracySOTA85.6GPT-379.3 However, the SOTA used fine-tuning on the HellaSwag training set, so it is pretty impressive that GPT-3 can get close without any task-specific training data! See theleaderboardfor the latest results.",
            "Now we consider (closed-book) question answering, where the input is a question and the output is an answer. Thelanguage model has to somehow \u201cknow\u201d the answerwithout looking up information in a database or a set of documents (we\u2019ll consider reading comprehension later, where the information is provided). Adaptation. We define a prompt based on the training instances (if any) and the question, and take the completion as the predicted answer (demo): Results. ModelAccuracyRAG68.0GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2 We also see that both increasing the model size and the number of in-context training instances helps:  Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG45.5GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5 Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG44.5GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9",
            "Adaptation. For the few-shot setting, we construct a prompt containing input-output training instances along with the input (demo): Results. Here are the results from German to English: ModelAccuracySOTA (supervised)40.2GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6",
            "GPT-3 is a language model (primarily on English), but we can evaluate it on a range of more \u201cabstract reasoning\u201d tasks, to evaluate GPT-3 as more of a general-purpose model. Adaptation. Pose the problem as question answering (demo): Results.  It doesn\u2019t work perfectly and can hardly be said to \u201cunderstand arithmetic\u201d fully, but it works surprisingly well.",
            "Adaptation. Note: in-context learning was needed to give the model an idea of what a prompt looks like. Results. Humans were able to able to detect classify \u201chuman\u201d versus \u201cmachine\u201d only 52% of the time (barely above random chance). For the article above, humans guessed \u201cmachine\u201d correctly only 12% of the time.",
            "Adaptation. Just describe the task in the prompt (demo): Adaptation. The prompt consists of input-output pairs (demo):",
            "Since the original paper, GPT-3 has been applied to many more tasks, including benchmark datasets and one-off demos. Here is an non-exhaustive list. Benchmarks. The performance on these benchmarks is still mediocre, but it\u2019s perhaps not bad given that we\u2019re doing few-shot learning! Demos. The demos are creative and interesting, but it\u2019s hard to tell how reliably they work.",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#translation": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#translation": ""
        },
        "text_between_h2": [
            "The most natural starting point for thinking about what a language model can do is to ask if it can do the thing that language models are supposed to do: model language. Recall that a language model \\(p\\) is a probability distribution over sequences of tokens. Suppose we take a corpus of text \\(x_{1:L}\\), for example: We can ask: what is the probability the language model assigns to it? Recall that we can break down the the joint probability into the product of the conditional probabilities for each token by the chain rule: Perplexity. The joint probability of a sequence depends on its length and thusgoes to zeroas the length grows, which makes it hard to track. (Just think about trying to get a better estimate of perplexity on newswire by getting more newswire.) Intuitively we want to average the per token probabilities \\(p(x_i \\mid x_{1:i-1})\\). We don\u2019t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn\u2019t penalize you for that. Instead, we want thegeometric average, which is exactly what perplexity does: Perplexity can be interpreted as theaverage \u201cbranching factor\u201dper token. Recall that \\(\\log \\frac{1}{p(x_i \\mid x_{1:i-1})}\\) is the code length. We are taking the average code length; exponentiating provides the number of possibilities. For intuition, take uniform distribution: a bitstring of length of 3 can encode \\(2^3\\) possible strings. Tale of two errors. There are two types of errors a language model can make, and perplexity treats them asymmetrically: Then we can compute the perplexity of \\(x_{1:L}\\) under \\(q\\): where the last approximate equality holds for small values of \\(\\epsilon\\). If we mix in 5% junk, then perplexity only by 5%. Note that the resulting language is horrible for generation, since every 20 tokens on average it\u2019s just going to generate a gibberish token. Now let\u2019s get on with evaluating perplexity on an actual dataset. ThePenn Tree Bankis a classic dataset in NLP, originally annotated for syntactic parsing. Beginning withEmami and Jelinek (2004)andMikolov and Zweig (2012), a version of the dataset that only contained Wall Street Journal articles was used as a language modeling evaluation. Note that the PTB language modeling benchmark involved some significant preprocessing of the original dataset (h/t toJohn Hewittfor pointing this out). Adaptation. Feed the entire text as a prompt into GPT-3 and evaluate the perplexity (demo): Results. GPT-3 vastly outperforms the existing state-of-the-art: ModelPerplexityGPT-320.5BERT-Large-CAs131.3 See theleaderboardfor the latest results. Train/test leakage. The authors did not evaluate on some datasets such asWikiText-103because GPT-3 was trained on Wikipedia. PTB had the advance of predating the Internet, and is only available through a paid license. This is another complication with large datasets: it is difficult to check that your test data did not appear in your training data and was memorized. Adaptation. Results. GPT-3 doesmuch betteron this task than the previous state-of-the-art (based on GPT-2): ModelPerplexityGPT-3 (few-shot)1.92SOTA8.63 See theleaderboardfor the latest results. Adaptation. This is amultiple-choice task, so the most natural thing to do is toscoreeach candidate answer with the language model and predict the \u201cbest\u201d one (demo): where ${answer} is one of: How do you score a candidate answer \\(y\\) given a question \\(x\\)? There\u2019s no principled answer, but here are someheuristics: Results. GPT-3 got close but did not exceed the state-of-the-art: ModelAccuracySOTA85.6GPT-379.3 However, the SOTA used fine-tuning on the HellaSwag training set, so it is pretty impressive that GPT-3 can get close without any task-specific training data! See theleaderboardfor the latest results.",
            "Now we consider (closed-book) question answering, where the input is a question and the output is an answer. Thelanguage model has to somehow \u201cknow\u201d the answerwithout looking up information in a database or a set of documents (we\u2019ll consider reading comprehension later, where the information is provided). Adaptation. We define a prompt based on the training instances (if any) and the question, and take the completion as the predicted answer (demo): Results. ModelAccuracyRAG68.0GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2 We also see that both increasing the model size and the number of in-context training instances helps:  Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG45.5GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5 Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG44.5GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9",
            "Adaptation. For the few-shot setting, we construct a prompt containing input-output training instances along with the input (demo): Results. Here are the results from German to English: ModelAccuracySOTA (supervised)40.2GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6",
            "GPT-3 is a language model (primarily on English), but we can evaluate it on a range of more \u201cabstract reasoning\u201d tasks, to evaluate GPT-3 as more of a general-purpose model. Adaptation. Pose the problem as question answering (demo): Results.  It doesn\u2019t work perfectly and can hardly be said to \u201cunderstand arithmetic\u201d fully, but it works surprisingly well.",
            "Adaptation. Note: in-context learning was needed to give the model an idea of what a prompt looks like. Results. Humans were able to able to detect classify \u201chuman\u201d versus \u201cmachine\u201d only 52% of the time (barely above random chance). For the article above, humans guessed \u201cmachine\u201d correctly only 12% of the time.",
            "Adaptation. Just describe the task in the prompt (demo): Adaptation. The prompt consists of input-output pairs (demo):",
            "Since the original paper, GPT-3 has been applied to many more tasks, including benchmark datasets and one-off demos. Here is an non-exhaustive list. Benchmarks. The performance on these benchmarks is still mediocre, but it\u2019s perhaps not bad given that we\u2019re doing few-shot learning! Demos. The demos are creative and interesting, but it\u2019s hard to tell how reliably they work.",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#arithmetic": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#arithmetic": ""
        },
        "text_between_h2": [
            "The most natural starting point for thinking about what a language model can do is to ask if it can do the thing that language models are supposed to do: model language. Recall that a language model \\(p\\) is a probability distribution over sequences of tokens. Suppose we take a corpus of text \\(x_{1:L}\\), for example: We can ask: what is the probability the language model assigns to it? Recall that we can break down the the joint probability into the product of the conditional probabilities for each token by the chain rule: Perplexity. The joint probability of a sequence depends on its length and thusgoes to zeroas the length grows, which makes it hard to track. (Just think about trying to get a better estimate of perplexity on newswire by getting more newswire.) Intuitively we want to average the per token probabilities \\(p(x_i \\mid x_{1:i-1})\\). We don\u2019t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn\u2019t penalize you for that. Instead, we want thegeometric average, which is exactly what perplexity does: Perplexity can be interpreted as theaverage \u201cbranching factor\u201dper token. Recall that \\(\\log \\frac{1}{p(x_i \\mid x_{1:i-1})}\\) is the code length. We are taking the average code length; exponentiating provides the number of possibilities. For intuition, take uniform distribution: a bitstring of length of 3 can encode \\(2^3\\) possible strings. Tale of two errors. There are two types of errors a language model can make, and perplexity treats them asymmetrically: Then we can compute the perplexity of \\(x_{1:L}\\) under \\(q\\): where the last approximate equality holds for small values of \\(\\epsilon\\). If we mix in 5% junk, then perplexity only by 5%. Note that the resulting language is horrible for generation, since every 20 tokens on average it\u2019s just going to generate a gibberish token. Now let\u2019s get on with evaluating perplexity on an actual dataset. ThePenn Tree Bankis a classic dataset in NLP, originally annotated for syntactic parsing. Beginning withEmami and Jelinek (2004)andMikolov and Zweig (2012), a version of the dataset that only contained Wall Street Journal articles was used as a language modeling evaluation. Note that the PTB language modeling benchmark involved some significant preprocessing of the original dataset (h/t toJohn Hewittfor pointing this out). Adaptation. Feed the entire text as a prompt into GPT-3 and evaluate the perplexity (demo): Results. GPT-3 vastly outperforms the existing state-of-the-art: ModelPerplexityGPT-320.5BERT-Large-CAs131.3 See theleaderboardfor the latest results. Train/test leakage. The authors did not evaluate on some datasets such asWikiText-103because GPT-3 was trained on Wikipedia. PTB had the advance of predating the Internet, and is only available through a paid license. This is another complication with large datasets: it is difficult to check that your test data did not appear in your training data and was memorized. Adaptation. Results. GPT-3 doesmuch betteron this task than the previous state-of-the-art (based on GPT-2): ModelPerplexityGPT-3 (few-shot)1.92SOTA8.63 See theleaderboardfor the latest results. Adaptation. This is amultiple-choice task, so the most natural thing to do is toscoreeach candidate answer with the language model and predict the \u201cbest\u201d one (demo): where ${answer} is one of: How do you score a candidate answer \\(y\\) given a question \\(x\\)? There\u2019s no principled answer, but here are someheuristics: Results. GPT-3 got close but did not exceed the state-of-the-art: ModelAccuracySOTA85.6GPT-379.3 However, the SOTA used fine-tuning on the HellaSwag training set, so it is pretty impressive that GPT-3 can get close without any task-specific training data! See theleaderboardfor the latest results.",
            "Now we consider (closed-book) question answering, where the input is a question and the output is an answer. Thelanguage model has to somehow \u201cknow\u201d the answerwithout looking up information in a database or a set of documents (we\u2019ll consider reading comprehension later, where the information is provided). Adaptation. We define a prompt based on the training instances (if any) and the question, and take the completion as the predicted answer (demo): Results. ModelAccuracyRAG68.0GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2 We also see that both increasing the model size and the number of in-context training instances helps:  Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG45.5GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5 Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG44.5GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9",
            "Adaptation. For the few-shot setting, we construct a prompt containing input-output training instances along with the input (demo): Results. Here are the results from German to English: ModelAccuracySOTA (supervised)40.2GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6",
            "GPT-3 is a language model (primarily on English), but we can evaluate it on a range of more \u201cabstract reasoning\u201d tasks, to evaluate GPT-3 as more of a general-purpose model. Adaptation. Pose the problem as question answering (demo): Results.  It doesn\u2019t work perfectly and can hardly be said to \u201cunderstand arithmetic\u201d fully, but it works surprisingly well.",
            "Adaptation. Note: in-context learning was needed to give the model an idea of what a prompt looks like. Results. Humans were able to able to detect classify \u201chuman\u201d versus \u201cmachine\u201d only 52% of the time (barely above random chance). For the article above, humans guessed \u201cmachine\u201d correctly only 12% of the time.",
            "Adaptation. Just describe the task in the prompt (demo): Adaptation. The prompt consists of input-output pairs (demo):",
            "Since the original paper, GPT-3 has been applied to many more tasks, including benchmark datasets and one-off demos. Here is an non-exhaustive list. Benchmarks. The performance on these benchmarks is still mediocre, but it\u2019s perhaps not bad given that we\u2019re doing few-shot learning! Demos. The demos are creative and interesting, but it\u2019s hard to tell how reliably they work.",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#news-article-generation": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#news-article-generation": ""
        },
        "text_between_h2": [
            "The most natural starting point for thinking about what a language model can do is to ask if it can do the thing that language models are supposed to do: model language. Recall that a language model \\(p\\) is a probability distribution over sequences of tokens. Suppose we take a corpus of text \\(x_{1:L}\\), for example: We can ask: what is the probability the language model assigns to it? Recall that we can break down the the joint probability into the product of the conditional probabilities for each token by the chain rule: Perplexity. The joint probability of a sequence depends on its length and thusgoes to zeroas the length grows, which makes it hard to track. (Just think about trying to get a better estimate of perplexity on newswire by getting more newswire.) Intuitively we want to average the per token probabilities \\(p(x_i \\mid x_{1:i-1})\\). We don\u2019t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn\u2019t penalize you for that. Instead, we want thegeometric average, which is exactly what perplexity does: Perplexity can be interpreted as theaverage \u201cbranching factor\u201dper token. Recall that \\(\\log \\frac{1}{p(x_i \\mid x_{1:i-1})}\\) is the code length. We are taking the average code length; exponentiating provides the number of possibilities. For intuition, take uniform distribution: a bitstring of length of 3 can encode \\(2^3\\) possible strings. Tale of two errors. There are two types of errors a language model can make, and perplexity treats them asymmetrically: Then we can compute the perplexity of \\(x_{1:L}\\) under \\(q\\): where the last approximate equality holds for small values of \\(\\epsilon\\). If we mix in 5% junk, then perplexity only by 5%. Note that the resulting language is horrible for generation, since every 20 tokens on average it\u2019s just going to generate a gibberish token. Now let\u2019s get on with evaluating perplexity on an actual dataset. ThePenn Tree Bankis a classic dataset in NLP, originally annotated for syntactic parsing. Beginning withEmami and Jelinek (2004)andMikolov and Zweig (2012), a version of the dataset that only contained Wall Street Journal articles was used as a language modeling evaluation. Note that the PTB language modeling benchmark involved some significant preprocessing of the original dataset (h/t toJohn Hewittfor pointing this out). Adaptation. Feed the entire text as a prompt into GPT-3 and evaluate the perplexity (demo): Results. GPT-3 vastly outperforms the existing state-of-the-art: ModelPerplexityGPT-320.5BERT-Large-CAs131.3 See theleaderboardfor the latest results. Train/test leakage. The authors did not evaluate on some datasets such asWikiText-103because GPT-3 was trained on Wikipedia. PTB had the advance of predating the Internet, and is only available through a paid license. This is another complication with large datasets: it is difficult to check that your test data did not appear in your training data and was memorized. Adaptation. Results. GPT-3 doesmuch betteron this task than the previous state-of-the-art (based on GPT-2): ModelPerplexityGPT-3 (few-shot)1.92SOTA8.63 See theleaderboardfor the latest results. Adaptation. This is amultiple-choice task, so the most natural thing to do is toscoreeach candidate answer with the language model and predict the \u201cbest\u201d one (demo): where ${answer} is one of: How do you score a candidate answer \\(y\\) given a question \\(x\\)? There\u2019s no principled answer, but here are someheuristics: Results. GPT-3 got close but did not exceed the state-of-the-art: ModelAccuracySOTA85.6GPT-379.3 However, the SOTA used fine-tuning on the HellaSwag training set, so it is pretty impressive that GPT-3 can get close without any task-specific training data! See theleaderboardfor the latest results.",
            "Now we consider (closed-book) question answering, where the input is a question and the output is an answer. Thelanguage model has to somehow \u201cknow\u201d the answerwithout looking up information in a database or a set of documents (we\u2019ll consider reading comprehension later, where the information is provided). Adaptation. We define a prompt based on the training instances (if any) and the question, and take the completion as the predicted answer (demo): Results. ModelAccuracyRAG68.0GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2 We also see that both increasing the model size and the number of in-context training instances helps:  Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG45.5GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5 Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG44.5GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9",
            "Adaptation. For the few-shot setting, we construct a prompt containing input-output training instances along with the input (demo): Results. Here are the results from German to English: ModelAccuracySOTA (supervised)40.2GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6",
            "GPT-3 is a language model (primarily on English), but we can evaluate it on a range of more \u201cabstract reasoning\u201d tasks, to evaluate GPT-3 as more of a general-purpose model. Adaptation. Pose the problem as question answering (demo): Results.  It doesn\u2019t work perfectly and can hardly be said to \u201cunderstand arithmetic\u201d fully, but it works surprisingly well.",
            "Adaptation. Note: in-context learning was needed to give the model an idea of what a prompt looks like. Results. Humans were able to able to detect classify \u201chuman\u201d versus \u201cmachine\u201d only 52% of the time (barely above random chance). For the article above, humans guessed \u201cmachine\u201d correctly only 12% of the time.",
            "Adaptation. Just describe the task in the prompt (demo): Adaptation. The prompt consists of input-output pairs (demo):",
            "Since the original paper, GPT-3 has been applied to many more tasks, including benchmark datasets and one-off demos. Here is an non-exhaustive list. Benchmarks. The performance on these benchmarks is still mediocre, but it\u2019s perhaps not bad given that we\u2019re doing few-shot learning! Demos. The demos are creative and interesting, but it\u2019s hard to tell how reliably they work.",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#novel-tasks": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#novel-tasks": ""
        },
        "text_between_h2": [
            "The most natural starting point for thinking about what a language model can do is to ask if it can do the thing that language models are supposed to do: model language. Recall that a language model \\(p\\) is a probability distribution over sequences of tokens. Suppose we take a corpus of text \\(x_{1:L}\\), for example: We can ask: what is the probability the language model assigns to it? Recall that we can break down the the joint probability into the product of the conditional probabilities for each token by the chain rule: Perplexity. The joint probability of a sequence depends on its length and thusgoes to zeroas the length grows, which makes it hard to track. (Just think about trying to get a better estimate of perplexity on newswire by getting more newswire.) Intuitively we want to average the per token probabilities \\(p(x_i \\mid x_{1:i-1})\\). We don\u2019t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn\u2019t penalize you for that. Instead, we want thegeometric average, which is exactly what perplexity does: Perplexity can be interpreted as theaverage \u201cbranching factor\u201dper token. Recall that \\(\\log \\frac{1}{p(x_i \\mid x_{1:i-1})}\\) is the code length. We are taking the average code length; exponentiating provides the number of possibilities. For intuition, take uniform distribution: a bitstring of length of 3 can encode \\(2^3\\) possible strings. Tale of two errors. There are two types of errors a language model can make, and perplexity treats them asymmetrically: Then we can compute the perplexity of \\(x_{1:L}\\) under \\(q\\): where the last approximate equality holds for small values of \\(\\epsilon\\). If we mix in 5% junk, then perplexity only by 5%. Note that the resulting language is horrible for generation, since every 20 tokens on average it\u2019s just going to generate a gibberish token. Now let\u2019s get on with evaluating perplexity on an actual dataset. ThePenn Tree Bankis a classic dataset in NLP, originally annotated for syntactic parsing. Beginning withEmami and Jelinek (2004)andMikolov and Zweig (2012), a version of the dataset that only contained Wall Street Journal articles was used as a language modeling evaluation. Note that the PTB language modeling benchmark involved some significant preprocessing of the original dataset (h/t toJohn Hewittfor pointing this out). Adaptation. Feed the entire text as a prompt into GPT-3 and evaluate the perplexity (demo): Results. GPT-3 vastly outperforms the existing state-of-the-art: ModelPerplexityGPT-320.5BERT-Large-CAs131.3 See theleaderboardfor the latest results. Train/test leakage. The authors did not evaluate on some datasets such asWikiText-103because GPT-3 was trained on Wikipedia. PTB had the advance of predating the Internet, and is only available through a paid license. This is another complication with large datasets: it is difficult to check that your test data did not appear in your training data and was memorized. Adaptation. Results. GPT-3 doesmuch betteron this task than the previous state-of-the-art (based on GPT-2): ModelPerplexityGPT-3 (few-shot)1.92SOTA8.63 See theleaderboardfor the latest results. Adaptation. This is amultiple-choice task, so the most natural thing to do is toscoreeach candidate answer with the language model and predict the \u201cbest\u201d one (demo): where ${answer} is one of: How do you score a candidate answer \\(y\\) given a question \\(x\\)? There\u2019s no principled answer, but here are someheuristics: Results. GPT-3 got close but did not exceed the state-of-the-art: ModelAccuracySOTA85.6GPT-379.3 However, the SOTA used fine-tuning on the HellaSwag training set, so it is pretty impressive that GPT-3 can get close without any task-specific training data! See theleaderboardfor the latest results.",
            "Now we consider (closed-book) question answering, where the input is a question and the output is an answer. Thelanguage model has to somehow \u201cknow\u201d the answerwithout looking up information in a database or a set of documents (we\u2019ll consider reading comprehension later, where the information is provided). Adaptation. We define a prompt based on the training instances (if any) and the question, and take the completion as the predicted answer (demo): Results. ModelAccuracyRAG68.0GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2 We also see that both increasing the model size and the number of in-context training instances helps:  Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG45.5GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5 Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG44.5GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9",
            "Adaptation. For the few-shot setting, we construct a prompt containing input-output training instances along with the input (demo): Results. Here are the results from German to English: ModelAccuracySOTA (supervised)40.2GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6",
            "GPT-3 is a language model (primarily on English), but we can evaluate it on a range of more \u201cabstract reasoning\u201d tasks, to evaluate GPT-3 as more of a general-purpose model. Adaptation. Pose the problem as question answering (demo): Results.  It doesn\u2019t work perfectly and can hardly be said to \u201cunderstand arithmetic\u201d fully, but it works surprisingly well.",
            "Adaptation. Note: in-context learning was needed to give the model an idea of what a prompt looks like. Results. Humans were able to able to detect classify \u201chuman\u201d versus \u201cmachine\u201d only 52% of the time (barely above random chance). For the article above, humans guessed \u201cmachine\u201d correctly only 12% of the time.",
            "Adaptation. Just describe the task in the prompt (demo): Adaptation. The prompt consists of input-output pairs (demo):",
            "Since the original paper, GPT-3 has been applied to many more tasks, including benchmark datasets and one-off demos. Here is an non-exhaustive list. Benchmarks. The performance on these benchmarks is still mediocre, but it\u2019s perhaps not bad given that we\u2019re doing few-shot learning! Demos. The demos are creative and interesting, but it\u2019s hard to tell how reliably they work.",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#penn-tree-bank": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#penn-tree-bank": ""
        },
        "text_between_h2": [
            "The most natural starting point for thinking about what a language model can do is to ask if it can do the thing that language models are supposed to do: model language. Recall that a language model \\(p\\) is a probability distribution over sequences of tokens. Suppose we take a corpus of text \\(x_{1:L}\\), for example: We can ask: what is the probability the language model assigns to it? Recall that we can break down the the joint probability into the product of the conditional probabilities for each token by the chain rule: Perplexity. The joint probability of a sequence depends on its length and thusgoes to zeroas the length grows, which makes it hard to track. (Just think about trying to get a better estimate of perplexity on newswire by getting more newswire.) Intuitively we want to average the per token probabilities \\(p(x_i \\mid x_{1:i-1})\\). We don\u2019t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn\u2019t penalize you for that. Instead, we want thegeometric average, which is exactly what perplexity does: Perplexity can be interpreted as theaverage \u201cbranching factor\u201dper token. Recall that \\(\\log \\frac{1}{p(x_i \\mid x_{1:i-1})}\\) is the code length. We are taking the average code length; exponentiating provides the number of possibilities. For intuition, take uniform distribution: a bitstring of length of 3 can encode \\(2^3\\) possible strings. Tale of two errors. There are two types of errors a language model can make, and perplexity treats them asymmetrically: Then we can compute the perplexity of \\(x_{1:L}\\) under \\(q\\): where the last approximate equality holds for small values of \\(\\epsilon\\). If we mix in 5% junk, then perplexity only by 5%. Note that the resulting language is horrible for generation, since every 20 tokens on average it\u2019s just going to generate a gibberish token. Now let\u2019s get on with evaluating perplexity on an actual dataset. ThePenn Tree Bankis a classic dataset in NLP, originally annotated for syntactic parsing. Beginning withEmami and Jelinek (2004)andMikolov and Zweig (2012), a version of the dataset that only contained Wall Street Journal articles was used as a language modeling evaluation. Note that the PTB language modeling benchmark involved some significant preprocessing of the original dataset (h/t toJohn Hewittfor pointing this out). Adaptation. Feed the entire text as a prompt into GPT-3 and evaluate the perplexity (demo): Results. GPT-3 vastly outperforms the existing state-of-the-art: ModelPerplexityGPT-320.5BERT-Large-CAs131.3 See theleaderboardfor the latest results. Train/test leakage. The authors did not evaluate on some datasets such asWikiText-103because GPT-3 was trained on Wikipedia. PTB had the advance of predating the Internet, and is only available through a paid license. This is another complication with large datasets: it is difficult to check that your test data did not appear in your training data and was memorized. Adaptation. Results. GPT-3 doesmuch betteron this task than the previous state-of-the-art (based on GPT-2): ModelPerplexityGPT-3 (few-shot)1.92SOTA8.63 See theleaderboardfor the latest results. Adaptation. This is amultiple-choice task, so the most natural thing to do is toscoreeach candidate answer with the language model and predict the \u201cbest\u201d one (demo): where ${answer} is one of: How do you score a candidate answer \\(y\\) given a question \\(x\\)? There\u2019s no principled answer, but here are someheuristics: Results. GPT-3 got close but did not exceed the state-of-the-art: ModelAccuracySOTA85.6GPT-379.3 However, the SOTA used fine-tuning on the HellaSwag training set, so it is pretty impressive that GPT-3 can get close without any task-specific training data! See theleaderboardfor the latest results.",
            "Now we consider (closed-book) question answering, where the input is a question and the output is an answer. Thelanguage model has to somehow \u201cknow\u201d the answerwithout looking up information in a database or a set of documents (we\u2019ll consider reading comprehension later, where the information is provided). Adaptation. We define a prompt based on the training instances (if any) and the question, and take the completion as the predicted answer (demo): Results. ModelAccuracyRAG68.0GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2 We also see that both increasing the model size and the number of in-context training instances helps:  Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG45.5GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5 Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG44.5GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9",
            "Adaptation. For the few-shot setting, we construct a prompt containing input-output training instances along with the input (demo): Results. Here are the results from German to English: ModelAccuracySOTA (supervised)40.2GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6",
            "GPT-3 is a language model (primarily on English), but we can evaluate it on a range of more \u201cabstract reasoning\u201d tasks, to evaluate GPT-3 as more of a general-purpose model. Adaptation. Pose the problem as question answering (demo): Results.  It doesn\u2019t work perfectly and can hardly be said to \u201cunderstand arithmetic\u201d fully, but it works surprisingly well.",
            "Adaptation. Note: in-context learning was needed to give the model an idea of what a prompt looks like. Results. Humans were able to able to detect classify \u201chuman\u201d versus \u201cmachine\u201d only 52% of the time (barely above random chance). For the article above, humans guessed \u201cmachine\u201d correctly only 12% of the time.",
            "Adaptation. Just describe the task in the prompt (demo): Adaptation. The prompt consists of input-output pairs (demo):",
            "Since the original paper, GPT-3 has been applied to many more tasks, including benchmark datasets and one-off demos. Here is an non-exhaustive list. Benchmarks. The performance on these benchmarks is still mediocre, but it\u2019s perhaps not bad given that we\u2019re doing few-shot learning! Demos. The demos are creative and interesting, but it\u2019s hard to tell how reliably they work.",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#lambada-paperno-et-al-2016": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#lambada-paperno-et-al-2016": ""
        },
        "text_between_h2": [
            "The most natural starting point for thinking about what a language model can do is to ask if it can do the thing that language models are supposed to do: model language. Recall that a language model \\(p\\) is a probability distribution over sequences of tokens. Suppose we take a corpus of text \\(x_{1:L}\\), for example: We can ask: what is the probability the language model assigns to it? Recall that we can break down the the joint probability into the product of the conditional probabilities for each token by the chain rule: Perplexity. The joint probability of a sequence depends on its length and thusgoes to zeroas the length grows, which makes it hard to track. (Just think about trying to get a better estimate of perplexity on newswire by getting more newswire.) Intuitively we want to average the per token probabilities \\(p(x_i \\mid x_{1:i-1})\\). We don\u2019t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn\u2019t penalize you for that. Instead, we want thegeometric average, which is exactly what perplexity does: Perplexity can be interpreted as theaverage \u201cbranching factor\u201dper token. Recall that \\(\\log \\frac{1}{p(x_i \\mid x_{1:i-1})}\\) is the code length. We are taking the average code length; exponentiating provides the number of possibilities. For intuition, take uniform distribution: a bitstring of length of 3 can encode \\(2^3\\) possible strings. Tale of two errors. There are two types of errors a language model can make, and perplexity treats them asymmetrically: Then we can compute the perplexity of \\(x_{1:L}\\) under \\(q\\): where the last approximate equality holds for small values of \\(\\epsilon\\). If we mix in 5% junk, then perplexity only by 5%. Note that the resulting language is horrible for generation, since every 20 tokens on average it\u2019s just going to generate a gibberish token. Now let\u2019s get on with evaluating perplexity on an actual dataset. ThePenn Tree Bankis a classic dataset in NLP, originally annotated for syntactic parsing. Beginning withEmami and Jelinek (2004)andMikolov and Zweig (2012), a version of the dataset that only contained Wall Street Journal articles was used as a language modeling evaluation. Note that the PTB language modeling benchmark involved some significant preprocessing of the original dataset (h/t toJohn Hewittfor pointing this out). Adaptation. Feed the entire text as a prompt into GPT-3 and evaluate the perplexity (demo): Results. GPT-3 vastly outperforms the existing state-of-the-art: ModelPerplexityGPT-320.5BERT-Large-CAs131.3 See theleaderboardfor the latest results. Train/test leakage. The authors did not evaluate on some datasets such asWikiText-103because GPT-3 was trained on Wikipedia. PTB had the advance of predating the Internet, and is only available through a paid license. This is another complication with large datasets: it is difficult to check that your test data did not appear in your training data and was memorized. Adaptation. Results. GPT-3 doesmuch betteron this task than the previous state-of-the-art (based on GPT-2): ModelPerplexityGPT-3 (few-shot)1.92SOTA8.63 See theleaderboardfor the latest results. Adaptation. This is amultiple-choice task, so the most natural thing to do is toscoreeach candidate answer with the language model and predict the \u201cbest\u201d one (demo): where ${answer} is one of: How do you score a candidate answer \\(y\\) given a question \\(x\\)? There\u2019s no principled answer, but here are someheuristics: Results. GPT-3 got close but did not exceed the state-of-the-art: ModelAccuracySOTA85.6GPT-379.3 However, the SOTA used fine-tuning on the HellaSwag training set, so it is pretty impressive that GPT-3 can get close without any task-specific training data! See theleaderboardfor the latest results.",
            "Now we consider (closed-book) question answering, where the input is a question and the output is an answer. Thelanguage model has to somehow \u201cknow\u201d the answerwithout looking up information in a database or a set of documents (we\u2019ll consider reading comprehension later, where the information is provided). Adaptation. We define a prompt based on the training instances (if any) and the question, and take the completion as the predicted answer (demo): Results. ModelAccuracyRAG68.0GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2 We also see that both increasing the model size and the number of in-context training instances helps:  Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG45.5GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5 Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG44.5GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9",
            "Adaptation. For the few-shot setting, we construct a prompt containing input-output training instances along with the input (demo): Results. Here are the results from German to English: ModelAccuracySOTA (supervised)40.2GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6",
            "GPT-3 is a language model (primarily on English), but we can evaluate it on a range of more \u201cabstract reasoning\u201d tasks, to evaluate GPT-3 as more of a general-purpose model. Adaptation. Pose the problem as question answering (demo): Results.  It doesn\u2019t work perfectly and can hardly be said to \u201cunderstand arithmetic\u201d fully, but it works surprisingly well.",
            "Adaptation. Note: in-context learning was needed to give the model an idea of what a prompt looks like. Results. Humans were able to able to detect classify \u201chuman\u201d versus \u201cmachine\u201d only 52% of the time (barely above random chance). For the article above, humans guessed \u201cmachine\u201d correctly only 12% of the time.",
            "Adaptation. Just describe the task in the prompt (demo): Adaptation. The prompt consists of input-output pairs (demo):",
            "Since the original paper, GPT-3 has been applied to many more tasks, including benchmark datasets and one-off demos. Here is an non-exhaustive list. Benchmarks. The performance on these benchmarks is still mediocre, but it\u2019s perhaps not bad given that we\u2019re doing few-shot learning! Demos. The demos are creative and interesting, but it\u2019s hard to tell how reliably they work.",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#hellaswag-zellers-et-al-2019": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#hellaswag-zellers-et-al-2019": ""
        },
        "text_between_h2": [
            "The most natural starting point for thinking about what a language model can do is to ask if it can do the thing that language models are supposed to do: model language. Recall that a language model \\(p\\) is a probability distribution over sequences of tokens. Suppose we take a corpus of text \\(x_{1:L}\\), for example: We can ask: what is the probability the language model assigns to it? Recall that we can break down the the joint probability into the product of the conditional probabilities for each token by the chain rule: Perplexity. The joint probability of a sequence depends on its length and thusgoes to zeroas the length grows, which makes it hard to track. (Just think about trying to get a better estimate of perplexity on newswire by getting more newswire.) Intuitively we want to average the per token probabilities \\(p(x_i \\mid x_{1:i-1})\\). We don\u2019t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn\u2019t penalize you for that. Instead, we want thegeometric average, which is exactly what perplexity does: Perplexity can be interpreted as theaverage \u201cbranching factor\u201dper token. Recall that \\(\\log \\frac{1}{p(x_i \\mid x_{1:i-1})}\\) is the code length. We are taking the average code length; exponentiating provides the number of possibilities. For intuition, take uniform distribution: a bitstring of length of 3 can encode \\(2^3\\) possible strings. Tale of two errors. There are two types of errors a language model can make, and perplexity treats them asymmetrically: Then we can compute the perplexity of \\(x_{1:L}\\) under \\(q\\): where the last approximate equality holds for small values of \\(\\epsilon\\). If we mix in 5% junk, then perplexity only by 5%. Note that the resulting language is horrible for generation, since every 20 tokens on average it\u2019s just going to generate a gibberish token. Now let\u2019s get on with evaluating perplexity on an actual dataset. ThePenn Tree Bankis a classic dataset in NLP, originally annotated for syntactic parsing. Beginning withEmami and Jelinek (2004)andMikolov and Zweig (2012), a version of the dataset that only contained Wall Street Journal articles was used as a language modeling evaluation. Note that the PTB language modeling benchmark involved some significant preprocessing of the original dataset (h/t toJohn Hewittfor pointing this out). Adaptation. Feed the entire text as a prompt into GPT-3 and evaluate the perplexity (demo): Results. GPT-3 vastly outperforms the existing state-of-the-art: ModelPerplexityGPT-320.5BERT-Large-CAs131.3 See theleaderboardfor the latest results. Train/test leakage. The authors did not evaluate on some datasets such asWikiText-103because GPT-3 was trained on Wikipedia. PTB had the advance of predating the Internet, and is only available through a paid license. This is another complication with large datasets: it is difficult to check that your test data did not appear in your training data and was memorized. Adaptation. Results. GPT-3 doesmuch betteron this task than the previous state-of-the-art (based on GPT-2): ModelPerplexityGPT-3 (few-shot)1.92SOTA8.63 See theleaderboardfor the latest results. Adaptation. This is amultiple-choice task, so the most natural thing to do is toscoreeach candidate answer with the language model and predict the \u201cbest\u201d one (demo): where ${answer} is one of: How do you score a candidate answer \\(y\\) given a question \\(x\\)? There\u2019s no principled answer, but here are someheuristics: Results. GPT-3 got close but did not exceed the state-of-the-art: ModelAccuracySOTA85.6GPT-379.3 However, the SOTA used fine-tuning on the HellaSwag training set, so it is pretty impressive that GPT-3 can get close without any task-specific training data! See theleaderboardfor the latest results.",
            "Now we consider (closed-book) question answering, where the input is a question and the output is an answer. Thelanguage model has to somehow \u201cknow\u201d the answerwithout looking up information in a database or a set of documents (we\u2019ll consider reading comprehension later, where the information is provided). Adaptation. We define a prompt based on the training instances (if any) and the question, and take the completion as the predicted answer (demo): Results. ModelAccuracyRAG68.0GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2 We also see that both increasing the model size and the number of in-context training instances helps:  Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG45.5GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5 Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG44.5GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9",
            "Adaptation. For the few-shot setting, we construct a prompt containing input-output training instances along with the input (demo): Results. Here are the results from German to English: ModelAccuracySOTA (supervised)40.2GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6",
            "GPT-3 is a language model (primarily on English), but we can evaluate it on a range of more \u201cabstract reasoning\u201d tasks, to evaluate GPT-3 as more of a general-purpose model. Adaptation. Pose the problem as question answering (demo): Results.  It doesn\u2019t work perfectly and can hardly be said to \u201cunderstand arithmetic\u201d fully, but it works surprisingly well.",
            "Adaptation. Note: in-context learning was needed to give the model an idea of what a prompt looks like. Results. Humans were able to able to detect classify \u201chuman\u201d versus \u201cmachine\u201d only 52% of the time (barely above random chance). For the article above, humans guessed \u201cmachine\u201d correctly only 12% of the time.",
            "Adaptation. Just describe the task in the prompt (demo): Adaptation. The prompt consists of input-output pairs (demo):",
            "Since the original paper, GPT-3 has been applied to many more tasks, including benchmark datasets and one-off demos. Here is an non-exhaustive list. Benchmarks. The performance on these benchmarks is still mediocre, but it\u2019s perhaps not bad given that we\u2019re doing few-shot learning! Demos. The demos are creative and interesting, but it\u2019s hard to tell how reliably they work.",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#triviaqa-joshi-et-al-2017": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#triviaqa-joshi-et-al-2017": ""
        },
        "text_between_h2": [
            "The most natural starting point for thinking about what a language model can do is to ask if it can do the thing that language models are supposed to do: model language. Recall that a language model \\(p\\) is a probability distribution over sequences of tokens. Suppose we take a corpus of text \\(x_{1:L}\\), for example: We can ask: what is the probability the language model assigns to it? Recall that we can break down the the joint probability into the product of the conditional probabilities for each token by the chain rule: Perplexity. The joint probability of a sequence depends on its length and thusgoes to zeroas the length grows, which makes it hard to track. (Just think about trying to get a better estimate of perplexity on newswire by getting more newswire.) Intuitively we want to average the per token probabilities \\(p(x_i \\mid x_{1:i-1})\\). We don\u2019t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn\u2019t penalize you for that. Instead, we want thegeometric average, which is exactly what perplexity does: Perplexity can be interpreted as theaverage \u201cbranching factor\u201dper token. Recall that \\(\\log \\frac{1}{p(x_i \\mid x_{1:i-1})}\\) is the code length. We are taking the average code length; exponentiating provides the number of possibilities. For intuition, take uniform distribution: a bitstring of length of 3 can encode \\(2^3\\) possible strings. Tale of two errors. There are two types of errors a language model can make, and perplexity treats them asymmetrically: Then we can compute the perplexity of \\(x_{1:L}\\) under \\(q\\): where the last approximate equality holds for small values of \\(\\epsilon\\). If we mix in 5% junk, then perplexity only by 5%. Note that the resulting language is horrible for generation, since every 20 tokens on average it\u2019s just going to generate a gibberish token. Now let\u2019s get on with evaluating perplexity on an actual dataset. ThePenn Tree Bankis a classic dataset in NLP, originally annotated for syntactic parsing. Beginning withEmami and Jelinek (2004)andMikolov and Zweig (2012), a version of the dataset that only contained Wall Street Journal articles was used as a language modeling evaluation. Note that the PTB language modeling benchmark involved some significant preprocessing of the original dataset (h/t toJohn Hewittfor pointing this out). Adaptation. Feed the entire text as a prompt into GPT-3 and evaluate the perplexity (demo): Results. GPT-3 vastly outperforms the existing state-of-the-art: ModelPerplexityGPT-320.5BERT-Large-CAs131.3 See theleaderboardfor the latest results. Train/test leakage. The authors did not evaluate on some datasets such asWikiText-103because GPT-3 was trained on Wikipedia. PTB had the advance of predating the Internet, and is only available through a paid license. This is another complication with large datasets: it is difficult to check that your test data did not appear in your training data and was memorized. Adaptation. Results. GPT-3 doesmuch betteron this task than the previous state-of-the-art (based on GPT-2): ModelPerplexityGPT-3 (few-shot)1.92SOTA8.63 See theleaderboardfor the latest results. Adaptation. This is amultiple-choice task, so the most natural thing to do is toscoreeach candidate answer with the language model and predict the \u201cbest\u201d one (demo): where ${answer} is one of: How do you score a candidate answer \\(y\\) given a question \\(x\\)? There\u2019s no principled answer, but here are someheuristics: Results. GPT-3 got close but did not exceed the state-of-the-art: ModelAccuracySOTA85.6GPT-379.3 However, the SOTA used fine-tuning on the HellaSwag training set, so it is pretty impressive that GPT-3 can get close without any task-specific training data! See theleaderboardfor the latest results.",
            "Now we consider (closed-book) question answering, where the input is a question and the output is an answer. Thelanguage model has to somehow \u201cknow\u201d the answerwithout looking up information in a database or a set of documents (we\u2019ll consider reading comprehension later, where the information is provided). Adaptation. We define a prompt based on the training instances (if any) and the question, and take the completion as the predicted answer (demo): Results. ModelAccuracyRAG68.0GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2 We also see that both increasing the model size and the number of in-context training instances helps:  Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG45.5GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5 Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG44.5GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9",
            "Adaptation. For the few-shot setting, we construct a prompt containing input-output training instances along with the input (demo): Results. Here are the results from German to English: ModelAccuracySOTA (supervised)40.2GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6",
            "GPT-3 is a language model (primarily on English), but we can evaluate it on a range of more \u201cabstract reasoning\u201d tasks, to evaluate GPT-3 as more of a general-purpose model. Adaptation. Pose the problem as question answering (demo): Results.  It doesn\u2019t work perfectly and can hardly be said to \u201cunderstand arithmetic\u201d fully, but it works surprisingly well.",
            "Adaptation. Note: in-context learning was needed to give the model an idea of what a prompt looks like. Results. Humans were able to able to detect classify \u201chuman\u201d versus \u201cmachine\u201d only 52% of the time (barely above random chance). For the article above, humans guessed \u201cmachine\u201d correctly only 12% of the time.",
            "Adaptation. Just describe the task in the prompt (demo): Adaptation. The prompt consists of input-output pairs (demo):",
            "Since the original paper, GPT-3 has been applied to many more tasks, including benchmark datasets and one-off demos. Here is an non-exhaustive list. Benchmarks. The performance on these benchmarks is still mediocre, but it\u2019s perhaps not bad given that we\u2019re doing few-shot learning! Demos. The demos are creative and interesting, but it\u2019s hard to tell how reliably they work.",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#webquestions-berant-et-al-2013": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#webquestions-berant-et-al-2013": ""
        },
        "text_between_h2": [
            "The most natural starting point for thinking about what a language model can do is to ask if it can do the thing that language models are supposed to do: model language. Recall that a language model \\(p\\) is a probability distribution over sequences of tokens. Suppose we take a corpus of text \\(x_{1:L}\\), for example: We can ask: what is the probability the language model assigns to it? Recall that we can break down the the joint probability into the product of the conditional probabilities for each token by the chain rule: Perplexity. The joint probability of a sequence depends on its length and thusgoes to zeroas the length grows, which makes it hard to track. (Just think about trying to get a better estimate of perplexity on newswire by getting more newswire.) Intuitively we want to average the per token probabilities \\(p(x_i \\mid x_{1:i-1})\\). We don\u2019t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn\u2019t penalize you for that. Instead, we want thegeometric average, which is exactly what perplexity does: Perplexity can be interpreted as theaverage \u201cbranching factor\u201dper token. Recall that \\(\\log \\frac{1}{p(x_i \\mid x_{1:i-1})}\\) is the code length. We are taking the average code length; exponentiating provides the number of possibilities. For intuition, take uniform distribution: a bitstring of length of 3 can encode \\(2^3\\) possible strings. Tale of two errors. There are two types of errors a language model can make, and perplexity treats them asymmetrically: Then we can compute the perplexity of \\(x_{1:L}\\) under \\(q\\): where the last approximate equality holds for small values of \\(\\epsilon\\). If we mix in 5% junk, then perplexity only by 5%. Note that the resulting language is horrible for generation, since every 20 tokens on average it\u2019s just going to generate a gibberish token. Now let\u2019s get on with evaluating perplexity on an actual dataset. ThePenn Tree Bankis a classic dataset in NLP, originally annotated for syntactic parsing. Beginning withEmami and Jelinek (2004)andMikolov and Zweig (2012), a version of the dataset that only contained Wall Street Journal articles was used as a language modeling evaluation. Note that the PTB language modeling benchmark involved some significant preprocessing of the original dataset (h/t toJohn Hewittfor pointing this out). Adaptation. Feed the entire text as a prompt into GPT-3 and evaluate the perplexity (demo): Results. GPT-3 vastly outperforms the existing state-of-the-art: ModelPerplexityGPT-320.5BERT-Large-CAs131.3 See theleaderboardfor the latest results. Train/test leakage. The authors did not evaluate on some datasets such asWikiText-103because GPT-3 was trained on Wikipedia. PTB had the advance of predating the Internet, and is only available through a paid license. This is another complication with large datasets: it is difficult to check that your test data did not appear in your training data and was memorized. Adaptation. Results. GPT-3 doesmuch betteron this task than the previous state-of-the-art (based on GPT-2): ModelPerplexityGPT-3 (few-shot)1.92SOTA8.63 See theleaderboardfor the latest results. Adaptation. This is amultiple-choice task, so the most natural thing to do is toscoreeach candidate answer with the language model and predict the \u201cbest\u201d one (demo): where ${answer} is one of: How do you score a candidate answer \\(y\\) given a question \\(x\\)? There\u2019s no principled answer, but here are someheuristics: Results. GPT-3 got close but did not exceed the state-of-the-art: ModelAccuracySOTA85.6GPT-379.3 However, the SOTA used fine-tuning on the HellaSwag training set, so it is pretty impressive that GPT-3 can get close without any task-specific training data! See theleaderboardfor the latest results.",
            "Now we consider (closed-book) question answering, where the input is a question and the output is an answer. Thelanguage model has to somehow \u201cknow\u201d the answerwithout looking up information in a database or a set of documents (we\u2019ll consider reading comprehension later, where the information is provided). Adaptation. We define a prompt based on the training instances (if any) and the question, and take the completion as the predicted answer (demo): Results. ModelAccuracyRAG68.0GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2 We also see that both increasing the model size and the number of in-context training instances helps:  Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG45.5GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5 Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG44.5GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9",
            "Adaptation. For the few-shot setting, we construct a prompt containing input-output training instances along with the input (demo): Results. Here are the results from German to English: ModelAccuracySOTA (supervised)40.2GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6",
            "GPT-3 is a language model (primarily on English), but we can evaluate it on a range of more \u201cabstract reasoning\u201d tasks, to evaluate GPT-3 as more of a general-purpose model. Adaptation. Pose the problem as question answering (demo): Results.  It doesn\u2019t work perfectly and can hardly be said to \u201cunderstand arithmetic\u201d fully, but it works surprisingly well.",
            "Adaptation. Note: in-context learning was needed to give the model an idea of what a prompt looks like. Results. Humans were able to able to detect classify \u201chuman\u201d versus \u201cmachine\u201d only 52% of the time (barely above random chance). For the article above, humans guessed \u201cmachine\u201d correctly only 12% of the time.",
            "Adaptation. Just describe the task in the prompt (demo): Adaptation. The prompt consists of input-output pairs (demo):",
            "Since the original paper, GPT-3 has been applied to many more tasks, including benchmark datasets and one-off demos. Here is an non-exhaustive list. Benchmarks. The performance on these benchmarks is still mediocre, but it\u2019s perhaps not bad given that we\u2019re doing few-shot learning! Demos. The demos are creative and interesting, but it\u2019s hard to tell how reliably they work.",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#naturalquestions": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#naturalquestions": ""
        },
        "text_between_h2": [
            "The most natural starting point for thinking about what a language model can do is to ask if it can do the thing that language models are supposed to do: model language. Recall that a language model \\(p\\) is a probability distribution over sequences of tokens. Suppose we take a corpus of text \\(x_{1:L}\\), for example: We can ask: what is the probability the language model assigns to it? Recall that we can break down the the joint probability into the product of the conditional probabilities for each token by the chain rule: Perplexity. The joint probability of a sequence depends on its length and thusgoes to zeroas the length grows, which makes it hard to track. (Just think about trying to get a better estimate of perplexity on newswire by getting more newswire.) Intuitively we want to average the per token probabilities \\(p(x_i \\mid x_{1:i-1})\\). We don\u2019t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn\u2019t penalize you for that. Instead, we want thegeometric average, which is exactly what perplexity does: Perplexity can be interpreted as theaverage \u201cbranching factor\u201dper token. Recall that \\(\\log \\frac{1}{p(x_i \\mid x_{1:i-1})}\\) is the code length. We are taking the average code length; exponentiating provides the number of possibilities. For intuition, take uniform distribution: a bitstring of length of 3 can encode \\(2^3\\) possible strings. Tale of two errors. There are two types of errors a language model can make, and perplexity treats them asymmetrically: Then we can compute the perplexity of \\(x_{1:L}\\) under \\(q\\): where the last approximate equality holds for small values of \\(\\epsilon\\). If we mix in 5% junk, then perplexity only by 5%. Note that the resulting language is horrible for generation, since every 20 tokens on average it\u2019s just going to generate a gibberish token. Now let\u2019s get on with evaluating perplexity on an actual dataset. ThePenn Tree Bankis a classic dataset in NLP, originally annotated for syntactic parsing. Beginning withEmami and Jelinek (2004)andMikolov and Zweig (2012), a version of the dataset that only contained Wall Street Journal articles was used as a language modeling evaluation. Note that the PTB language modeling benchmark involved some significant preprocessing of the original dataset (h/t toJohn Hewittfor pointing this out). Adaptation. Feed the entire text as a prompt into GPT-3 and evaluate the perplexity (demo): Results. GPT-3 vastly outperforms the existing state-of-the-art: ModelPerplexityGPT-320.5BERT-Large-CAs131.3 See theleaderboardfor the latest results. Train/test leakage. The authors did not evaluate on some datasets such asWikiText-103because GPT-3 was trained on Wikipedia. PTB had the advance of predating the Internet, and is only available through a paid license. This is another complication with large datasets: it is difficult to check that your test data did not appear in your training data and was memorized. Adaptation. Results. GPT-3 doesmuch betteron this task than the previous state-of-the-art (based on GPT-2): ModelPerplexityGPT-3 (few-shot)1.92SOTA8.63 See theleaderboardfor the latest results. Adaptation. This is amultiple-choice task, so the most natural thing to do is toscoreeach candidate answer with the language model and predict the \u201cbest\u201d one (demo): where ${answer} is one of: How do you score a candidate answer \\(y\\) given a question \\(x\\)? There\u2019s no principled answer, but here are someheuristics: Results. GPT-3 got close but did not exceed the state-of-the-art: ModelAccuracySOTA85.6GPT-379.3 However, the SOTA used fine-tuning on the HellaSwag training set, so it is pretty impressive that GPT-3 can get close without any task-specific training data! See theleaderboardfor the latest results.",
            "Now we consider (closed-book) question answering, where the input is a question and the output is an answer. Thelanguage model has to somehow \u201cknow\u201d the answerwithout looking up information in a database or a set of documents (we\u2019ll consider reading comprehension later, where the information is provided). Adaptation. We define a prompt based on the training instances (if any) and the question, and take the completion as the predicted answer (demo): Results. ModelAccuracyRAG68.0GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2 We also see that both increasing the model size and the number of in-context training instances helps:  Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG45.5GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5 Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG44.5GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9",
            "Adaptation. For the few-shot setting, we construct a prompt containing input-output training instances along with the input (demo): Results. Here are the results from German to English: ModelAccuracySOTA (supervised)40.2GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6",
            "GPT-3 is a language model (primarily on English), but we can evaluate it on a range of more \u201cabstract reasoning\u201d tasks, to evaluate GPT-3 as more of a general-purpose model. Adaptation. Pose the problem as question answering (demo): Results.  It doesn\u2019t work perfectly and can hardly be said to \u201cunderstand arithmetic\u201d fully, but it works surprisingly well.",
            "Adaptation. Note: in-context learning was needed to give the model an idea of what a prompt looks like. Results. Humans were able to able to detect classify \u201chuman\u201d versus \u201cmachine\u201d only 52% of the time (barely above random chance). For the article above, humans guessed \u201cmachine\u201d correctly only 12% of the time.",
            "Adaptation. Just describe the task in the prompt (demo): Adaptation. The prompt consists of input-output pairs (demo):",
            "Since the original paper, GPT-3 has been applied to many more tasks, including benchmark datasets and one-off demos. Here is an non-exhaustive list. Benchmarks. The performance on these benchmarks is still mediocre, but it\u2019s perhaps not bad given that we\u2019re doing few-shot learning! Demos. The demos are creative and interesting, but it\u2019s hard to tell how reliably they work.",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#using-new-words": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#using-new-words": ""
        },
        "text_between_h2": [
            "The most natural starting point for thinking about what a language model can do is to ask if it can do the thing that language models are supposed to do: model language. Recall that a language model \\(p\\) is a probability distribution over sequences of tokens. Suppose we take a corpus of text \\(x_{1:L}\\), for example: We can ask: what is the probability the language model assigns to it? Recall that we can break down the the joint probability into the product of the conditional probabilities for each token by the chain rule: Perplexity. The joint probability of a sequence depends on its length and thusgoes to zeroas the length grows, which makes it hard to track. (Just think about trying to get a better estimate of perplexity on newswire by getting more newswire.) Intuitively we want to average the per token probabilities \\(p(x_i \\mid x_{1:i-1})\\). We don\u2019t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn\u2019t penalize you for that. Instead, we want thegeometric average, which is exactly what perplexity does: Perplexity can be interpreted as theaverage \u201cbranching factor\u201dper token. Recall that \\(\\log \\frac{1}{p(x_i \\mid x_{1:i-1})}\\) is the code length. We are taking the average code length; exponentiating provides the number of possibilities. For intuition, take uniform distribution: a bitstring of length of 3 can encode \\(2^3\\) possible strings. Tale of two errors. There are two types of errors a language model can make, and perplexity treats them asymmetrically: Then we can compute the perplexity of \\(x_{1:L}\\) under \\(q\\): where the last approximate equality holds for small values of \\(\\epsilon\\). If we mix in 5% junk, then perplexity only by 5%. Note that the resulting language is horrible for generation, since every 20 tokens on average it\u2019s just going to generate a gibberish token. Now let\u2019s get on with evaluating perplexity on an actual dataset. ThePenn Tree Bankis a classic dataset in NLP, originally annotated for syntactic parsing. Beginning withEmami and Jelinek (2004)andMikolov and Zweig (2012), a version of the dataset that only contained Wall Street Journal articles was used as a language modeling evaluation. Note that the PTB language modeling benchmark involved some significant preprocessing of the original dataset (h/t toJohn Hewittfor pointing this out). Adaptation. Feed the entire text as a prompt into GPT-3 and evaluate the perplexity (demo): Results. GPT-3 vastly outperforms the existing state-of-the-art: ModelPerplexityGPT-320.5BERT-Large-CAs131.3 See theleaderboardfor the latest results. Train/test leakage. The authors did not evaluate on some datasets such asWikiText-103because GPT-3 was trained on Wikipedia. PTB had the advance of predating the Internet, and is only available through a paid license. This is another complication with large datasets: it is difficult to check that your test data did not appear in your training data and was memorized. Adaptation. Results. GPT-3 doesmuch betteron this task than the previous state-of-the-art (based on GPT-2): ModelPerplexityGPT-3 (few-shot)1.92SOTA8.63 See theleaderboardfor the latest results. Adaptation. This is amultiple-choice task, so the most natural thing to do is toscoreeach candidate answer with the language model and predict the \u201cbest\u201d one (demo): where ${answer} is one of: How do you score a candidate answer \\(y\\) given a question \\(x\\)? There\u2019s no principled answer, but here are someheuristics: Results. GPT-3 got close but did not exceed the state-of-the-art: ModelAccuracySOTA85.6GPT-379.3 However, the SOTA used fine-tuning on the HellaSwag training set, so it is pretty impressive that GPT-3 can get close without any task-specific training data! See theleaderboardfor the latest results.",
            "Now we consider (closed-book) question answering, where the input is a question and the output is an answer. Thelanguage model has to somehow \u201cknow\u201d the answerwithout looking up information in a database or a set of documents (we\u2019ll consider reading comprehension later, where the information is provided). Adaptation. We define a prompt based on the training instances (if any) and the question, and take the completion as the predicted answer (demo): Results. ModelAccuracyRAG68.0GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2 We also see that both increasing the model size and the number of in-context training instances helps:  Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG45.5GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5 Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG44.5GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9",
            "Adaptation. For the few-shot setting, we construct a prompt containing input-output training instances along with the input (demo): Results. Here are the results from German to English: ModelAccuracySOTA (supervised)40.2GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6",
            "GPT-3 is a language model (primarily on English), but we can evaluate it on a range of more \u201cabstract reasoning\u201d tasks, to evaluate GPT-3 as more of a general-purpose model. Adaptation. Pose the problem as question answering (demo): Results.  It doesn\u2019t work perfectly and can hardly be said to \u201cunderstand arithmetic\u201d fully, but it works surprisingly well.",
            "Adaptation. Note: in-context learning was needed to give the model an idea of what a prompt looks like. Results. Humans were able to able to detect classify \u201chuman\u201d versus \u201cmachine\u201d only 52% of the time (barely above random chance). For the article above, humans guessed \u201cmachine\u201d correctly only 12% of the time.",
            "Adaptation. Just describe the task in the prompt (demo): Adaptation. The prompt consists of input-output pairs (demo):",
            "Since the original paper, GPT-3 has been applied to many more tasks, including benchmark datasets and one-off demos. Here is an non-exhaustive list. Benchmarks. The performance on these benchmarks is still mediocre, but it\u2019s perhaps not bad given that we\u2019re doing few-shot learning! Demos. The demos are creative and interesting, but it\u2019s hard to tell how reliably they work.",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#correcting-english-grammar": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#correcting-english-grammar": ""
        },
        "text_between_h2": [
            "The most natural starting point for thinking about what a language model can do is to ask if it can do the thing that language models are supposed to do: model language. Recall that a language model \\(p\\) is a probability distribution over sequences of tokens. Suppose we take a corpus of text \\(x_{1:L}\\), for example: We can ask: what is the probability the language model assigns to it? Recall that we can break down the the joint probability into the product of the conditional probabilities for each token by the chain rule: Perplexity. The joint probability of a sequence depends on its length and thusgoes to zeroas the length grows, which makes it hard to track. (Just think about trying to get a better estimate of perplexity on newswire by getting more newswire.) Intuitively we want to average the per token probabilities \\(p(x_i \\mid x_{1:i-1})\\). We don\u2019t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn\u2019t penalize you for that. Instead, we want thegeometric average, which is exactly what perplexity does: Perplexity can be interpreted as theaverage \u201cbranching factor\u201dper token. Recall that \\(\\log \\frac{1}{p(x_i \\mid x_{1:i-1})}\\) is the code length. We are taking the average code length; exponentiating provides the number of possibilities. For intuition, take uniform distribution: a bitstring of length of 3 can encode \\(2^3\\) possible strings. Tale of two errors. There are two types of errors a language model can make, and perplexity treats them asymmetrically: Then we can compute the perplexity of \\(x_{1:L}\\) under \\(q\\): where the last approximate equality holds for small values of \\(\\epsilon\\). If we mix in 5% junk, then perplexity only by 5%. Note that the resulting language is horrible for generation, since every 20 tokens on average it\u2019s just going to generate a gibberish token. Now let\u2019s get on with evaluating perplexity on an actual dataset. ThePenn Tree Bankis a classic dataset in NLP, originally annotated for syntactic parsing. Beginning withEmami and Jelinek (2004)andMikolov and Zweig (2012), a version of the dataset that only contained Wall Street Journal articles was used as a language modeling evaluation. Note that the PTB language modeling benchmark involved some significant preprocessing of the original dataset (h/t toJohn Hewittfor pointing this out). Adaptation. Feed the entire text as a prompt into GPT-3 and evaluate the perplexity (demo): Results. GPT-3 vastly outperforms the existing state-of-the-art: ModelPerplexityGPT-320.5BERT-Large-CAs131.3 See theleaderboardfor the latest results. Train/test leakage. The authors did not evaluate on some datasets such asWikiText-103because GPT-3 was trained on Wikipedia. PTB had the advance of predating the Internet, and is only available through a paid license. This is another complication with large datasets: it is difficult to check that your test data did not appear in your training data and was memorized. Adaptation. Results. GPT-3 doesmuch betteron this task than the previous state-of-the-art (based on GPT-2): ModelPerplexityGPT-3 (few-shot)1.92SOTA8.63 See theleaderboardfor the latest results. Adaptation. This is amultiple-choice task, so the most natural thing to do is toscoreeach candidate answer with the language model and predict the \u201cbest\u201d one (demo): where ${answer} is one of: How do you score a candidate answer \\(y\\) given a question \\(x\\)? There\u2019s no principled answer, but here are someheuristics: Results. GPT-3 got close but did not exceed the state-of-the-art: ModelAccuracySOTA85.6GPT-379.3 However, the SOTA used fine-tuning on the HellaSwag training set, so it is pretty impressive that GPT-3 can get close without any task-specific training data! See theleaderboardfor the latest results.",
            "Now we consider (closed-book) question answering, where the input is a question and the output is an answer. Thelanguage model has to somehow \u201cknow\u201d the answerwithout looking up information in a database or a set of documents (we\u2019ll consider reading comprehension later, where the information is provided). Adaptation. We define a prompt based on the training instances (if any) and the question, and take the completion as the predicted answer (demo): Results. ModelAccuracyRAG68.0GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2 We also see that both increasing the model size and the number of in-context training instances helps:  Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG45.5GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5 Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG44.5GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9",
            "Adaptation. For the few-shot setting, we construct a prompt containing input-output training instances along with the input (demo): Results. Here are the results from German to English: ModelAccuracySOTA (supervised)40.2GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6",
            "GPT-3 is a language model (primarily on English), but we can evaluate it on a range of more \u201cabstract reasoning\u201d tasks, to evaluate GPT-3 as more of a general-purpose model. Adaptation. Pose the problem as question answering (demo): Results.  It doesn\u2019t work perfectly and can hardly be said to \u201cunderstand arithmetic\u201d fully, but it works surprisingly well.",
            "Adaptation. Note: in-context learning was needed to give the model an idea of what a prompt looks like. Results. Humans were able to able to detect classify \u201chuman\u201d versus \u201cmachine\u201d only 52% of the time (barely above random chance). For the article above, humans guessed \u201cmachine\u201d correctly only 12% of the time.",
            "Adaptation. Just describe the task in the prompt (demo): Adaptation. The prompt consists of input-output pairs (demo):",
            "Since the original paper, GPT-3 has been applied to many more tasks, including benchmark datasets and one-off demos. Here is an non-exhaustive list. Benchmarks. The performance on these benchmarks is still mediocre, but it\u2019s perhaps not bad given that we\u2019re doing few-shot learning! Demos. The demos are creative and interesting, but it\u2019s hard to tell how reliably they work.",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#other-tasks": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#other-tasks": ""
        },
        "text_between_h2": [
            "The most natural starting point for thinking about what a language model can do is to ask if it can do the thing that language models are supposed to do: model language. Recall that a language model \\(p\\) is a probability distribution over sequences of tokens. Suppose we take a corpus of text \\(x_{1:L}\\), for example: We can ask: what is the probability the language model assigns to it? Recall that we can break down the the joint probability into the product of the conditional probabilities for each token by the chain rule: Perplexity. The joint probability of a sequence depends on its length and thusgoes to zeroas the length grows, which makes it hard to track. (Just think about trying to get a better estimate of perplexity on newswire by getting more newswire.) Intuitively we want to average the per token probabilities \\(p(x_i \\mid x_{1:i-1})\\). We don\u2019t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn\u2019t penalize you for that. Instead, we want thegeometric average, which is exactly what perplexity does: Perplexity can be interpreted as theaverage \u201cbranching factor\u201dper token. Recall that \\(\\log \\frac{1}{p(x_i \\mid x_{1:i-1})}\\) is the code length. We are taking the average code length; exponentiating provides the number of possibilities. For intuition, take uniform distribution: a bitstring of length of 3 can encode \\(2^3\\) possible strings. Tale of two errors. There are two types of errors a language model can make, and perplexity treats them asymmetrically: Then we can compute the perplexity of \\(x_{1:L}\\) under \\(q\\): where the last approximate equality holds for small values of \\(\\epsilon\\). If we mix in 5% junk, then perplexity only by 5%. Note that the resulting language is horrible for generation, since every 20 tokens on average it\u2019s just going to generate a gibberish token. Now let\u2019s get on with evaluating perplexity on an actual dataset. ThePenn Tree Bankis a classic dataset in NLP, originally annotated for syntactic parsing. Beginning withEmami and Jelinek (2004)andMikolov and Zweig (2012), a version of the dataset that only contained Wall Street Journal articles was used as a language modeling evaluation. Note that the PTB language modeling benchmark involved some significant preprocessing of the original dataset (h/t toJohn Hewittfor pointing this out). Adaptation. Feed the entire text as a prompt into GPT-3 and evaluate the perplexity (demo): Results. GPT-3 vastly outperforms the existing state-of-the-art: ModelPerplexityGPT-320.5BERT-Large-CAs131.3 See theleaderboardfor the latest results. Train/test leakage. The authors did not evaluate on some datasets such asWikiText-103because GPT-3 was trained on Wikipedia. PTB had the advance of predating the Internet, and is only available through a paid license. This is another complication with large datasets: it is difficult to check that your test data did not appear in your training data and was memorized. Adaptation. Results. GPT-3 doesmuch betteron this task than the previous state-of-the-art (based on GPT-2): ModelPerplexityGPT-3 (few-shot)1.92SOTA8.63 See theleaderboardfor the latest results. Adaptation. This is amultiple-choice task, so the most natural thing to do is toscoreeach candidate answer with the language model and predict the \u201cbest\u201d one (demo): where ${answer} is one of: How do you score a candidate answer \\(y\\) given a question \\(x\\)? There\u2019s no principled answer, but here are someheuristics: Results. GPT-3 got close but did not exceed the state-of-the-art: ModelAccuracySOTA85.6GPT-379.3 However, the SOTA used fine-tuning on the HellaSwag training set, so it is pretty impressive that GPT-3 can get close without any task-specific training data! See theleaderboardfor the latest results.",
            "Now we consider (closed-book) question answering, where the input is a question and the output is an answer. Thelanguage model has to somehow \u201cknow\u201d the answerwithout looking up information in a database or a set of documents (we\u2019ll consider reading comprehension later, where the information is provided). Adaptation. We define a prompt based on the training instances (if any) and the question, and take the completion as the predicted answer (demo): Results. ModelAccuracyRAG68.0GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2 We also see that both increasing the model size and the number of in-context training instances helps:  Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG45.5GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5 Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG44.5GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9",
            "Adaptation. For the few-shot setting, we construct a prompt containing input-output training instances along with the input (demo): Results. Here are the results from German to English: ModelAccuracySOTA (supervised)40.2GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6",
            "GPT-3 is a language model (primarily on English), but we can evaluate it on a range of more \u201cabstract reasoning\u201d tasks, to evaluate GPT-3 as more of a general-purpose model. Adaptation. Pose the problem as question answering (demo): Results.  It doesn\u2019t work perfectly and can hardly be said to \u201cunderstand arithmetic\u201d fully, but it works surprisingly well.",
            "Adaptation. Note: in-context learning was needed to give the model an idea of what a prompt looks like. Results. Humans were able to able to detect classify \u201chuman\u201d versus \u201cmachine\u201d only 52% of the time (barely above random chance). For the article above, humans guessed \u201cmachine\u201d correctly only 12% of the time.",
            "Adaptation. Just describe the task in the prompt (demo): Adaptation. The prompt consists of input-output pairs (demo):",
            "Since the original paper, GPT-3 has been applied to many more tasks, including benchmark datasets and one-off demos. Here is an non-exhaustive list. Benchmarks. The performance on these benchmarks is still mediocre, but it\u2019s perhaps not bad given that we\u2019re doing few-shot learning! Demos. The demos are creative and interesting, but it\u2019s hard to tell how reliably they work.",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#summary": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#summary": ""
        },
        "text_between_h2": [
            "The most natural starting point for thinking about what a language model can do is to ask if it can do the thing that language models are supposed to do: model language. Recall that a language model \\(p\\) is a probability distribution over sequences of tokens. Suppose we take a corpus of text \\(x_{1:L}\\), for example: We can ask: what is the probability the language model assigns to it? Recall that we can break down the the joint probability into the product of the conditional probabilities for each token by the chain rule: Perplexity. The joint probability of a sequence depends on its length and thusgoes to zeroas the length grows, which makes it hard to track. (Just think about trying to get a better estimate of perplexity on newswire by getting more newswire.) Intuitively we want to average the per token probabilities \\(p(x_i \\mid x_{1:i-1})\\). We don\u2019t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn\u2019t penalize you for that. Instead, we want thegeometric average, which is exactly what perplexity does: Perplexity can be interpreted as theaverage \u201cbranching factor\u201dper token. Recall that \\(\\log \\frac{1}{p(x_i \\mid x_{1:i-1})}\\) is the code length. We are taking the average code length; exponentiating provides the number of possibilities. For intuition, take uniform distribution: a bitstring of length of 3 can encode \\(2^3\\) possible strings. Tale of two errors. There are two types of errors a language model can make, and perplexity treats them asymmetrically: Then we can compute the perplexity of \\(x_{1:L}\\) under \\(q\\): where the last approximate equality holds for small values of \\(\\epsilon\\). If we mix in 5% junk, then perplexity only by 5%. Note that the resulting language is horrible for generation, since every 20 tokens on average it\u2019s just going to generate a gibberish token. Now let\u2019s get on with evaluating perplexity on an actual dataset. ThePenn Tree Bankis a classic dataset in NLP, originally annotated for syntactic parsing. Beginning withEmami and Jelinek (2004)andMikolov and Zweig (2012), a version of the dataset that only contained Wall Street Journal articles was used as a language modeling evaluation. Note that the PTB language modeling benchmark involved some significant preprocessing of the original dataset (h/t toJohn Hewittfor pointing this out). Adaptation. Feed the entire text as a prompt into GPT-3 and evaluate the perplexity (demo): Results. GPT-3 vastly outperforms the existing state-of-the-art: ModelPerplexityGPT-320.5BERT-Large-CAs131.3 See theleaderboardfor the latest results. Train/test leakage. The authors did not evaluate on some datasets such asWikiText-103because GPT-3 was trained on Wikipedia. PTB had the advance of predating the Internet, and is only available through a paid license. This is another complication with large datasets: it is difficult to check that your test data did not appear in your training data and was memorized. Adaptation. Results. GPT-3 doesmuch betteron this task than the previous state-of-the-art (based on GPT-2): ModelPerplexityGPT-3 (few-shot)1.92SOTA8.63 See theleaderboardfor the latest results. Adaptation. This is amultiple-choice task, so the most natural thing to do is toscoreeach candidate answer with the language model and predict the \u201cbest\u201d one (demo): where ${answer} is one of: How do you score a candidate answer \\(y\\) given a question \\(x\\)? There\u2019s no principled answer, but here are someheuristics: Results. GPT-3 got close but did not exceed the state-of-the-art: ModelAccuracySOTA85.6GPT-379.3 However, the SOTA used fine-tuning on the HellaSwag training set, so it is pretty impressive that GPT-3 can get close without any task-specific training data! See theleaderboardfor the latest results.",
            "Now we consider (closed-book) question answering, where the input is a question and the output is an answer. Thelanguage model has to somehow \u201cknow\u201d the answerwithout looking up information in a database or a set of documents (we\u2019ll consider reading comprehension later, where the information is provided). Adaptation. We define a prompt based on the training instances (if any) and the question, and take the completion as the predicted answer (demo): Results. ModelAccuracyRAG68.0GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2 We also see that both increasing the model size and the number of in-context training instances helps:  Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG45.5GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5 Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG44.5GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9",
            "Adaptation. For the few-shot setting, we construct a prompt containing input-output training instances along with the input (demo): Results. Here are the results from German to English: ModelAccuracySOTA (supervised)40.2GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6",
            "GPT-3 is a language model (primarily on English), but we can evaluate it on a range of more \u201cabstract reasoning\u201d tasks, to evaluate GPT-3 as more of a general-purpose model. Adaptation. Pose the problem as question answering (demo): Results.  It doesn\u2019t work perfectly and can hardly be said to \u201cunderstand arithmetic\u201d fully, but it works surprisingly well.",
            "Adaptation. Note: in-context learning was needed to give the model an idea of what a prompt looks like. Results. Humans were able to able to detect classify \u201chuman\u201d versus \u201cmachine\u201d only 52% of the time (barely above random chance). For the article above, humans guessed \u201cmachine\u201d correctly only 12% of the time.",
            "Adaptation. Just describe the task in the prompt (demo): Adaptation. The prompt consists of input-output pairs (demo):",
            "Since the original paper, GPT-3 has been applied to many more tasks, including benchmark datasets and one-off demos. Here is an non-exhaustive list. Benchmarks. The performance on these benchmarks is still mediocre, but it\u2019s perhaps not bad given that we\u2019re doing few-shot learning! Demos. The demos are creative and interesting, but it\u2019s hard to tell how reliably they work.",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#further-reading": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/capabilities/#further-reading": ""
        },
        "text_between_h2": [
            "The most natural starting point for thinking about what a language model can do is to ask if it can do the thing that language models are supposed to do: model language. Recall that a language model \\(p\\) is a probability distribution over sequences of tokens. Suppose we take a corpus of text \\(x_{1:L}\\), for example: We can ask: what is the probability the language model assigns to it? Recall that we can break down the the joint probability into the product of the conditional probabilities for each token by the chain rule: Perplexity. The joint probability of a sequence depends on its length and thusgoes to zeroas the length grows, which makes it hard to track. (Just think about trying to get a better estimate of perplexity on newswire by getting more newswire.) Intuitively we want to average the per token probabilities \\(p(x_i \\mid x_{1:i-1})\\). We don\u2019t want to take the arithmetic average because assigning a token probability 0 is really bad (think about coding: your code length would be infinite), but the arithmetic average doesn\u2019t penalize you for that. Instead, we want thegeometric average, which is exactly what perplexity does: Perplexity can be interpreted as theaverage \u201cbranching factor\u201dper token. Recall that \\(\\log \\frac{1}{p(x_i \\mid x_{1:i-1})}\\) is the code length. We are taking the average code length; exponentiating provides the number of possibilities. For intuition, take uniform distribution: a bitstring of length of 3 can encode \\(2^3\\) possible strings. Tale of two errors. There are two types of errors a language model can make, and perplexity treats them asymmetrically: Then we can compute the perplexity of \\(x_{1:L}\\) under \\(q\\): where the last approximate equality holds for small values of \\(\\epsilon\\). If we mix in 5% junk, then perplexity only by 5%. Note that the resulting language is horrible for generation, since every 20 tokens on average it\u2019s just going to generate a gibberish token. Now let\u2019s get on with evaluating perplexity on an actual dataset. ThePenn Tree Bankis a classic dataset in NLP, originally annotated for syntactic parsing. Beginning withEmami and Jelinek (2004)andMikolov and Zweig (2012), a version of the dataset that only contained Wall Street Journal articles was used as a language modeling evaluation. Note that the PTB language modeling benchmark involved some significant preprocessing of the original dataset (h/t toJohn Hewittfor pointing this out). Adaptation. Feed the entire text as a prompt into GPT-3 and evaluate the perplexity (demo): Results. GPT-3 vastly outperforms the existing state-of-the-art: ModelPerplexityGPT-320.5BERT-Large-CAs131.3 See theleaderboardfor the latest results. Train/test leakage. The authors did not evaluate on some datasets such asWikiText-103because GPT-3 was trained on Wikipedia. PTB had the advance of predating the Internet, and is only available through a paid license. This is another complication with large datasets: it is difficult to check that your test data did not appear in your training data and was memorized. Adaptation. Results. GPT-3 doesmuch betteron this task than the previous state-of-the-art (based on GPT-2): ModelPerplexityGPT-3 (few-shot)1.92SOTA8.63 See theleaderboardfor the latest results. Adaptation. This is amultiple-choice task, so the most natural thing to do is toscoreeach candidate answer with the language model and predict the \u201cbest\u201d one (demo): where ${answer} is one of: How do you score a candidate answer \\(y\\) given a question \\(x\\)? There\u2019s no principled answer, but here are someheuristics: Results. GPT-3 got close but did not exceed the state-of-the-art: ModelAccuracySOTA85.6GPT-379.3 However, the SOTA used fine-tuning on the HellaSwag training set, so it is pretty impressive that GPT-3 can get close without any task-specific training data! See theleaderboardfor the latest results.",
            "Now we consider (closed-book) question answering, where the input is a question and the output is an answer. Thelanguage model has to somehow \u201cknow\u201d the answerwithout looking up information in a database or a set of documents (we\u2019ll consider reading comprehension later, where the information is provided). Adaptation. We define a prompt based on the training instances (if any) and the question, and take the completion as the predicted answer (demo): Results. ModelAccuracyRAG68.0GPT-3 (zero-shot)64.3GPT-3 (few-shot)71.2 We also see that both increasing the model size and the number of in-context training instances helps:  Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG45.5GPT-3 (zero-shot)14.4GPT-3 (few-shot)41.5 Adaptation. We define a prompt the same as above (demo): Results. ModelAccuracyRAG44.5GPT-3 (zero-shot)14.6GPT-3 (few-shot)29.9",
            "Adaptation. For the few-shot setting, we construct a prompt containing input-output training instances along with the input (demo): Results. Here are the results from German to English: ModelAccuracySOTA (supervised)40.2GPT-3 (zero-shot)27.2GPT-3 (few-shot)40.6",
            "GPT-3 is a language model (primarily on English), but we can evaluate it on a range of more \u201cabstract reasoning\u201d tasks, to evaluate GPT-3 as more of a general-purpose model. Adaptation. Pose the problem as question answering (demo): Results.  It doesn\u2019t work perfectly and can hardly be said to \u201cunderstand arithmetic\u201d fully, but it works surprisingly well.",
            "Adaptation. Note: in-context learning was needed to give the model an idea of what a prompt looks like. Results. Humans were able to able to detect classify \u201chuman\u201d versus \u201cmachine\u201d only 52% of the time (barely above random chance). For the article above, humans guessed \u201cmachine\u201d correctly only 12% of the time.",
            "Adaptation. Just describe the task in the prompt (demo): Adaptation. The prompt consists of input-output pairs (demo):",
            "Since the original paper, GPT-3 has been applied to many more tasks, including benchmark datasets and one-off demos. Here is an non-exhaustive list. Benchmarks. The performance on these benchmarks is still mediocre, but it\u2019s perhaps not bad given that we\u2019re doing few-shot learning! Demos. The demos are creative and interesting, but it\u2019s hard to tell how reliably they work.",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/harms-1/": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/harms-1/": "lecture two on capabilities",
            "https://stanford-cs324.github.io/winter2022/lectures/harms-1/#social-groups": "",
            "https://stanford-cs324.github.io/winter2022/lectures/harms-1/#examples-of-performance-disparities-in-llms": "",
            "https://stanford-cs324.github.io/winter2022/lectures/harms-1/#examples-of-social-biases-and-stereotypes-in-llms": "",
            "https://stanford-cs324.github.io/winter2022/lectures/harms-1/#measurement": "",
            "https://stanford-cs324.github.io/winter2022/lectures/harms-1/#other-considerations": "",
            "https://stanford-cs324.github.io/winter2022/lectures/harms-1/#further-reading": ""
        },
        "text_between_h2": [
            "Social Groups in Language.For text, we can identify social groups based on the: Identifying Social Groups. What Social Groups are of interest? Historically Marginalization.",
            "Name Artifacts (Schwartz et al. 2020). Results: ModelParametersOriginal acc.Modified acc.FlipsRoBERTa-base123M91.249.615.7RoBERTa-large354M94.482.29.8RoBERTA-large w/RACE354M94.487.97.7 See thepaperfor the full results.",
            "Large language models associate Muslims with Violence (Abid et al., 2021). Results. StereoSet (Nadeem et al., 2021). Results. All models show a systematic preference for stereotypical data. Larger models tend to have higher stereotype scores. ModelParametersStereotype ScoreGPT-2 Small117M56.4GPT-2 Medium345M58.2GPT-2 Large774M60.0 See theleaderboardfor the latest results.",
            "",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/harms-1/#social-groups": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/harms-1/#social-groups": ""
        },
        "text_between_h2": [
            "Social Groups in Language.For text, we can identify social groups based on the: Identifying Social Groups. What Social Groups are of interest? Historically Marginalization.",
            "Name Artifacts (Schwartz et al. 2020). Results: ModelParametersOriginal acc.Modified acc.FlipsRoBERTa-base123M91.249.615.7RoBERTa-large354M94.482.29.8RoBERTA-large w/RACE354M94.487.97.7 See thepaperfor the full results.",
            "Large language models associate Muslims with Violence (Abid et al., 2021). Results. StereoSet (Nadeem et al., 2021). Results. All models show a systematic preference for stereotypical data. Larger models tend to have higher stereotype scores. ModelParametersStereotype ScoreGPT-2 Small117M56.4GPT-2 Medium345M58.2GPT-2 Large774M60.0 See theleaderboardfor the latest results.",
            "",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/harms-1/#examples-of-performance-disparities-in-llms": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/harms-1/#examples-of-performance-disparities-in-llms": ""
        },
        "text_between_h2": [
            "Social Groups in Language.For text, we can identify social groups based on the: Identifying Social Groups. What Social Groups are of interest? Historically Marginalization.",
            "Name Artifacts (Schwartz et al. 2020). Results: ModelParametersOriginal acc.Modified acc.FlipsRoBERTa-base123M91.249.615.7RoBERTa-large354M94.482.29.8RoBERTA-large w/RACE354M94.487.97.7 See thepaperfor the full results.",
            "Large language models associate Muslims with Violence (Abid et al., 2021). Results. StereoSet (Nadeem et al., 2021). Results. All models show a systematic preference for stereotypical data. Larger models tend to have higher stereotype scores. ModelParametersStereotype ScoreGPT-2 Small117M56.4GPT-2 Medium345M58.2GPT-2 Large774M60.0 See theleaderboardfor the latest results.",
            "",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/harms-1/#examples-of-social-biases-and-stereotypes-in-llms": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/harms-1/#examples-of-social-biases-and-stereotypes-in-llms": ""
        },
        "text_between_h2": [
            "Social Groups in Language.For text, we can identify social groups based on the: Identifying Social Groups. What Social Groups are of interest? Historically Marginalization.",
            "Name Artifacts (Schwartz et al. 2020). Results: ModelParametersOriginal acc.Modified acc.FlipsRoBERTa-base123M91.249.615.7RoBERTa-large354M94.482.29.8RoBERTA-large w/RACE354M94.487.97.7 See thepaperfor the full results.",
            "Large language models associate Muslims with Violence (Abid et al., 2021). Results. StereoSet (Nadeem et al., 2021). Results. All models show a systematic preference for stereotypical data. Larger models tend to have higher stereotype scores. ModelParametersStereotype ScoreGPT-2 Small117M56.4GPT-2 Medium345M58.2GPT-2 Large774M60.0 See theleaderboardfor the latest results.",
            "",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/harms-1/#measurement": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/harms-1/#measurement": ""
        },
        "text_between_h2": [
            "Social Groups in Language.For text, we can identify social groups based on the: Identifying Social Groups. What Social Groups are of interest? Historically Marginalization.",
            "Name Artifacts (Schwartz et al. 2020). Results: ModelParametersOriginal acc.Modified acc.FlipsRoBERTa-base123M91.249.615.7RoBERTa-large354M94.482.29.8RoBERTA-large w/RACE354M94.487.97.7 See thepaperfor the full results.",
            "Large language models associate Muslims with Violence (Abid et al., 2021). Results. StereoSet (Nadeem et al., 2021). Results. All models show a systematic preference for stereotypical data. Larger models tend to have higher stereotype scores. ModelParametersStereotype ScoreGPT-2 Small117M56.4GPT-2 Medium345M58.2GPT-2 Large774M60.0 See theleaderboardfor the latest results.",
            "",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/harms-1/#other-considerations": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/harms-1/#other-considerations": ""
        },
        "text_between_h2": [
            "Social Groups in Language.For text, we can identify social groups based on the: Identifying Social Groups. What Social Groups are of interest? Historically Marginalization.",
            "Name Artifacts (Schwartz et al. 2020). Results: ModelParametersOriginal acc.Modified acc.FlipsRoBERTa-base123M91.249.615.7RoBERTa-large354M94.482.29.8RoBERTA-large w/RACE354M94.487.97.7 See thepaperfor the full results.",
            "Large language models associate Muslims with Violence (Abid et al., 2021). Results. StereoSet (Nadeem et al., 2021). Results. All models show a systematic preference for stereotypical data. Larger models tend to have higher stereotype scores. ModelParametersStereotype ScoreGPT-2 Small117M56.4GPT-2 Medium345M58.2GPT-2 Large774M60.0 See theleaderboardfor the latest results.",
            "",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/harms-1/#further-reading": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/harms-1/#further-reading": ""
        },
        "text_between_h2": [
            "Social Groups in Language.For text, we can identify social groups based on the: Identifying Social Groups. What Social Groups are of interest? Historically Marginalization.",
            "Name Artifacts (Schwartz et al. 2020). Results: ModelParametersOriginal acc.Modified acc.FlipsRoBERTa-base123M91.249.615.7RoBERTa-large354M94.482.29.8RoBERTA-large w/RACE354M94.487.97.7 See thepaperfor the full results.",
            "Large language models associate Muslims with Violence (Abid et al., 2021). Results. StereoSet (Nadeem et al., 2021). Results. All models show a systematic preference for stereotypical data. Larger models tend to have higher stereotype scores. ModelParametersStereotype ScoreGPT-2 Small117M56.4GPT-2 Medium345M58.2GPT-2 Large774M60.0 See theleaderboardfor the latest results.",
            "",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/harms-2/": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/harms-2/": "Harms II",
            "https://stanford-cs324.github.io/winter2022/lectures/harms-2/#overview": "",
            "https://stanford-cs324.github.io/winter2022/lectures/harms-2/#content-moderation": "",
            "https://stanford-cs324.github.io/winter2022/lectures/harms-2/#toxicity": "",
            "https://stanford-cs324.github.io/winter2022/lectures/harms-2/#perspective-api": "",
            "https://stanford-cs324.github.io/winter2022/lectures/harms-2/#realtoxicityprompts": "",
            "https://stanford-cs324.github.io/winter2022/lectures/harms-2/#summary": "",
            "https://stanford-cs324.github.io/winter2022/lectures/harms-2/#disinformation": "",
            "https://stanford-cs324.github.io/winter2022/lectures/harms-2/#content-moderation-1": "",
            "https://stanford-cs324.github.io/winter2022/lectures/harms-2/#further-reading": ""
        },
        "text_between_h2": [
            "In this lecture, we will discuss two more behavioral harms: Before we dive in, we should point out a disconnect: Before we get to large language models, it is helpful to ground out toxicity and disinformation in the very critical problem of content moderation. Context-dependence. What constitutes harmful content is verycontext-dependent.Chandrasekhran et al. 2018performed a detailed study on Reddit: While there are norms common to almost all subreddits, many norms are specific to subreddits, for example: Dual use. There are two ways in which language models can be used in the context of toxicity and disinformation:",
            "We want to understand the harms of large language models related to toxicity. There are two possible recipients of the harm: Working definition. What is toxicity? As mentioned above, harms are about what happens to people, so it is important to remember that the definition is very context-dependent. To make some progress, we can use the following working definition:Borkan et al, 2017defines toxicity as anything that is \u201crude, disrespectful, or unreasonable that would make someone want to leave a conversation.\u201d Examples: Word lists. How far can one get by simply defining toxicity in terms of presence of certain\u201cbad words\u201d? Aside: The Clossal, Cleaned Common Crawl (C4) dataset was filtered using this word list and used to train the T5 language model. We will talk about the complexities of data later in the course. Using a word list is inadequate because: Jigaw, a unit within Google focused on technological solutions to social problems (e.g., extremism), developed a popular (proprietary) service for performing toxicity classification called the Perspective API in 2017. You cantry it out here. Anecdotally, it works for some things: However, it doesn\u2019t always work: In general, the Perspective API suffers from a few related problems: While the Perspective API is a popular starting point that is used by the ML and NLP community, it is important to take it with a moderate grain of salt. Gehman et al, 2020introduced a dataset to evaluate the toxicity of generation from a language model. For example (demo;warning: contains offensive content): Caveats. Unprompted experiments. Prompting experiments.  Takeaway: possible to generate \u201ctoxic\u201d completions even given \u201cnon-toxic\u201d prompts. Mitigating toxicity. InterventionNo promptsNon-toxic promptsToxic promptsDo nothing44%51%75%Data-based (DAPT)30%37%57%Decoding-based (PPLM)28%32%52% But reducing toxicity isn\u2019t the only thing that matters (otherwise there are trivial solutions).",
            "Terminology (further discussion): Note that misinformation and disinformationneed not be falsifiable; sometimes it incites or shifts burden of proof to the audience. Things that are not true, but don\u2019t count as misinformation or disinformation: Disinformation can is created on behalf of a malicious actor and disseminated, often on social media platforms (Facebook, Twitter). Examples of disinformation: The state of disinformation campaigns: The economics: Some relevant work:",
            "We\u2019ve talked about language models generating toxic content, but if they can generate it, they might also be used to detect it and other harmful content. Facebook (or Meta) has been fighting toxicity for a long time and recently been leveraging language models to automatically detect it. For example,RoBERTahas been used for a few years. TheFew-Shot Learneris Meta\u2019s latest powerful model for content moderation.  Some anecdotal examples of subtle utterances that are classifed correctly as harmful content:",
            "Performance disparities: Content moderation: Toxicity: Disinformation:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/harms-2/#overview": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/harms-2/#overview": ""
        },
        "text_between_h2": [
            "In this lecture, we will discuss two more behavioral harms: Before we dive in, we should point out a disconnect: Before we get to large language models, it is helpful to ground out toxicity and disinformation in the very critical problem of content moderation. Context-dependence. What constitutes harmful content is verycontext-dependent.Chandrasekhran et al. 2018performed a detailed study on Reddit: While there are norms common to almost all subreddits, many norms are specific to subreddits, for example: Dual use. There are two ways in which language models can be used in the context of toxicity and disinformation:",
            "We want to understand the harms of large language models related to toxicity. There are two possible recipients of the harm: Working definition. What is toxicity? As mentioned above, harms are about what happens to people, so it is important to remember that the definition is very context-dependent. To make some progress, we can use the following working definition:Borkan et al, 2017defines toxicity as anything that is \u201crude, disrespectful, or unreasonable that would make someone want to leave a conversation.\u201d Examples: Word lists. How far can one get by simply defining toxicity in terms of presence of certain\u201cbad words\u201d? Aside: The Clossal, Cleaned Common Crawl (C4) dataset was filtered using this word list and used to train the T5 language model. We will talk about the complexities of data later in the course. Using a word list is inadequate because: Jigaw, a unit within Google focused on technological solutions to social problems (e.g., extremism), developed a popular (proprietary) service for performing toxicity classification called the Perspective API in 2017. You cantry it out here. Anecdotally, it works for some things: However, it doesn\u2019t always work: In general, the Perspective API suffers from a few related problems: While the Perspective API is a popular starting point that is used by the ML and NLP community, it is important to take it with a moderate grain of salt. Gehman et al, 2020introduced a dataset to evaluate the toxicity of generation from a language model. For example (demo;warning: contains offensive content): Caveats. Unprompted experiments. Prompting experiments.  Takeaway: possible to generate \u201ctoxic\u201d completions even given \u201cnon-toxic\u201d prompts. Mitigating toxicity. InterventionNo promptsNon-toxic promptsToxic promptsDo nothing44%51%75%Data-based (DAPT)30%37%57%Decoding-based (PPLM)28%32%52% But reducing toxicity isn\u2019t the only thing that matters (otherwise there are trivial solutions).",
            "Terminology (further discussion): Note that misinformation and disinformationneed not be falsifiable; sometimes it incites or shifts burden of proof to the audience. Things that are not true, but don\u2019t count as misinformation or disinformation: Disinformation can is created on behalf of a malicious actor and disseminated, often on social media platforms (Facebook, Twitter). Examples of disinformation: The state of disinformation campaigns: The economics: Some relevant work:",
            "We\u2019ve talked about language models generating toxic content, but if they can generate it, they might also be used to detect it and other harmful content. Facebook (or Meta) has been fighting toxicity for a long time and recently been leveraging language models to automatically detect it. For example,RoBERTahas been used for a few years. TheFew-Shot Learneris Meta\u2019s latest powerful model for content moderation.  Some anecdotal examples of subtle utterances that are classifed correctly as harmful content:",
            "Performance disparities: Content moderation: Toxicity: Disinformation:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/harms-2/#content-moderation": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/harms-2/#content-moderation": "",
            "https://stanford-cs324.github.io/winter2022/lectures/harms-2/#content-moderation-1": ""
        },
        "text_between_h2": [
            "In this lecture, we will discuss two more behavioral harms: Before we dive in, we should point out a disconnect: Before we get to large language models, it is helpful to ground out toxicity and disinformation in the very critical problem of content moderation. Context-dependence. What constitutes harmful content is verycontext-dependent.Chandrasekhran et al. 2018performed a detailed study on Reddit: While there are norms common to almost all subreddits, many norms are specific to subreddits, for example: Dual use. There are two ways in which language models can be used in the context of toxicity and disinformation:",
            "We want to understand the harms of large language models related to toxicity. There are two possible recipients of the harm: Working definition. What is toxicity? As mentioned above, harms are about what happens to people, so it is important to remember that the definition is very context-dependent. To make some progress, we can use the following working definition:Borkan et al, 2017defines toxicity as anything that is \u201crude, disrespectful, or unreasonable that would make someone want to leave a conversation.\u201d Examples: Word lists. How far can one get by simply defining toxicity in terms of presence of certain\u201cbad words\u201d? Aside: The Clossal, Cleaned Common Crawl (C4) dataset was filtered using this word list and used to train the T5 language model. We will talk about the complexities of data later in the course. Using a word list is inadequate because: Jigaw, a unit within Google focused on technological solutions to social problems (e.g., extremism), developed a popular (proprietary) service for performing toxicity classification called the Perspective API in 2017. You cantry it out here. Anecdotally, it works for some things: However, it doesn\u2019t always work: In general, the Perspective API suffers from a few related problems: While the Perspective API is a popular starting point that is used by the ML and NLP community, it is important to take it with a moderate grain of salt. Gehman et al, 2020introduced a dataset to evaluate the toxicity of generation from a language model. For example (demo;warning: contains offensive content): Caveats. Unprompted experiments. Prompting experiments.  Takeaway: possible to generate \u201ctoxic\u201d completions even given \u201cnon-toxic\u201d prompts. Mitigating toxicity. InterventionNo promptsNon-toxic promptsToxic promptsDo nothing44%51%75%Data-based (DAPT)30%37%57%Decoding-based (PPLM)28%32%52% But reducing toxicity isn\u2019t the only thing that matters (otherwise there are trivial solutions).",
            "Terminology (further discussion): Note that misinformation and disinformationneed not be falsifiable; sometimes it incites or shifts burden of proof to the audience. Things that are not true, but don\u2019t count as misinformation or disinformation: Disinformation can is created on behalf of a malicious actor and disseminated, often on social media platforms (Facebook, Twitter). Examples of disinformation: The state of disinformation campaigns: The economics: Some relevant work:",
            "We\u2019ve talked about language models generating toxic content, but if they can generate it, they might also be used to detect it and other harmful content. Facebook (or Meta) has been fighting toxicity for a long time and recently been leveraging language models to automatically detect it. For example,RoBERTahas been used for a few years. TheFew-Shot Learneris Meta\u2019s latest powerful model for content moderation.  Some anecdotal examples of subtle utterances that are classifed correctly as harmful content:",
            "Performance disparities: Content moderation: Toxicity: Disinformation:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/harms-2/#content-moderation-1": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/harms-2/#content-moderation-1": ""
        },
        "text_between_h2": [
            "In this lecture, we will discuss two more behavioral harms: Before we dive in, we should point out a disconnect: Before we get to large language models, it is helpful to ground out toxicity and disinformation in the very critical problem of content moderation. Context-dependence. What constitutes harmful content is verycontext-dependent.Chandrasekhran et al. 2018performed a detailed study on Reddit: While there are norms common to almost all subreddits, many norms are specific to subreddits, for example: Dual use. There are two ways in which language models can be used in the context of toxicity and disinformation:",
            "We want to understand the harms of large language models related to toxicity. There are two possible recipients of the harm: Working definition. What is toxicity? As mentioned above, harms are about what happens to people, so it is important to remember that the definition is very context-dependent. To make some progress, we can use the following working definition:Borkan et al, 2017defines toxicity as anything that is \u201crude, disrespectful, or unreasonable that would make someone want to leave a conversation.\u201d Examples: Word lists. How far can one get by simply defining toxicity in terms of presence of certain\u201cbad words\u201d? Aside: The Clossal, Cleaned Common Crawl (C4) dataset was filtered using this word list and used to train the T5 language model. We will talk about the complexities of data later in the course. Using a word list is inadequate because: Jigaw, a unit within Google focused on technological solutions to social problems (e.g., extremism), developed a popular (proprietary) service for performing toxicity classification called the Perspective API in 2017. You cantry it out here. Anecdotally, it works for some things: However, it doesn\u2019t always work: In general, the Perspective API suffers from a few related problems: While the Perspective API is a popular starting point that is used by the ML and NLP community, it is important to take it with a moderate grain of salt. Gehman et al, 2020introduced a dataset to evaluate the toxicity of generation from a language model. For example (demo;warning: contains offensive content): Caveats. Unprompted experiments. Prompting experiments.  Takeaway: possible to generate \u201ctoxic\u201d completions even given \u201cnon-toxic\u201d prompts. Mitigating toxicity. InterventionNo promptsNon-toxic promptsToxic promptsDo nothing44%51%75%Data-based (DAPT)30%37%57%Decoding-based (PPLM)28%32%52% But reducing toxicity isn\u2019t the only thing that matters (otherwise there are trivial solutions).",
            "Terminology (further discussion): Note that misinformation and disinformationneed not be falsifiable; sometimes it incites or shifts burden of proof to the audience. Things that are not true, but don\u2019t count as misinformation or disinformation: Disinformation can is created on behalf of a malicious actor and disseminated, often on social media platforms (Facebook, Twitter). Examples of disinformation: The state of disinformation campaigns: The economics: Some relevant work:",
            "We\u2019ve talked about language models generating toxic content, but if they can generate it, they might also be used to detect it and other harmful content. Facebook (or Meta) has been fighting toxicity for a long time and recently been leveraging language models to automatically detect it. For example,RoBERTahas been used for a few years. TheFew-Shot Learneris Meta\u2019s latest powerful model for content moderation.  Some anecdotal examples of subtle utterances that are classifed correctly as harmful content:",
            "Performance disparities: Content moderation: Toxicity: Disinformation:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/harms-2/#toxicity": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/harms-2/#toxicity": ""
        },
        "text_between_h2": [
            "In this lecture, we will discuss two more behavioral harms: Before we dive in, we should point out a disconnect: Before we get to large language models, it is helpful to ground out toxicity and disinformation in the very critical problem of content moderation. Context-dependence. What constitutes harmful content is verycontext-dependent.Chandrasekhran et al. 2018performed a detailed study on Reddit: While there are norms common to almost all subreddits, many norms are specific to subreddits, for example: Dual use. There are two ways in which language models can be used in the context of toxicity and disinformation:",
            "We want to understand the harms of large language models related to toxicity. There are two possible recipients of the harm: Working definition. What is toxicity? As mentioned above, harms are about what happens to people, so it is important to remember that the definition is very context-dependent. To make some progress, we can use the following working definition:Borkan et al, 2017defines toxicity as anything that is \u201crude, disrespectful, or unreasonable that would make someone want to leave a conversation.\u201d Examples: Word lists. How far can one get by simply defining toxicity in terms of presence of certain\u201cbad words\u201d? Aside: The Clossal, Cleaned Common Crawl (C4) dataset was filtered using this word list and used to train the T5 language model. We will talk about the complexities of data later in the course. Using a word list is inadequate because: Jigaw, a unit within Google focused on technological solutions to social problems (e.g., extremism), developed a popular (proprietary) service for performing toxicity classification called the Perspective API in 2017. You cantry it out here. Anecdotally, it works for some things: However, it doesn\u2019t always work: In general, the Perspective API suffers from a few related problems: While the Perspective API is a popular starting point that is used by the ML and NLP community, it is important to take it with a moderate grain of salt. Gehman et al, 2020introduced a dataset to evaluate the toxicity of generation from a language model. For example (demo;warning: contains offensive content): Caveats. Unprompted experiments. Prompting experiments.  Takeaway: possible to generate \u201ctoxic\u201d completions even given \u201cnon-toxic\u201d prompts. Mitigating toxicity. InterventionNo promptsNon-toxic promptsToxic promptsDo nothing44%51%75%Data-based (DAPT)30%37%57%Decoding-based (PPLM)28%32%52% But reducing toxicity isn\u2019t the only thing that matters (otherwise there are trivial solutions).",
            "Terminology (further discussion): Note that misinformation and disinformationneed not be falsifiable; sometimes it incites or shifts burden of proof to the audience. Things that are not true, but don\u2019t count as misinformation or disinformation: Disinformation can is created on behalf of a malicious actor and disseminated, often on social media platforms (Facebook, Twitter). Examples of disinformation: The state of disinformation campaigns: The economics: Some relevant work:",
            "We\u2019ve talked about language models generating toxic content, but if they can generate it, they might also be used to detect it and other harmful content. Facebook (or Meta) has been fighting toxicity for a long time and recently been leveraging language models to automatically detect it. For example,RoBERTahas been used for a few years. TheFew-Shot Learneris Meta\u2019s latest powerful model for content moderation.  Some anecdotal examples of subtle utterances that are classifed correctly as harmful content:",
            "Performance disparities: Content moderation: Toxicity: Disinformation:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/harms-2/#perspective-api": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/harms-2/#perspective-api": ""
        },
        "text_between_h2": [
            "In this lecture, we will discuss two more behavioral harms: Before we dive in, we should point out a disconnect: Before we get to large language models, it is helpful to ground out toxicity and disinformation in the very critical problem of content moderation. Context-dependence. What constitutes harmful content is verycontext-dependent.Chandrasekhran et al. 2018performed a detailed study on Reddit: While there are norms common to almost all subreddits, many norms are specific to subreddits, for example: Dual use. There are two ways in which language models can be used in the context of toxicity and disinformation:",
            "We want to understand the harms of large language models related to toxicity. There are two possible recipients of the harm: Working definition. What is toxicity? As mentioned above, harms are about what happens to people, so it is important to remember that the definition is very context-dependent. To make some progress, we can use the following working definition:Borkan et al, 2017defines toxicity as anything that is \u201crude, disrespectful, or unreasonable that would make someone want to leave a conversation.\u201d Examples: Word lists. How far can one get by simply defining toxicity in terms of presence of certain\u201cbad words\u201d? Aside: The Clossal, Cleaned Common Crawl (C4) dataset was filtered using this word list and used to train the T5 language model. We will talk about the complexities of data later in the course. Using a word list is inadequate because: Jigaw, a unit within Google focused on technological solutions to social problems (e.g., extremism), developed a popular (proprietary) service for performing toxicity classification called the Perspective API in 2017. You cantry it out here. Anecdotally, it works for some things: However, it doesn\u2019t always work: In general, the Perspective API suffers from a few related problems: While the Perspective API is a popular starting point that is used by the ML and NLP community, it is important to take it with a moderate grain of salt. Gehman et al, 2020introduced a dataset to evaluate the toxicity of generation from a language model. For example (demo;warning: contains offensive content): Caveats. Unprompted experiments. Prompting experiments.  Takeaway: possible to generate \u201ctoxic\u201d completions even given \u201cnon-toxic\u201d prompts. Mitigating toxicity. InterventionNo promptsNon-toxic promptsToxic promptsDo nothing44%51%75%Data-based (DAPT)30%37%57%Decoding-based (PPLM)28%32%52% But reducing toxicity isn\u2019t the only thing that matters (otherwise there are trivial solutions).",
            "Terminology (further discussion): Note that misinformation and disinformationneed not be falsifiable; sometimes it incites or shifts burden of proof to the audience. Things that are not true, but don\u2019t count as misinformation or disinformation: Disinformation can is created on behalf of a malicious actor and disseminated, often on social media platforms (Facebook, Twitter). Examples of disinformation: The state of disinformation campaigns: The economics: Some relevant work:",
            "We\u2019ve talked about language models generating toxic content, but if they can generate it, they might also be used to detect it and other harmful content. Facebook (or Meta) has been fighting toxicity for a long time and recently been leveraging language models to automatically detect it. For example,RoBERTahas been used for a few years. TheFew-Shot Learneris Meta\u2019s latest powerful model for content moderation.  Some anecdotal examples of subtle utterances that are classifed correctly as harmful content:",
            "Performance disparities: Content moderation: Toxicity: Disinformation:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/harms-2/#realtoxicityprompts": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/harms-2/#realtoxicityprompts": ""
        },
        "text_between_h2": [
            "In this lecture, we will discuss two more behavioral harms: Before we dive in, we should point out a disconnect: Before we get to large language models, it is helpful to ground out toxicity and disinformation in the very critical problem of content moderation. Context-dependence. What constitutes harmful content is verycontext-dependent.Chandrasekhran et al. 2018performed a detailed study on Reddit: While there are norms common to almost all subreddits, many norms are specific to subreddits, for example: Dual use. There are two ways in which language models can be used in the context of toxicity and disinformation:",
            "We want to understand the harms of large language models related to toxicity. There are two possible recipients of the harm: Working definition. What is toxicity? As mentioned above, harms are about what happens to people, so it is important to remember that the definition is very context-dependent. To make some progress, we can use the following working definition:Borkan et al, 2017defines toxicity as anything that is \u201crude, disrespectful, or unreasonable that would make someone want to leave a conversation.\u201d Examples: Word lists. How far can one get by simply defining toxicity in terms of presence of certain\u201cbad words\u201d? Aside: The Clossal, Cleaned Common Crawl (C4) dataset was filtered using this word list and used to train the T5 language model. We will talk about the complexities of data later in the course. Using a word list is inadequate because: Jigaw, a unit within Google focused on technological solutions to social problems (e.g., extremism), developed a popular (proprietary) service for performing toxicity classification called the Perspective API in 2017. You cantry it out here. Anecdotally, it works for some things: However, it doesn\u2019t always work: In general, the Perspective API suffers from a few related problems: While the Perspective API is a popular starting point that is used by the ML and NLP community, it is important to take it with a moderate grain of salt. Gehman et al, 2020introduced a dataset to evaluate the toxicity of generation from a language model. For example (demo;warning: contains offensive content): Caveats. Unprompted experiments. Prompting experiments.  Takeaway: possible to generate \u201ctoxic\u201d completions even given \u201cnon-toxic\u201d prompts. Mitigating toxicity. InterventionNo promptsNon-toxic promptsToxic promptsDo nothing44%51%75%Data-based (DAPT)30%37%57%Decoding-based (PPLM)28%32%52% But reducing toxicity isn\u2019t the only thing that matters (otherwise there are trivial solutions).",
            "Terminology (further discussion): Note that misinformation and disinformationneed not be falsifiable; sometimes it incites or shifts burden of proof to the audience. Things that are not true, but don\u2019t count as misinformation or disinformation: Disinformation can is created on behalf of a malicious actor and disseminated, often on social media platforms (Facebook, Twitter). Examples of disinformation: The state of disinformation campaigns: The economics: Some relevant work:",
            "We\u2019ve talked about language models generating toxic content, but if they can generate it, they might also be used to detect it and other harmful content. Facebook (or Meta) has been fighting toxicity for a long time and recently been leveraging language models to automatically detect it. For example,RoBERTahas been used for a few years. TheFew-Shot Learneris Meta\u2019s latest powerful model for content moderation.  Some anecdotal examples of subtle utterances that are classifed correctly as harmful content:",
            "Performance disparities: Content moderation: Toxicity: Disinformation:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/harms-2/#summary": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/harms-2/#summary": ""
        },
        "text_between_h2": [
            "In this lecture, we will discuss two more behavioral harms: Before we dive in, we should point out a disconnect: Before we get to large language models, it is helpful to ground out toxicity and disinformation in the very critical problem of content moderation. Context-dependence. What constitutes harmful content is verycontext-dependent.Chandrasekhran et al. 2018performed a detailed study on Reddit: While there are norms common to almost all subreddits, many norms are specific to subreddits, for example: Dual use. There are two ways in which language models can be used in the context of toxicity and disinformation:",
            "We want to understand the harms of large language models related to toxicity. There are two possible recipients of the harm: Working definition. What is toxicity? As mentioned above, harms are about what happens to people, so it is important to remember that the definition is very context-dependent. To make some progress, we can use the following working definition:Borkan et al, 2017defines toxicity as anything that is \u201crude, disrespectful, or unreasonable that would make someone want to leave a conversation.\u201d Examples: Word lists. How far can one get by simply defining toxicity in terms of presence of certain\u201cbad words\u201d? Aside: The Clossal, Cleaned Common Crawl (C4) dataset was filtered using this word list and used to train the T5 language model. We will talk about the complexities of data later in the course. Using a word list is inadequate because: Jigaw, a unit within Google focused on technological solutions to social problems (e.g., extremism), developed a popular (proprietary) service for performing toxicity classification called the Perspective API in 2017. You cantry it out here. Anecdotally, it works for some things: However, it doesn\u2019t always work: In general, the Perspective API suffers from a few related problems: While the Perspective API is a popular starting point that is used by the ML and NLP community, it is important to take it with a moderate grain of salt. Gehman et al, 2020introduced a dataset to evaluate the toxicity of generation from a language model. For example (demo;warning: contains offensive content): Caveats. Unprompted experiments. Prompting experiments.  Takeaway: possible to generate \u201ctoxic\u201d completions even given \u201cnon-toxic\u201d prompts. Mitigating toxicity. InterventionNo promptsNon-toxic promptsToxic promptsDo nothing44%51%75%Data-based (DAPT)30%37%57%Decoding-based (PPLM)28%32%52% But reducing toxicity isn\u2019t the only thing that matters (otherwise there are trivial solutions).",
            "Terminology (further discussion): Note that misinformation and disinformationneed not be falsifiable; sometimes it incites or shifts burden of proof to the audience. Things that are not true, but don\u2019t count as misinformation or disinformation: Disinformation can is created on behalf of a malicious actor and disseminated, often on social media platforms (Facebook, Twitter). Examples of disinformation: The state of disinformation campaigns: The economics: Some relevant work:",
            "We\u2019ve talked about language models generating toxic content, but if they can generate it, they might also be used to detect it and other harmful content. Facebook (or Meta) has been fighting toxicity for a long time and recently been leveraging language models to automatically detect it. For example,RoBERTahas been used for a few years. TheFew-Shot Learneris Meta\u2019s latest powerful model for content moderation.  Some anecdotal examples of subtle utterances that are classifed correctly as harmful content:",
            "Performance disparities: Content moderation: Toxicity: Disinformation:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/harms-2/#disinformation": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/harms-2/#disinformation": ""
        },
        "text_between_h2": [
            "In this lecture, we will discuss two more behavioral harms: Before we dive in, we should point out a disconnect: Before we get to large language models, it is helpful to ground out toxicity and disinformation in the very critical problem of content moderation. Context-dependence. What constitutes harmful content is verycontext-dependent.Chandrasekhran et al. 2018performed a detailed study on Reddit: While there are norms common to almost all subreddits, many norms are specific to subreddits, for example: Dual use. There are two ways in which language models can be used in the context of toxicity and disinformation:",
            "We want to understand the harms of large language models related to toxicity. There are two possible recipients of the harm: Working definition. What is toxicity? As mentioned above, harms are about what happens to people, so it is important to remember that the definition is very context-dependent. To make some progress, we can use the following working definition:Borkan et al, 2017defines toxicity as anything that is \u201crude, disrespectful, or unreasonable that would make someone want to leave a conversation.\u201d Examples: Word lists. How far can one get by simply defining toxicity in terms of presence of certain\u201cbad words\u201d? Aside: The Clossal, Cleaned Common Crawl (C4) dataset was filtered using this word list and used to train the T5 language model. We will talk about the complexities of data later in the course. Using a word list is inadequate because: Jigaw, a unit within Google focused on technological solutions to social problems (e.g., extremism), developed a popular (proprietary) service for performing toxicity classification called the Perspective API in 2017. You cantry it out here. Anecdotally, it works for some things: However, it doesn\u2019t always work: In general, the Perspective API suffers from a few related problems: While the Perspective API is a popular starting point that is used by the ML and NLP community, it is important to take it with a moderate grain of salt. Gehman et al, 2020introduced a dataset to evaluate the toxicity of generation from a language model. For example (demo;warning: contains offensive content): Caveats. Unprompted experiments. Prompting experiments.  Takeaway: possible to generate \u201ctoxic\u201d completions even given \u201cnon-toxic\u201d prompts. Mitigating toxicity. InterventionNo promptsNon-toxic promptsToxic promptsDo nothing44%51%75%Data-based (DAPT)30%37%57%Decoding-based (PPLM)28%32%52% But reducing toxicity isn\u2019t the only thing that matters (otherwise there are trivial solutions).",
            "Terminology (further discussion): Note that misinformation and disinformationneed not be falsifiable; sometimes it incites or shifts burden of proof to the audience. Things that are not true, but don\u2019t count as misinformation or disinformation: Disinformation can is created on behalf of a malicious actor and disseminated, often on social media platforms (Facebook, Twitter). Examples of disinformation: The state of disinformation campaigns: The economics: Some relevant work:",
            "We\u2019ve talked about language models generating toxic content, but if they can generate it, they might also be used to detect it and other harmful content. Facebook (or Meta) has been fighting toxicity for a long time and recently been leveraging language models to automatically detect it. For example,RoBERTahas been used for a few years. TheFew-Shot Learneris Meta\u2019s latest powerful model for content moderation.  Some anecdotal examples of subtle utterances that are classifed correctly as harmful content:",
            "Performance disparities: Content moderation: Toxicity: Disinformation:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/harms-2/#further-reading": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/harms-2/#further-reading": ""
        },
        "text_between_h2": [
            "In this lecture, we will discuss two more behavioral harms: Before we dive in, we should point out a disconnect: Before we get to large language models, it is helpful to ground out toxicity and disinformation in the very critical problem of content moderation. Context-dependence. What constitutes harmful content is verycontext-dependent.Chandrasekhran et al. 2018performed a detailed study on Reddit: While there are norms common to almost all subreddits, many norms are specific to subreddits, for example: Dual use. There are two ways in which language models can be used in the context of toxicity and disinformation:",
            "We want to understand the harms of large language models related to toxicity. There are two possible recipients of the harm: Working definition. What is toxicity? As mentioned above, harms are about what happens to people, so it is important to remember that the definition is very context-dependent. To make some progress, we can use the following working definition:Borkan et al, 2017defines toxicity as anything that is \u201crude, disrespectful, or unreasonable that would make someone want to leave a conversation.\u201d Examples: Word lists. How far can one get by simply defining toxicity in terms of presence of certain\u201cbad words\u201d? Aside: The Clossal, Cleaned Common Crawl (C4) dataset was filtered using this word list and used to train the T5 language model. We will talk about the complexities of data later in the course. Using a word list is inadequate because: Jigaw, a unit within Google focused on technological solutions to social problems (e.g., extremism), developed a popular (proprietary) service for performing toxicity classification called the Perspective API in 2017. You cantry it out here. Anecdotally, it works for some things: However, it doesn\u2019t always work: In general, the Perspective API suffers from a few related problems: While the Perspective API is a popular starting point that is used by the ML and NLP community, it is important to take it with a moderate grain of salt. Gehman et al, 2020introduced a dataset to evaluate the toxicity of generation from a language model. For example (demo;warning: contains offensive content): Caveats. Unprompted experiments. Prompting experiments.  Takeaway: possible to generate \u201ctoxic\u201d completions even given \u201cnon-toxic\u201d prompts. Mitigating toxicity. InterventionNo promptsNon-toxic promptsToxic promptsDo nothing44%51%75%Data-based (DAPT)30%37%57%Decoding-based (PPLM)28%32%52% But reducing toxicity isn\u2019t the only thing that matters (otherwise there are trivial solutions).",
            "Terminology (further discussion): Note that misinformation and disinformationneed not be falsifiable; sometimes it incites or shifts burden of proof to the audience. Things that are not true, but don\u2019t count as misinformation or disinformation: Disinformation can is created on behalf of a malicious actor and disseminated, often on social media platforms (Facebook, Twitter). Examples of disinformation: The state of disinformation campaigns: The economics: Some relevant work:",
            "We\u2019ve talked about language models generating toxic content, but if they can generate it, they might also be used to detect it and other harmful content. Facebook (or Meta) has been fighting toxicity for a long time and recently been leveraging language models to automatically detect it. For example,RoBERTahas been used for a few years. TheFew-Shot Learneris Meta\u2019s latest powerful model for content moderation.  Some anecdotal examples of subtle utterances that are classifed correctly as harmful content:",
            "Performance disparities: Content moderation: Toxicity: Disinformation:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/data/": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/data/": "Data",
            "https://stanford-cs324.github.io/winter2022/lectures/data/#data-behind-large-language-models": "",
            "https://stanford-cs324.github.io/winter2022/lectures/data/#documentation-of-datasets": "Documentation of datasets",
            "https://stanford-cs324.github.io/winter2022/lectures/data/#data-ecosystems": "",
            "https://stanford-cs324.github.io/winter2022/lectures/data/#webtext-and-openwebtext": "",
            "https://stanford-cs324.github.io/winter2022/lectures/data/#colossal-clean-crawled-corpus": "",
            "https://stanford-cs324.github.io/winter2022/lectures/data/#gpt-3-dataset": "",
            "https://stanford-cs324.github.io/winter2022/lectures/data/#the-pile": "",
            "https://stanford-cs324.github.io/winter2022/lectures/data/#summary": "",
            "https://stanford-cs324.github.io/winter2022/lectures/data/#documentation-for-datasets": "",
            "https://stanford-cs324.github.io/winter2022/lectures/data/#further-reading": ""
        },
        "text_between_h2": [
            "Recall that large language models are trained on \u201craw text\u201d. To be highly capable (e.g., have linguistic and world knowledge), this text should span abroadrange of domains, genres, languages, etc. A natural place (but not the only place) to look for such text is theweb, so this will be a major focus of our attention. The web is absolutely huge. As a lower bound, the Google search index is 100 petabytes (reference). The actual web is likely even larger, and theDeep Webis even larger than that. It is worth noting thatprivate datasetsthat reside in big companies are even larger than what\u2019s available publicly. For example,WalMartgenerates 2.5 petabytes of data each hour! Common Crawlis a nonprofit organization that crawls the web and provides snapshots that are free to the public. Because of its convenience, it has been a standard source of data to train many models such as T5, GPT-3, and Gopher. The April 2021 snapshot ofCommon Crawlhas 320 terabytes of data, which is a few orders of magnitude smaller than the Google index. Representation. Despite the richness of web data, it has been noted inBender et al, 2021that: Takeaway: it is crucial to understand and document the composition of the datasets used to train large language models. WebText. The WebText dataset was used to train GPT-2. OpenWebText. WebText was not released by OpenAI, but it was replicated (in spirit) by theOpenWebTextdataset. Toxicity analysis.Gehman et al. 2020, the RealToxicityPrompts paper, analyzed these two datasets and found: The Colossal Clean Crawled Corpus (C4) is a larger was created to train the T5 model. Analysis.Dodge et al. 2021performed a thorough analysis of the C4 dataset. Documentation levels: Note:Raffel et al. 2020only provided scripts to recreate; cost thousands of dollars just to run these scripts.  Benchmark data contamination. Example from theXSumsummarization dataset: There are two types of contamination: Note that contamination is not due to hosting datasets (as they are usually stored in a JSON file, not as a webpage). The dataset could also be responsible for various harms:   Data composition.  Compare:  Takeaway: The Pile contains a lot of information that\u2019s not well covered by GPT-3\u2019s dataset. They also performed analysis of pejorative content, gender/religion biases. The findings are qualitatively similar to previous work.",
            "We now step back from the specifics of language modeling datasets and discuss general principles around data. Two purposes: Dataset lifecycle(a sample of the questions from each category are provided below): Data statements. Thedata statementswork is specialized to NLP datasets, and covers other aspects: As an example, let\u2019s look at thedatasheet for The Pile.",
            "So far, we have mostly focused on the analysis of existing datasets (for large language models) as well as documentation. But data is a broad concept which can be studied from many other angles. Data management: In machine learning research, we tend to think of datasets as fixed objects that you collect and you feed into a training algorithm. In the the databases community, there is whole subfield thinking about the ecosystem in which data comes to be and is used, and this is particularly relevant inindustry.  Data dignity. This is a concept that originated out of Microsoft and RadicalxChange that seeks to think about the nature of data.",
            "Documentation for datasets: Datasets: Analysis of datasets: Filtering datasets: Data ecosystems:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/data/#data-behind-large-language-models": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/data/#data-behind-large-language-models": ""
        },
        "text_between_h2": [
            "Recall that large language models are trained on \u201craw text\u201d. To be highly capable (e.g., have linguistic and world knowledge), this text should span abroadrange of domains, genres, languages, etc. A natural place (but not the only place) to look for such text is theweb, so this will be a major focus of our attention. The web is absolutely huge. As a lower bound, the Google search index is 100 petabytes (reference). The actual web is likely even larger, and theDeep Webis even larger than that. It is worth noting thatprivate datasetsthat reside in big companies are even larger than what\u2019s available publicly. For example,WalMartgenerates 2.5 petabytes of data each hour! Common Crawlis a nonprofit organization that crawls the web and provides snapshots that are free to the public. Because of its convenience, it has been a standard source of data to train many models such as T5, GPT-3, and Gopher. The April 2021 snapshot ofCommon Crawlhas 320 terabytes of data, which is a few orders of magnitude smaller than the Google index. Representation. Despite the richness of web data, it has been noted inBender et al, 2021that: Takeaway: it is crucial to understand and document the composition of the datasets used to train large language models. WebText. The WebText dataset was used to train GPT-2. OpenWebText. WebText was not released by OpenAI, but it was replicated (in spirit) by theOpenWebTextdataset. Toxicity analysis.Gehman et al. 2020, the RealToxicityPrompts paper, analyzed these two datasets and found: The Colossal Clean Crawled Corpus (C4) is a larger was created to train the T5 model. Analysis.Dodge et al. 2021performed a thorough analysis of the C4 dataset. Documentation levels: Note:Raffel et al. 2020only provided scripts to recreate; cost thousands of dollars just to run these scripts.  Benchmark data contamination. Example from theXSumsummarization dataset: There are two types of contamination: Note that contamination is not due to hosting datasets (as they are usually stored in a JSON file, not as a webpage). The dataset could also be responsible for various harms:   Data composition.  Compare:  Takeaway: The Pile contains a lot of information that\u2019s not well covered by GPT-3\u2019s dataset. They also performed analysis of pejorative content, gender/religion biases. The findings are qualitatively similar to previous work.",
            "We now step back from the specifics of language modeling datasets and discuss general principles around data. Two purposes: Dataset lifecycle(a sample of the questions from each category are provided below): Data statements. Thedata statementswork is specialized to NLP datasets, and covers other aspects: As an example, let\u2019s look at thedatasheet for The Pile.",
            "So far, we have mostly focused on the analysis of existing datasets (for large language models) as well as documentation. But data is a broad concept which can be studied from many other angles. Data management: In machine learning research, we tend to think of datasets as fixed objects that you collect and you feed into a training algorithm. In the the databases community, there is whole subfield thinking about the ecosystem in which data comes to be and is used, and this is particularly relevant inindustry.  Data dignity. This is a concept that originated out of Microsoft and RadicalxChange that seeks to think about the nature of data.",
            "Documentation for datasets: Datasets: Analysis of datasets: Filtering datasets: Data ecosystems:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/data/#documentation-of-datasets": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/data/#documentation-of-datasets": "Documentation of datasets"
        },
        "text_between_h2": [
            "Recall that large language models are trained on \u201craw text\u201d. To be highly capable (e.g., have linguistic and world knowledge), this text should span abroadrange of domains, genres, languages, etc. A natural place (but not the only place) to look for such text is theweb, so this will be a major focus of our attention. The web is absolutely huge. As a lower bound, the Google search index is 100 petabytes (reference). The actual web is likely even larger, and theDeep Webis even larger than that. It is worth noting thatprivate datasetsthat reside in big companies are even larger than what\u2019s available publicly. For example,WalMartgenerates 2.5 petabytes of data each hour! Common Crawlis a nonprofit organization that crawls the web and provides snapshots that are free to the public. Because of its convenience, it has been a standard source of data to train many models such as T5, GPT-3, and Gopher. The April 2021 snapshot ofCommon Crawlhas 320 terabytes of data, which is a few orders of magnitude smaller than the Google index. Representation. Despite the richness of web data, it has been noted inBender et al, 2021that: Takeaway: it is crucial to understand and document the composition of the datasets used to train large language models. WebText. The WebText dataset was used to train GPT-2. OpenWebText. WebText was not released by OpenAI, but it was replicated (in spirit) by theOpenWebTextdataset. Toxicity analysis.Gehman et al. 2020, the RealToxicityPrompts paper, analyzed these two datasets and found: The Colossal Clean Crawled Corpus (C4) is a larger was created to train the T5 model. Analysis.Dodge et al. 2021performed a thorough analysis of the C4 dataset. Documentation levels: Note:Raffel et al. 2020only provided scripts to recreate; cost thousands of dollars just to run these scripts.  Benchmark data contamination. Example from theXSumsummarization dataset: There are two types of contamination: Note that contamination is not due to hosting datasets (as they are usually stored in a JSON file, not as a webpage). The dataset could also be responsible for various harms:   Data composition.  Compare:  Takeaway: The Pile contains a lot of information that\u2019s not well covered by GPT-3\u2019s dataset. They also performed analysis of pejorative content, gender/religion biases. The findings are qualitatively similar to previous work.",
            "We now step back from the specifics of language modeling datasets and discuss general principles around data. Two purposes: Dataset lifecycle(a sample of the questions from each category are provided below): Data statements. Thedata statementswork is specialized to NLP datasets, and covers other aspects: As an example, let\u2019s look at thedatasheet for The Pile.",
            "So far, we have mostly focused on the analysis of existing datasets (for large language models) as well as documentation. But data is a broad concept which can be studied from many other angles. Data management: In machine learning research, we tend to think of datasets as fixed objects that you collect and you feed into a training algorithm. In the the databases community, there is whole subfield thinking about the ecosystem in which data comes to be and is used, and this is particularly relevant inindustry.  Data dignity. This is a concept that originated out of Microsoft and RadicalxChange that seeks to think about the nature of data.",
            "Documentation for datasets: Datasets: Analysis of datasets: Filtering datasets: Data ecosystems:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/data/#data-ecosystems": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/data/#data-ecosystems": ""
        },
        "text_between_h2": [
            "Recall that large language models are trained on \u201craw text\u201d. To be highly capable (e.g., have linguistic and world knowledge), this text should span abroadrange of domains, genres, languages, etc. A natural place (but not the only place) to look for such text is theweb, so this will be a major focus of our attention. The web is absolutely huge. As a lower bound, the Google search index is 100 petabytes (reference). The actual web is likely even larger, and theDeep Webis even larger than that. It is worth noting thatprivate datasetsthat reside in big companies are even larger than what\u2019s available publicly. For example,WalMartgenerates 2.5 petabytes of data each hour! Common Crawlis a nonprofit organization that crawls the web and provides snapshots that are free to the public. Because of its convenience, it has been a standard source of data to train many models such as T5, GPT-3, and Gopher. The April 2021 snapshot ofCommon Crawlhas 320 terabytes of data, which is a few orders of magnitude smaller than the Google index. Representation. Despite the richness of web data, it has been noted inBender et al, 2021that: Takeaway: it is crucial to understand and document the composition of the datasets used to train large language models. WebText. The WebText dataset was used to train GPT-2. OpenWebText. WebText was not released by OpenAI, but it was replicated (in spirit) by theOpenWebTextdataset. Toxicity analysis.Gehman et al. 2020, the RealToxicityPrompts paper, analyzed these two datasets and found: The Colossal Clean Crawled Corpus (C4) is a larger was created to train the T5 model. Analysis.Dodge et al. 2021performed a thorough analysis of the C4 dataset. Documentation levels: Note:Raffel et al. 2020only provided scripts to recreate; cost thousands of dollars just to run these scripts.  Benchmark data contamination. Example from theXSumsummarization dataset: There are two types of contamination: Note that contamination is not due to hosting datasets (as they are usually stored in a JSON file, not as a webpage). The dataset could also be responsible for various harms:   Data composition.  Compare:  Takeaway: The Pile contains a lot of information that\u2019s not well covered by GPT-3\u2019s dataset. They also performed analysis of pejorative content, gender/religion biases. The findings are qualitatively similar to previous work.",
            "We now step back from the specifics of language modeling datasets and discuss general principles around data. Two purposes: Dataset lifecycle(a sample of the questions from each category are provided below): Data statements. Thedata statementswork is specialized to NLP datasets, and covers other aspects: As an example, let\u2019s look at thedatasheet for The Pile.",
            "So far, we have mostly focused on the analysis of existing datasets (for large language models) as well as documentation. But data is a broad concept which can be studied from many other angles. Data management: In machine learning research, we tend to think of datasets as fixed objects that you collect and you feed into a training algorithm. In the the databases community, there is whole subfield thinking about the ecosystem in which data comes to be and is used, and this is particularly relevant inindustry.  Data dignity. This is a concept that originated out of Microsoft and RadicalxChange that seeks to think about the nature of data.",
            "Documentation for datasets: Datasets: Analysis of datasets: Filtering datasets: Data ecosystems:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/data/#webtext-and-openwebtext": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/data/#webtext-and-openwebtext": ""
        },
        "text_between_h2": [
            "Recall that large language models are trained on \u201craw text\u201d. To be highly capable (e.g., have linguistic and world knowledge), this text should span abroadrange of domains, genres, languages, etc. A natural place (but not the only place) to look for such text is theweb, so this will be a major focus of our attention. The web is absolutely huge. As a lower bound, the Google search index is 100 petabytes (reference). The actual web is likely even larger, and theDeep Webis even larger than that. It is worth noting thatprivate datasetsthat reside in big companies are even larger than what\u2019s available publicly. For example,WalMartgenerates 2.5 petabytes of data each hour! Common Crawlis a nonprofit organization that crawls the web and provides snapshots that are free to the public. Because of its convenience, it has been a standard source of data to train many models such as T5, GPT-3, and Gopher. The April 2021 snapshot ofCommon Crawlhas 320 terabytes of data, which is a few orders of magnitude smaller than the Google index. Representation. Despite the richness of web data, it has been noted inBender et al, 2021that: Takeaway: it is crucial to understand and document the composition of the datasets used to train large language models. WebText. The WebText dataset was used to train GPT-2. OpenWebText. WebText was not released by OpenAI, but it was replicated (in spirit) by theOpenWebTextdataset. Toxicity analysis.Gehman et al. 2020, the RealToxicityPrompts paper, analyzed these two datasets and found: The Colossal Clean Crawled Corpus (C4) is a larger was created to train the T5 model. Analysis.Dodge et al. 2021performed a thorough analysis of the C4 dataset. Documentation levels: Note:Raffel et al. 2020only provided scripts to recreate; cost thousands of dollars just to run these scripts.  Benchmark data contamination. Example from theXSumsummarization dataset: There are two types of contamination: Note that contamination is not due to hosting datasets (as they are usually stored in a JSON file, not as a webpage). The dataset could also be responsible for various harms:   Data composition.  Compare:  Takeaway: The Pile contains a lot of information that\u2019s not well covered by GPT-3\u2019s dataset. They also performed analysis of pejorative content, gender/religion biases. The findings are qualitatively similar to previous work.",
            "We now step back from the specifics of language modeling datasets and discuss general principles around data. Two purposes: Dataset lifecycle(a sample of the questions from each category are provided below): Data statements. Thedata statementswork is specialized to NLP datasets, and covers other aspects: As an example, let\u2019s look at thedatasheet for The Pile.",
            "So far, we have mostly focused on the analysis of existing datasets (for large language models) as well as documentation. But data is a broad concept which can be studied from many other angles. Data management: In machine learning research, we tend to think of datasets as fixed objects that you collect and you feed into a training algorithm. In the the databases community, there is whole subfield thinking about the ecosystem in which data comes to be and is used, and this is particularly relevant inindustry.  Data dignity. This is a concept that originated out of Microsoft and RadicalxChange that seeks to think about the nature of data.",
            "Documentation for datasets: Datasets: Analysis of datasets: Filtering datasets: Data ecosystems:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/data/#colossal-clean-crawled-corpus": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/data/#colossal-clean-crawled-corpus": ""
        },
        "text_between_h2": [
            "Recall that large language models are trained on \u201craw text\u201d. To be highly capable (e.g., have linguistic and world knowledge), this text should span abroadrange of domains, genres, languages, etc. A natural place (but not the only place) to look for such text is theweb, so this will be a major focus of our attention. The web is absolutely huge. As a lower bound, the Google search index is 100 petabytes (reference). The actual web is likely even larger, and theDeep Webis even larger than that. It is worth noting thatprivate datasetsthat reside in big companies are even larger than what\u2019s available publicly. For example,WalMartgenerates 2.5 petabytes of data each hour! Common Crawlis a nonprofit organization that crawls the web and provides snapshots that are free to the public. Because of its convenience, it has been a standard source of data to train many models such as T5, GPT-3, and Gopher. The April 2021 snapshot ofCommon Crawlhas 320 terabytes of data, which is a few orders of magnitude smaller than the Google index. Representation. Despite the richness of web data, it has been noted inBender et al, 2021that: Takeaway: it is crucial to understand and document the composition of the datasets used to train large language models. WebText. The WebText dataset was used to train GPT-2. OpenWebText. WebText was not released by OpenAI, but it was replicated (in spirit) by theOpenWebTextdataset. Toxicity analysis.Gehman et al. 2020, the RealToxicityPrompts paper, analyzed these two datasets and found: The Colossal Clean Crawled Corpus (C4) is a larger was created to train the T5 model. Analysis.Dodge et al. 2021performed a thorough analysis of the C4 dataset. Documentation levels: Note:Raffel et al. 2020only provided scripts to recreate; cost thousands of dollars just to run these scripts.  Benchmark data contamination. Example from theXSumsummarization dataset: There are two types of contamination: Note that contamination is not due to hosting datasets (as they are usually stored in a JSON file, not as a webpage). The dataset could also be responsible for various harms:   Data composition.  Compare:  Takeaway: The Pile contains a lot of information that\u2019s not well covered by GPT-3\u2019s dataset. They also performed analysis of pejorative content, gender/religion biases. The findings are qualitatively similar to previous work.",
            "We now step back from the specifics of language modeling datasets and discuss general principles around data. Two purposes: Dataset lifecycle(a sample of the questions from each category are provided below): Data statements. Thedata statementswork is specialized to NLP datasets, and covers other aspects: As an example, let\u2019s look at thedatasheet for The Pile.",
            "So far, we have mostly focused on the analysis of existing datasets (for large language models) as well as documentation. But data is a broad concept which can be studied from many other angles. Data management: In machine learning research, we tend to think of datasets as fixed objects that you collect and you feed into a training algorithm. In the the databases community, there is whole subfield thinking about the ecosystem in which data comes to be and is used, and this is particularly relevant inindustry.  Data dignity. This is a concept that originated out of Microsoft and RadicalxChange that seeks to think about the nature of data.",
            "Documentation for datasets: Datasets: Analysis of datasets: Filtering datasets: Data ecosystems:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/data/#gpt-3-dataset": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/data/#gpt-3-dataset": ""
        },
        "text_between_h2": [
            "Recall that large language models are trained on \u201craw text\u201d. To be highly capable (e.g., have linguistic and world knowledge), this text should span abroadrange of domains, genres, languages, etc. A natural place (but not the only place) to look for such text is theweb, so this will be a major focus of our attention. The web is absolutely huge. As a lower bound, the Google search index is 100 petabytes (reference). The actual web is likely even larger, and theDeep Webis even larger than that. It is worth noting thatprivate datasetsthat reside in big companies are even larger than what\u2019s available publicly. For example,WalMartgenerates 2.5 petabytes of data each hour! Common Crawlis a nonprofit organization that crawls the web and provides snapshots that are free to the public. Because of its convenience, it has been a standard source of data to train many models such as T5, GPT-3, and Gopher. The April 2021 snapshot ofCommon Crawlhas 320 terabytes of data, which is a few orders of magnitude smaller than the Google index. Representation. Despite the richness of web data, it has been noted inBender et al, 2021that: Takeaway: it is crucial to understand and document the composition of the datasets used to train large language models. WebText. The WebText dataset was used to train GPT-2. OpenWebText. WebText was not released by OpenAI, but it was replicated (in spirit) by theOpenWebTextdataset. Toxicity analysis.Gehman et al. 2020, the RealToxicityPrompts paper, analyzed these two datasets and found: The Colossal Clean Crawled Corpus (C4) is a larger was created to train the T5 model. Analysis.Dodge et al. 2021performed a thorough analysis of the C4 dataset. Documentation levels: Note:Raffel et al. 2020only provided scripts to recreate; cost thousands of dollars just to run these scripts.  Benchmark data contamination. Example from theXSumsummarization dataset: There are two types of contamination: Note that contamination is not due to hosting datasets (as they are usually stored in a JSON file, not as a webpage). The dataset could also be responsible for various harms:   Data composition.  Compare:  Takeaway: The Pile contains a lot of information that\u2019s not well covered by GPT-3\u2019s dataset. They also performed analysis of pejorative content, gender/religion biases. The findings are qualitatively similar to previous work.",
            "We now step back from the specifics of language modeling datasets and discuss general principles around data. Two purposes: Dataset lifecycle(a sample of the questions from each category are provided below): Data statements. Thedata statementswork is specialized to NLP datasets, and covers other aspects: As an example, let\u2019s look at thedatasheet for The Pile.",
            "So far, we have mostly focused on the analysis of existing datasets (for large language models) as well as documentation. But data is a broad concept which can be studied from many other angles. Data management: In machine learning research, we tend to think of datasets as fixed objects that you collect and you feed into a training algorithm. In the the databases community, there is whole subfield thinking about the ecosystem in which data comes to be and is used, and this is particularly relevant inindustry.  Data dignity. This is a concept that originated out of Microsoft and RadicalxChange that seeks to think about the nature of data.",
            "Documentation for datasets: Datasets: Analysis of datasets: Filtering datasets: Data ecosystems:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/data/#the-pile": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/data/#the-pile": ""
        },
        "text_between_h2": [
            "Recall that large language models are trained on \u201craw text\u201d. To be highly capable (e.g., have linguistic and world knowledge), this text should span abroadrange of domains, genres, languages, etc. A natural place (but not the only place) to look for such text is theweb, so this will be a major focus of our attention. The web is absolutely huge. As a lower bound, the Google search index is 100 petabytes (reference). The actual web is likely even larger, and theDeep Webis even larger than that. It is worth noting thatprivate datasetsthat reside in big companies are even larger than what\u2019s available publicly. For example,WalMartgenerates 2.5 petabytes of data each hour! Common Crawlis a nonprofit organization that crawls the web and provides snapshots that are free to the public. Because of its convenience, it has been a standard source of data to train many models such as T5, GPT-3, and Gopher. The April 2021 snapshot ofCommon Crawlhas 320 terabytes of data, which is a few orders of magnitude smaller than the Google index. Representation. Despite the richness of web data, it has been noted inBender et al, 2021that: Takeaway: it is crucial to understand and document the composition of the datasets used to train large language models. WebText. The WebText dataset was used to train GPT-2. OpenWebText. WebText was not released by OpenAI, but it was replicated (in spirit) by theOpenWebTextdataset. Toxicity analysis.Gehman et al. 2020, the RealToxicityPrompts paper, analyzed these two datasets and found: The Colossal Clean Crawled Corpus (C4) is a larger was created to train the T5 model. Analysis.Dodge et al. 2021performed a thorough analysis of the C4 dataset. Documentation levels: Note:Raffel et al. 2020only provided scripts to recreate; cost thousands of dollars just to run these scripts.  Benchmark data contamination. Example from theXSumsummarization dataset: There are two types of contamination: Note that contamination is not due to hosting datasets (as they are usually stored in a JSON file, not as a webpage). The dataset could also be responsible for various harms:   Data composition.  Compare:  Takeaway: The Pile contains a lot of information that\u2019s not well covered by GPT-3\u2019s dataset. They also performed analysis of pejorative content, gender/religion biases. The findings are qualitatively similar to previous work.",
            "We now step back from the specifics of language modeling datasets and discuss general principles around data. Two purposes: Dataset lifecycle(a sample of the questions from each category are provided below): Data statements. Thedata statementswork is specialized to NLP datasets, and covers other aspects: As an example, let\u2019s look at thedatasheet for The Pile.",
            "So far, we have mostly focused on the analysis of existing datasets (for large language models) as well as documentation. But data is a broad concept which can be studied from many other angles. Data management: In machine learning research, we tend to think of datasets as fixed objects that you collect and you feed into a training algorithm. In the the databases community, there is whole subfield thinking about the ecosystem in which data comes to be and is used, and this is particularly relevant inindustry.  Data dignity. This is a concept that originated out of Microsoft and RadicalxChange that seeks to think about the nature of data.",
            "Documentation for datasets: Datasets: Analysis of datasets: Filtering datasets: Data ecosystems:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/data/#summary": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/data/#summary": ""
        },
        "text_between_h2": [
            "Recall that large language models are trained on \u201craw text\u201d. To be highly capable (e.g., have linguistic and world knowledge), this text should span abroadrange of domains, genres, languages, etc. A natural place (but not the only place) to look for such text is theweb, so this will be a major focus of our attention. The web is absolutely huge. As a lower bound, the Google search index is 100 petabytes (reference). The actual web is likely even larger, and theDeep Webis even larger than that. It is worth noting thatprivate datasetsthat reside in big companies are even larger than what\u2019s available publicly. For example,WalMartgenerates 2.5 petabytes of data each hour! Common Crawlis a nonprofit organization that crawls the web and provides snapshots that are free to the public. Because of its convenience, it has been a standard source of data to train many models such as T5, GPT-3, and Gopher. The April 2021 snapshot ofCommon Crawlhas 320 terabytes of data, which is a few orders of magnitude smaller than the Google index. Representation. Despite the richness of web data, it has been noted inBender et al, 2021that: Takeaway: it is crucial to understand and document the composition of the datasets used to train large language models. WebText. The WebText dataset was used to train GPT-2. OpenWebText. WebText was not released by OpenAI, but it was replicated (in spirit) by theOpenWebTextdataset. Toxicity analysis.Gehman et al. 2020, the RealToxicityPrompts paper, analyzed these two datasets and found: The Colossal Clean Crawled Corpus (C4) is a larger was created to train the T5 model. Analysis.Dodge et al. 2021performed a thorough analysis of the C4 dataset. Documentation levels: Note:Raffel et al. 2020only provided scripts to recreate; cost thousands of dollars just to run these scripts.  Benchmark data contamination. Example from theXSumsummarization dataset: There are two types of contamination: Note that contamination is not due to hosting datasets (as they are usually stored in a JSON file, not as a webpage). The dataset could also be responsible for various harms:   Data composition.  Compare:  Takeaway: The Pile contains a lot of information that\u2019s not well covered by GPT-3\u2019s dataset. They also performed analysis of pejorative content, gender/religion biases. The findings are qualitatively similar to previous work.",
            "We now step back from the specifics of language modeling datasets and discuss general principles around data. Two purposes: Dataset lifecycle(a sample of the questions from each category are provided below): Data statements. Thedata statementswork is specialized to NLP datasets, and covers other aspects: As an example, let\u2019s look at thedatasheet for The Pile.",
            "So far, we have mostly focused on the analysis of existing datasets (for large language models) as well as documentation. But data is a broad concept which can be studied from many other angles. Data management: In machine learning research, we tend to think of datasets as fixed objects that you collect and you feed into a training algorithm. In the the databases community, there is whole subfield thinking about the ecosystem in which data comes to be and is used, and this is particularly relevant inindustry.  Data dignity. This is a concept that originated out of Microsoft and RadicalxChange that seeks to think about the nature of data.",
            "Documentation for datasets: Datasets: Analysis of datasets: Filtering datasets: Data ecosystems:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/data/#documentation-for-datasets": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/data/#documentation-for-datasets": ""
        },
        "text_between_h2": [
            "Recall that large language models are trained on \u201craw text\u201d. To be highly capable (e.g., have linguistic and world knowledge), this text should span abroadrange of domains, genres, languages, etc. A natural place (but not the only place) to look for such text is theweb, so this will be a major focus of our attention. The web is absolutely huge. As a lower bound, the Google search index is 100 petabytes (reference). The actual web is likely even larger, and theDeep Webis even larger than that. It is worth noting thatprivate datasetsthat reside in big companies are even larger than what\u2019s available publicly. For example,WalMartgenerates 2.5 petabytes of data each hour! Common Crawlis a nonprofit organization that crawls the web and provides snapshots that are free to the public. Because of its convenience, it has been a standard source of data to train many models such as T5, GPT-3, and Gopher. The April 2021 snapshot ofCommon Crawlhas 320 terabytes of data, which is a few orders of magnitude smaller than the Google index. Representation. Despite the richness of web data, it has been noted inBender et al, 2021that: Takeaway: it is crucial to understand and document the composition of the datasets used to train large language models. WebText. The WebText dataset was used to train GPT-2. OpenWebText. WebText was not released by OpenAI, but it was replicated (in spirit) by theOpenWebTextdataset. Toxicity analysis.Gehman et al. 2020, the RealToxicityPrompts paper, analyzed these two datasets and found: The Colossal Clean Crawled Corpus (C4) is a larger was created to train the T5 model. Analysis.Dodge et al. 2021performed a thorough analysis of the C4 dataset. Documentation levels: Note:Raffel et al. 2020only provided scripts to recreate; cost thousands of dollars just to run these scripts.  Benchmark data contamination. Example from theXSumsummarization dataset: There are two types of contamination: Note that contamination is not due to hosting datasets (as they are usually stored in a JSON file, not as a webpage). The dataset could also be responsible for various harms:   Data composition.  Compare:  Takeaway: The Pile contains a lot of information that\u2019s not well covered by GPT-3\u2019s dataset. They also performed analysis of pejorative content, gender/religion biases. The findings are qualitatively similar to previous work.",
            "We now step back from the specifics of language modeling datasets and discuss general principles around data. Two purposes: Dataset lifecycle(a sample of the questions from each category are provided below): Data statements. Thedata statementswork is specialized to NLP datasets, and covers other aspects: As an example, let\u2019s look at thedatasheet for The Pile.",
            "So far, we have mostly focused on the analysis of existing datasets (for large language models) as well as documentation. But data is a broad concept which can be studied from many other angles. Data management: In machine learning research, we tend to think of datasets as fixed objects that you collect and you feed into a training algorithm. In the the databases community, there is whole subfield thinking about the ecosystem in which data comes to be and is used, and this is particularly relevant inindustry.  Data dignity. This is a concept that originated out of Microsoft and RadicalxChange that seeks to think about the nature of data.",
            "Documentation for datasets: Datasets: Analysis of datasets: Filtering datasets: Data ecosystems:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/data/#further-reading": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/data/#further-reading": ""
        },
        "text_between_h2": [
            "Recall that large language models are trained on \u201craw text\u201d. To be highly capable (e.g., have linguistic and world knowledge), this text should span abroadrange of domains, genres, languages, etc. A natural place (but not the only place) to look for such text is theweb, so this will be a major focus of our attention. The web is absolutely huge. As a lower bound, the Google search index is 100 petabytes (reference). The actual web is likely even larger, and theDeep Webis even larger than that. It is worth noting thatprivate datasetsthat reside in big companies are even larger than what\u2019s available publicly. For example,WalMartgenerates 2.5 petabytes of data each hour! Common Crawlis a nonprofit organization that crawls the web and provides snapshots that are free to the public. Because of its convenience, it has been a standard source of data to train many models such as T5, GPT-3, and Gopher. The April 2021 snapshot ofCommon Crawlhas 320 terabytes of data, which is a few orders of magnitude smaller than the Google index. Representation. Despite the richness of web data, it has been noted inBender et al, 2021that: Takeaway: it is crucial to understand and document the composition of the datasets used to train large language models. WebText. The WebText dataset was used to train GPT-2. OpenWebText. WebText was not released by OpenAI, but it was replicated (in spirit) by theOpenWebTextdataset. Toxicity analysis.Gehman et al. 2020, the RealToxicityPrompts paper, analyzed these two datasets and found: The Colossal Clean Crawled Corpus (C4) is a larger was created to train the T5 model. Analysis.Dodge et al. 2021performed a thorough analysis of the C4 dataset. Documentation levels: Note:Raffel et al. 2020only provided scripts to recreate; cost thousands of dollars just to run these scripts.  Benchmark data contamination. Example from theXSumsummarization dataset: There are two types of contamination: Note that contamination is not due to hosting datasets (as they are usually stored in a JSON file, not as a webpage). The dataset could also be responsible for various harms:   Data composition.  Compare:  Takeaway: The Pile contains a lot of information that\u2019s not well covered by GPT-3\u2019s dataset. They also performed analysis of pejorative content, gender/religion biases. The findings are qualitatively similar to previous work.",
            "We now step back from the specifics of language modeling datasets and discuss general principles around data. Two purposes: Dataset lifecycle(a sample of the questions from each category are provided below): Data statements. Thedata statementswork is specialized to NLP datasets, and covers other aspects: As an example, let\u2019s look at thedatasheet for The Pile.",
            "So far, we have mostly focused on the analysis of existing datasets (for large language models) as well as documentation. But data is a broad concept which can be studied from many other angles. Data management: In machine learning research, we tend to think of datasets as fixed objects that you collect and you feed into a training algorithm. In the the databases community, there is whole subfield thinking about the ecosystem in which data comes to be and is used, and this is particularly relevant inindustry.  Data dignity. This is a concept that originated out of Microsoft and RadicalxChange that seeks to think about the nature of data.",
            "Documentation for datasets: Datasets: Analysis of datasets: Filtering datasets: Data ecosystems:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/security/": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/security/": "Security",
            "https://stanford-cs324.github.io/winter2022/lectures/security/#further-reading": ""
        },
        "text_between_h2": [
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/security/#further-reading": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/security/#further-reading": ""
        },
        "text_between_h2": [
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/legality/": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/legality/": "Legality",
            "https://stanford-cs324.github.io/winter2022/lectures/legality/#copyright-law": "",
            "https://stanford-cs324.github.io/winter2022/lectures/legality/#privacy-law": "",
            "https://stanford-cs324.github.io/winter2022/lectures/legality/#other-laws": "",
            "https://stanford-cs324.github.io/winter2022/lectures/legality/#summary": "",
            "https://stanford-cs324.github.io/winter2022/lectures/legality/#further-reading": ""
        },
        "text_between_h2": [
            "Large language models, or any machine learning model, is trained on data, which results from the fruits of a human being\u2019s labor (e.g., author, programmer, photographer, etc.). What can someone other than the creators can do with these creations (e.g., books, code, photographs, etc.) is in the realm of intellectual property law. Intellectual property law. Copyright law. The key legislation that governs copyright in the United States isCopyright Act of 1976. There are two ways you can use a copyrighted work: Licenses. Fair use (section 107). Terms of service. There is one additional hurdle:terms of service, which might impose additional restrictions. Notes: Next, we will go over a number of cases that have ruled for or against fair use. Authors Guild v. Google Google v. Oracle Fox News v. TVEyes Kelly v. Arriba Sega v. Accolade Fair learningargues that machine learning is fair use: Whether training large language models is fair use is rapidly evolving. Looking back at the history of information technology, we see three phases:",
            "Next we will briefly discuss some examples of privacy laws. Clearview AI California Consumer Privacy Act (2018) California Privacy Rights Act of 2020 GDPR",
            "California\u2019s bot disclosure bill:",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/legality/#copyright-law": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/legality/#copyright-law": ""
        },
        "text_between_h2": [
            "Large language models, or any machine learning model, is trained on data, which results from the fruits of a human being\u2019s labor (e.g., author, programmer, photographer, etc.). What can someone other than the creators can do with these creations (e.g., books, code, photographs, etc.) is in the realm of intellectual property law. Intellectual property law. Copyright law. The key legislation that governs copyright in the United States isCopyright Act of 1976. There are two ways you can use a copyrighted work: Licenses. Fair use (section 107). Terms of service. There is one additional hurdle:terms of service, which might impose additional restrictions. Notes: Next, we will go over a number of cases that have ruled for or against fair use. Authors Guild v. Google Google v. Oracle Fox News v. TVEyes Kelly v. Arriba Sega v. Accolade Fair learningargues that machine learning is fair use: Whether training large language models is fair use is rapidly evolving. Looking back at the history of information technology, we see three phases:",
            "Next we will briefly discuss some examples of privacy laws. Clearview AI California Consumer Privacy Act (2018) California Privacy Rights Act of 2020 GDPR",
            "California\u2019s bot disclosure bill:",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/legality/#privacy-law": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/legality/#privacy-law": ""
        },
        "text_between_h2": [
            "Large language models, or any machine learning model, is trained on data, which results from the fruits of a human being\u2019s labor (e.g., author, programmer, photographer, etc.). What can someone other than the creators can do with these creations (e.g., books, code, photographs, etc.) is in the realm of intellectual property law. Intellectual property law. Copyright law. The key legislation that governs copyright in the United States isCopyright Act of 1976. There are two ways you can use a copyrighted work: Licenses. Fair use (section 107). Terms of service. There is one additional hurdle:terms of service, which might impose additional restrictions. Notes: Next, we will go over a number of cases that have ruled for or against fair use. Authors Guild v. Google Google v. Oracle Fox News v. TVEyes Kelly v. Arriba Sega v. Accolade Fair learningargues that machine learning is fair use: Whether training large language models is fair use is rapidly evolving. Looking back at the history of information technology, we see three phases:",
            "Next we will briefly discuss some examples of privacy laws. Clearview AI California Consumer Privacy Act (2018) California Privacy Rights Act of 2020 GDPR",
            "California\u2019s bot disclosure bill:",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/legality/#other-laws": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/legality/#other-laws": ""
        },
        "text_between_h2": [
            "Large language models, or any machine learning model, is trained on data, which results from the fruits of a human being\u2019s labor (e.g., author, programmer, photographer, etc.). What can someone other than the creators can do with these creations (e.g., books, code, photographs, etc.) is in the realm of intellectual property law. Intellectual property law. Copyright law. The key legislation that governs copyright in the United States isCopyright Act of 1976. There are two ways you can use a copyrighted work: Licenses. Fair use (section 107). Terms of service. There is one additional hurdle:terms of service, which might impose additional restrictions. Notes: Next, we will go over a number of cases that have ruled for or against fair use. Authors Guild v. Google Google v. Oracle Fox News v. TVEyes Kelly v. Arriba Sega v. Accolade Fair learningargues that machine learning is fair use: Whether training large language models is fair use is rapidly evolving. Looking back at the history of information technology, we see three phases:",
            "Next we will briefly discuss some examples of privacy laws. Clearview AI California Consumer Privacy Act (2018) California Privacy Rights Act of 2020 GDPR",
            "California\u2019s bot disclosure bill:",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/legality/#summary": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/legality/#summary": ""
        },
        "text_between_h2": [
            "Large language models, or any machine learning model, is trained on data, which results from the fruits of a human being\u2019s labor (e.g., author, programmer, photographer, etc.). What can someone other than the creators can do with these creations (e.g., books, code, photographs, etc.) is in the realm of intellectual property law. Intellectual property law. Copyright law. The key legislation that governs copyright in the United States isCopyright Act of 1976. There are two ways you can use a copyrighted work: Licenses. Fair use (section 107). Terms of service. There is one additional hurdle:terms of service, which might impose additional restrictions. Notes: Next, we will go over a number of cases that have ruled for or against fair use. Authors Guild v. Google Google v. Oracle Fox News v. TVEyes Kelly v. Arriba Sega v. Accolade Fair learningargues that machine learning is fair use: Whether training large language models is fair use is rapidly evolving. Looking back at the history of information technology, we see three phases:",
            "Next we will briefly discuss some examples of privacy laws. Clearview AI California Consumer Privacy Act (2018) California Privacy Rights Act of 2020 GDPR",
            "California\u2019s bot disclosure bill:",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/legality/#further-reading": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/legality/#further-reading": ""
        },
        "text_between_h2": [
            "Large language models, or any machine learning model, is trained on data, which results from the fruits of a human being\u2019s labor (e.g., author, programmer, photographer, etc.). What can someone other than the creators can do with these creations (e.g., books, code, photographs, etc.) is in the realm of intellectual property law. Intellectual property law. Copyright law. The key legislation that governs copyright in the United States isCopyright Act of 1976. There are two ways you can use a copyrighted work: Licenses. Fair use (section 107). Terms of service. There is one additional hurdle:terms of service, which might impose additional restrictions. Notes: Next, we will go over a number of cases that have ruled for or against fair use. Authors Guild v. Google Google v. Oracle Fox News v. TVEyes Kelly v. Arriba Sega v. Accolade Fair learningargues that machine learning is fair use: Whether training large language models is fair use is rapidly evolving. Looking back at the history of information technology, we see three phases:",
            "Next we will briefly discuss some examples of privacy laws. Clearview AI California Consumer Privacy Act (2018) California Privacy Rights Act of 2020 GDPR",
            "California\u2019s bot disclosure bill:",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/modeling/": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/": "Modeling",
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/#tokenization": "",
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/#model-architecture": "Model architecture",
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/#split-by-spaces": "",
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/#byte-pair-encoding": "",
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/#unigram-model-sentencepiece": "",
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/#comparing-tokenizers": "",
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/#models": "",
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/#types-of-language-models": "",
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/#preliminaries": "",
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/#preliminaries-1": "",
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/#recurrent-neural-networks": "",
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/#transformers": "",
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/#further-reading": ""
        },
        "text_between_h2": [
            "Recall that a language model \\(p\\) is a probability distribution over asequence of tokenswhere each token comes from some vocabulary \\(\\sV\\): However, natural language doesn\u2019t come as a sequence of tokens, but as just a string (concretely, sequence of Unicode characters): Atokenizerconverts any string into a sequence of tokens. This is not necessarily the most glamorous part of language modeling, but plays a really important role in determining how well a model will work. The simplest solution is to do: Therefore, splitting by spaces by spaces to identify words is quite problematic. What makes a good tokenization? Sennrich et al, 2015applied thebyte pair encoding(BPE) algorithm, originally developed for data compression, to produce one of the most commonly used tokenizers. Learning the tokenizer. Intuition: start with each character as its own token and combine tokens that co-occur a lot. Example: The output of learning is: Applying the tokenizer. To tokenize a new string, apply the merges in the same order: Unicode. Rather than just splitting by frequency, a more \u201cprincipled\u201d approach is to define an objective function that captures what a good tokenization looks like. We now describe theunigram model(Kudo 2018). Given a sequence \\(x_{1:L}\\), a tokenization \\(T\\) is a set of Example: Algorithm: Impact: Examples of tokenizations for both GPT-3 and Jurassic (demo):",
            "Thus far, we have defined language models as a probability distribution over sequences of tokens \\(p(x_1, \\dots, x_L)\\), which as we saw was very elegant and powerful (via prompting, a language model can in principle do anything, as GPT-3 hints at). In practice, however, it can be more efficient for specialized tasks to avoid having to generatively model the entire sequence. Contextual embeddings. As a prerequisite, the main key development is to associate a sequence of tokens with a corresponding sequence of contextual embeddings: We will broaden our notion of language models to three types of models. Encoder-only(BERT, RoBERTa, etc.). These language models produce contextual embeddings but cannot be used directly to generate text. These contextual embeddings are generally used for classification tasks (sometimes boldly called natural language understanding tasks). Decoder-only(GPT-2, GPT-3, etc.). These are our standard autoregressive language models, which given a prompt \\(x_{1:i}\\) produces both contextual embeddings and a distribution over next tokens \\(x_{i+1}\\) (and recursively, over the entire completion \\(x_{i+1:L}\\)). Encoder-decoder(BART, T5, etc.). These models in some ways can the best of both worlds: they can use bidirectional contextual embeddings for the input \\(x_{1:L}\\) and can generate the output \\(y_{1:L}\\). We now describe the innards of the embedding function \\(\\phi : \\sV^L \\to \\R^{d \\times L}\\): We now introduce the model architectures for language model, with an emphasis on the ubiquitous Transformer architecture. Our exposition of the Transformer architecture will be based on theseslides from CS221 on differentiable programming, and will depart a bit from the standard presentation. The beauty of deep learning is being able to create building blocks, just like we build whole programs out of functions. So we want to be able to functions like the following to encapsulate the complexity: This function will have parameters which we will include in the body but elide in the function signature for simplicity. In what follows, we will define a library of building blocks until we get to the full Transformer. First, we have to convert sequences of tokens into sequences of vectors. \\(\\EmbedToken\\) does exactly this by looking up each token in an embedding matrix \\(E \\in \\R^{|\\sV| \\times d}\\) (a parameter that will be learned from data): def \\(\\EmbedToken(\\x: \\sV^L) \\to \\R^{d \\times L}\\): These are exactly the (context-independent) word embeddings of yore. We define an abstract \\(\\SequenceModel\\) function that takes thesecontext-independent embeddingsand maps them intocontextual embeddings. def \\(\\SequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The simplest type of sequence model is based on feedforward networks (Bengio et al., 2003) applied to afixed lengthcontext, just as in an n-gram model: def \\(\\FeedForwardSequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The first \u201creal\u201d sequence model is a recurrent neural network (RNN), which is a family of models that include simple RNNs, LSTMs, and GRUs. The basic form of an RNN simply computes a sequence ofhidden statesrecursively. def \\(\\SequenceRNN(x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The actual module that does the hard work is the \\(\\RNN\\), which analogous to a finite state machine, takes the current state \\(h\\), a new observation \\(x\\), and returns the updated state: def \\(\\RNN(h: \\R^d, x: \\R^d) \\to \\R^d\\): There are three ways to implement the \\(\\RNN\\). The earliest RNN is a simple RNNElman, 1990, which takes a linear combination of \\(h\\) and \\(x\\) and pushes it through an elementwise non-linear function \\(\\sigma\\) (e.g., logistic \\(\\sigma(z) = (1 + e^{-z})^{-1}\\) or more the modern ReLU \\(\\sigma(z) = \\max(0, z)\\)). def \\(\\SimpleRNN(h: \\R^d, x: \\R^d) \\to \\R^d\\): As defined RNNs only depend on the past, but we can them depend on the future two by running another RNN backwards. These models were used byELMoandULMFiT. def \\(\\BidirectionalSequenceRNN(\\x: \\R^{d \\times L}) \\to \\R^{2d \\times L}\\): Notes: We will not discuss these models in the interest of time. Now, we will discuss Transformers (Vaswani et al. 2017), the sequence model that is really responsible for the takeoff of large language models; they are the building blocks of decoder-only (GPT-2, GPT-3), encoder-only (BERT, RoBERTa), and decoder-encoder (BART, T5) models. There are great resources for learning about the Transformer: You are highly encouraged to read these references. In this lecture, I will strive to take a middle path which emphasizes pseudocode functions and interfaces. The crux of the Transformers are theattention mechanism, which was developed earlier for machine translation (Bahdananu et al. 2017). One can think of attention as a \u201csoft\u201d lookup table, where we have a query \\(y\\) that we want to match against each element in a sequence \\(x_{1:L} = [x_1, \\dots, x_L]\\): We can think of each \\(x_i\\) as representing a key-value pair via linear transformations: and forming the query via another linear transformation: The key and the query can be compared to give a score: These scores can be exponentiated and normalized to form a probability distribution over the token positions \\(\\{ 1, \\dots, L \\}\\): Then the final output is a weighted combination over the values: We can write this all succinctly in matrix form: def \\(\\Attention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d\\): We can think of there as being multiple aspects (e.g., syntax, semantics) that we would want to match on. To accommodate this, we can simultaneously have multipleattention headsand simply combine their outputs. def \\(\\MultiHeadedAttention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d:\\) Self-attention layer. Now we will substitute each \\(x_i\\) in for \\(y\\) as the query argument to produce: def \\(\\SelfAttention(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L})\\): Feedforward layer. Self-attention allows all the tokens to \u201ctalk\u201d to each other, whereas feedforward connections provide: def \\(\\FeedForward(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Improving trainability. We\u2019re almost done. We could in principle just take the \\(\\FeedForward \\circ \\SelfAttention\\) sequence model and iterate it 96 times to make GPT-3, but that network would be hard to optimize (for the same vanishing gradients problems that afflicted RNNs, now just along the depth direction). So we have to do two shenanigans to make sure that the network is trainable. Residual connections. One trick from computer vision is residual connections (ResNet). Instead of applying some function \\(f\\): we add a residual (skip) connection so that if \\(f\\)\u2019s gradients vanish, gradients can still flow through \\(\\x\\): Layer normalization. Another trick islayer normalization, which takes a takes a vector and makes sure its elements are too big: def \\(\\LayerNorm(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): We first define an adapter function that takes a sequence model \\(f\\) and makes it \u201crobust\u201d: def \\(\\AddNorm(f: (\\R^{d \\times L} \\to \\R^{d \\times L}), \\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Finally, we can define the Transformer block succinctly as follows: def \\(\\TransformerBlock(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Positional embeddings. You might have noticed that as defined, the embedding of a token doesn\u2019t depend on where it occurs in the sequence, so \\(\\nl{mouse}\\) in both sentences would have the same embedding, which is not sensible. To fix this, we addpositional informationinto the embedding: def \\(\\EmbedTokenWithPosition(\\x: \\R^{d \\times L})\\): GPT-3. With all the pieces in place, we can now define roughly GPT-3 architecture in one line, just by stacking the Transformer block 96 times: Shape of the architecture (how the 175 billion parameters are allocated): These decisions are not necessarily optimal.Levine et al. 2020provide some theoretical justification, showing that the GPT-3 is too deep, which motivated the training of a deeper but wider Jurassic architecture. There are important but detailed differences between different versions of Transformers:",
            "Tokenization: Modeling: Decoder-only architectures: Encoder-only architectures: Encoder-decoder architectures:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/modeling/#tokenization": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/#tokenization": ""
        },
        "text_between_h2": [
            "Recall that a language model \\(p\\) is a probability distribution over asequence of tokenswhere each token comes from some vocabulary \\(\\sV\\): However, natural language doesn\u2019t come as a sequence of tokens, but as just a string (concretely, sequence of Unicode characters): Atokenizerconverts any string into a sequence of tokens. This is not necessarily the most glamorous part of language modeling, but plays a really important role in determining how well a model will work. The simplest solution is to do: Therefore, splitting by spaces by spaces to identify words is quite problematic. What makes a good tokenization? Sennrich et al, 2015applied thebyte pair encoding(BPE) algorithm, originally developed for data compression, to produce one of the most commonly used tokenizers. Learning the tokenizer. Intuition: start with each character as its own token and combine tokens that co-occur a lot. Example: The output of learning is: Applying the tokenizer. To tokenize a new string, apply the merges in the same order: Unicode. Rather than just splitting by frequency, a more \u201cprincipled\u201d approach is to define an objective function that captures what a good tokenization looks like. We now describe theunigram model(Kudo 2018). Given a sequence \\(x_{1:L}\\), a tokenization \\(T\\) is a set of Example: Algorithm: Impact: Examples of tokenizations for both GPT-3 and Jurassic (demo):",
            "Thus far, we have defined language models as a probability distribution over sequences of tokens \\(p(x_1, \\dots, x_L)\\), which as we saw was very elegant and powerful (via prompting, a language model can in principle do anything, as GPT-3 hints at). In practice, however, it can be more efficient for specialized tasks to avoid having to generatively model the entire sequence. Contextual embeddings. As a prerequisite, the main key development is to associate a sequence of tokens with a corresponding sequence of contextual embeddings: We will broaden our notion of language models to three types of models. Encoder-only(BERT, RoBERTa, etc.). These language models produce contextual embeddings but cannot be used directly to generate text. These contextual embeddings are generally used for classification tasks (sometimes boldly called natural language understanding tasks). Decoder-only(GPT-2, GPT-3, etc.). These are our standard autoregressive language models, which given a prompt \\(x_{1:i}\\) produces both contextual embeddings and a distribution over next tokens \\(x_{i+1}\\) (and recursively, over the entire completion \\(x_{i+1:L}\\)). Encoder-decoder(BART, T5, etc.). These models in some ways can the best of both worlds: they can use bidirectional contextual embeddings for the input \\(x_{1:L}\\) and can generate the output \\(y_{1:L}\\). We now describe the innards of the embedding function \\(\\phi : \\sV^L \\to \\R^{d \\times L}\\): We now introduce the model architectures for language model, with an emphasis on the ubiquitous Transformer architecture. Our exposition of the Transformer architecture will be based on theseslides from CS221 on differentiable programming, and will depart a bit from the standard presentation. The beauty of deep learning is being able to create building blocks, just like we build whole programs out of functions. So we want to be able to functions like the following to encapsulate the complexity: This function will have parameters which we will include in the body but elide in the function signature for simplicity. In what follows, we will define a library of building blocks until we get to the full Transformer. First, we have to convert sequences of tokens into sequences of vectors. \\(\\EmbedToken\\) does exactly this by looking up each token in an embedding matrix \\(E \\in \\R^{|\\sV| \\times d}\\) (a parameter that will be learned from data): def \\(\\EmbedToken(\\x: \\sV^L) \\to \\R^{d \\times L}\\): These are exactly the (context-independent) word embeddings of yore. We define an abstract \\(\\SequenceModel\\) function that takes thesecontext-independent embeddingsand maps them intocontextual embeddings. def \\(\\SequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The simplest type of sequence model is based on feedforward networks (Bengio et al., 2003) applied to afixed lengthcontext, just as in an n-gram model: def \\(\\FeedForwardSequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The first \u201creal\u201d sequence model is a recurrent neural network (RNN), which is a family of models that include simple RNNs, LSTMs, and GRUs. The basic form of an RNN simply computes a sequence ofhidden statesrecursively. def \\(\\SequenceRNN(x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The actual module that does the hard work is the \\(\\RNN\\), which analogous to a finite state machine, takes the current state \\(h\\), a new observation \\(x\\), and returns the updated state: def \\(\\RNN(h: \\R^d, x: \\R^d) \\to \\R^d\\): There are three ways to implement the \\(\\RNN\\). The earliest RNN is a simple RNNElman, 1990, which takes a linear combination of \\(h\\) and \\(x\\) and pushes it through an elementwise non-linear function \\(\\sigma\\) (e.g., logistic \\(\\sigma(z) = (1 + e^{-z})^{-1}\\) or more the modern ReLU \\(\\sigma(z) = \\max(0, z)\\)). def \\(\\SimpleRNN(h: \\R^d, x: \\R^d) \\to \\R^d\\): As defined RNNs only depend on the past, but we can them depend on the future two by running another RNN backwards. These models were used byELMoandULMFiT. def \\(\\BidirectionalSequenceRNN(\\x: \\R^{d \\times L}) \\to \\R^{2d \\times L}\\): Notes: We will not discuss these models in the interest of time. Now, we will discuss Transformers (Vaswani et al. 2017), the sequence model that is really responsible for the takeoff of large language models; they are the building blocks of decoder-only (GPT-2, GPT-3), encoder-only (BERT, RoBERTa), and decoder-encoder (BART, T5) models. There are great resources for learning about the Transformer: You are highly encouraged to read these references. In this lecture, I will strive to take a middle path which emphasizes pseudocode functions and interfaces. The crux of the Transformers are theattention mechanism, which was developed earlier for machine translation (Bahdananu et al. 2017). One can think of attention as a \u201csoft\u201d lookup table, where we have a query \\(y\\) that we want to match against each element in a sequence \\(x_{1:L} = [x_1, \\dots, x_L]\\): We can think of each \\(x_i\\) as representing a key-value pair via linear transformations: and forming the query via another linear transformation: The key and the query can be compared to give a score: These scores can be exponentiated and normalized to form a probability distribution over the token positions \\(\\{ 1, \\dots, L \\}\\): Then the final output is a weighted combination over the values: We can write this all succinctly in matrix form: def \\(\\Attention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d\\): We can think of there as being multiple aspects (e.g., syntax, semantics) that we would want to match on. To accommodate this, we can simultaneously have multipleattention headsand simply combine their outputs. def \\(\\MultiHeadedAttention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d:\\) Self-attention layer. Now we will substitute each \\(x_i\\) in for \\(y\\) as the query argument to produce: def \\(\\SelfAttention(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L})\\): Feedforward layer. Self-attention allows all the tokens to \u201ctalk\u201d to each other, whereas feedforward connections provide: def \\(\\FeedForward(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Improving trainability. We\u2019re almost done. We could in principle just take the \\(\\FeedForward \\circ \\SelfAttention\\) sequence model and iterate it 96 times to make GPT-3, but that network would be hard to optimize (for the same vanishing gradients problems that afflicted RNNs, now just along the depth direction). So we have to do two shenanigans to make sure that the network is trainable. Residual connections. One trick from computer vision is residual connections (ResNet). Instead of applying some function \\(f\\): we add a residual (skip) connection so that if \\(f\\)\u2019s gradients vanish, gradients can still flow through \\(\\x\\): Layer normalization. Another trick islayer normalization, which takes a takes a vector and makes sure its elements are too big: def \\(\\LayerNorm(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): We first define an adapter function that takes a sequence model \\(f\\) and makes it \u201crobust\u201d: def \\(\\AddNorm(f: (\\R^{d \\times L} \\to \\R^{d \\times L}), \\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Finally, we can define the Transformer block succinctly as follows: def \\(\\TransformerBlock(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Positional embeddings. You might have noticed that as defined, the embedding of a token doesn\u2019t depend on where it occurs in the sequence, so \\(\\nl{mouse}\\) in both sentences would have the same embedding, which is not sensible. To fix this, we addpositional informationinto the embedding: def \\(\\EmbedTokenWithPosition(\\x: \\R^{d \\times L})\\): GPT-3. With all the pieces in place, we can now define roughly GPT-3 architecture in one line, just by stacking the Transformer block 96 times: Shape of the architecture (how the 175 billion parameters are allocated): These decisions are not necessarily optimal.Levine et al. 2020provide some theoretical justification, showing that the GPT-3 is too deep, which motivated the training of a deeper but wider Jurassic architecture. There are important but detailed differences between different versions of Transformers:",
            "Tokenization: Modeling: Decoder-only architectures: Encoder-only architectures: Encoder-decoder architectures:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/modeling/#model-architecture": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/#model-architecture": "Model architecture"
        },
        "text_between_h2": [
            "Recall that a language model \\(p\\) is a probability distribution over asequence of tokenswhere each token comes from some vocabulary \\(\\sV\\): However, natural language doesn\u2019t come as a sequence of tokens, but as just a string (concretely, sequence of Unicode characters): Atokenizerconverts any string into a sequence of tokens. This is not necessarily the most glamorous part of language modeling, but plays a really important role in determining how well a model will work. The simplest solution is to do: Therefore, splitting by spaces by spaces to identify words is quite problematic. What makes a good tokenization? Sennrich et al, 2015applied thebyte pair encoding(BPE) algorithm, originally developed for data compression, to produce one of the most commonly used tokenizers. Learning the tokenizer. Intuition: start with each character as its own token and combine tokens that co-occur a lot. Example: The output of learning is: Applying the tokenizer. To tokenize a new string, apply the merges in the same order: Unicode. Rather than just splitting by frequency, a more \u201cprincipled\u201d approach is to define an objective function that captures what a good tokenization looks like. We now describe theunigram model(Kudo 2018). Given a sequence \\(x_{1:L}\\), a tokenization \\(T\\) is a set of Example: Algorithm: Impact: Examples of tokenizations for both GPT-3 and Jurassic (demo):",
            "Thus far, we have defined language models as a probability distribution over sequences of tokens \\(p(x_1, \\dots, x_L)\\), which as we saw was very elegant and powerful (via prompting, a language model can in principle do anything, as GPT-3 hints at). In practice, however, it can be more efficient for specialized tasks to avoid having to generatively model the entire sequence. Contextual embeddings. As a prerequisite, the main key development is to associate a sequence of tokens with a corresponding sequence of contextual embeddings: We will broaden our notion of language models to three types of models. Encoder-only(BERT, RoBERTa, etc.). These language models produce contextual embeddings but cannot be used directly to generate text. These contextual embeddings are generally used for classification tasks (sometimes boldly called natural language understanding tasks). Decoder-only(GPT-2, GPT-3, etc.). These are our standard autoregressive language models, which given a prompt \\(x_{1:i}\\) produces both contextual embeddings and a distribution over next tokens \\(x_{i+1}\\) (and recursively, over the entire completion \\(x_{i+1:L}\\)). Encoder-decoder(BART, T5, etc.). These models in some ways can the best of both worlds: they can use bidirectional contextual embeddings for the input \\(x_{1:L}\\) and can generate the output \\(y_{1:L}\\). We now describe the innards of the embedding function \\(\\phi : \\sV^L \\to \\R^{d \\times L}\\): We now introduce the model architectures for language model, with an emphasis on the ubiquitous Transformer architecture. Our exposition of the Transformer architecture will be based on theseslides from CS221 on differentiable programming, and will depart a bit from the standard presentation. The beauty of deep learning is being able to create building blocks, just like we build whole programs out of functions. So we want to be able to functions like the following to encapsulate the complexity: This function will have parameters which we will include in the body but elide in the function signature for simplicity. In what follows, we will define a library of building blocks until we get to the full Transformer. First, we have to convert sequences of tokens into sequences of vectors. \\(\\EmbedToken\\) does exactly this by looking up each token in an embedding matrix \\(E \\in \\R^{|\\sV| \\times d}\\) (a parameter that will be learned from data): def \\(\\EmbedToken(\\x: \\sV^L) \\to \\R^{d \\times L}\\): These are exactly the (context-independent) word embeddings of yore. We define an abstract \\(\\SequenceModel\\) function that takes thesecontext-independent embeddingsand maps them intocontextual embeddings. def \\(\\SequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The simplest type of sequence model is based on feedforward networks (Bengio et al., 2003) applied to afixed lengthcontext, just as in an n-gram model: def \\(\\FeedForwardSequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The first \u201creal\u201d sequence model is a recurrent neural network (RNN), which is a family of models that include simple RNNs, LSTMs, and GRUs. The basic form of an RNN simply computes a sequence ofhidden statesrecursively. def \\(\\SequenceRNN(x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The actual module that does the hard work is the \\(\\RNN\\), which analogous to a finite state machine, takes the current state \\(h\\), a new observation \\(x\\), and returns the updated state: def \\(\\RNN(h: \\R^d, x: \\R^d) \\to \\R^d\\): There are three ways to implement the \\(\\RNN\\). The earliest RNN is a simple RNNElman, 1990, which takes a linear combination of \\(h\\) and \\(x\\) and pushes it through an elementwise non-linear function \\(\\sigma\\) (e.g., logistic \\(\\sigma(z) = (1 + e^{-z})^{-1}\\) or more the modern ReLU \\(\\sigma(z) = \\max(0, z)\\)). def \\(\\SimpleRNN(h: \\R^d, x: \\R^d) \\to \\R^d\\): As defined RNNs only depend on the past, but we can them depend on the future two by running another RNN backwards. These models were used byELMoandULMFiT. def \\(\\BidirectionalSequenceRNN(\\x: \\R^{d \\times L}) \\to \\R^{2d \\times L}\\): Notes: We will not discuss these models in the interest of time. Now, we will discuss Transformers (Vaswani et al. 2017), the sequence model that is really responsible for the takeoff of large language models; they are the building blocks of decoder-only (GPT-2, GPT-3), encoder-only (BERT, RoBERTa), and decoder-encoder (BART, T5) models. There are great resources for learning about the Transformer: You are highly encouraged to read these references. In this lecture, I will strive to take a middle path which emphasizes pseudocode functions and interfaces. The crux of the Transformers are theattention mechanism, which was developed earlier for machine translation (Bahdananu et al. 2017). One can think of attention as a \u201csoft\u201d lookup table, where we have a query \\(y\\) that we want to match against each element in a sequence \\(x_{1:L} = [x_1, \\dots, x_L]\\): We can think of each \\(x_i\\) as representing a key-value pair via linear transformations: and forming the query via another linear transformation: The key and the query can be compared to give a score: These scores can be exponentiated and normalized to form a probability distribution over the token positions \\(\\{ 1, \\dots, L \\}\\): Then the final output is a weighted combination over the values: We can write this all succinctly in matrix form: def \\(\\Attention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d\\): We can think of there as being multiple aspects (e.g., syntax, semantics) that we would want to match on. To accommodate this, we can simultaneously have multipleattention headsand simply combine their outputs. def \\(\\MultiHeadedAttention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d:\\) Self-attention layer. Now we will substitute each \\(x_i\\) in for \\(y\\) as the query argument to produce: def \\(\\SelfAttention(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L})\\): Feedforward layer. Self-attention allows all the tokens to \u201ctalk\u201d to each other, whereas feedforward connections provide: def \\(\\FeedForward(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Improving trainability. We\u2019re almost done. We could in principle just take the \\(\\FeedForward \\circ \\SelfAttention\\) sequence model and iterate it 96 times to make GPT-3, but that network would be hard to optimize (for the same vanishing gradients problems that afflicted RNNs, now just along the depth direction). So we have to do two shenanigans to make sure that the network is trainable. Residual connections. One trick from computer vision is residual connections (ResNet). Instead of applying some function \\(f\\): we add a residual (skip) connection so that if \\(f\\)\u2019s gradients vanish, gradients can still flow through \\(\\x\\): Layer normalization. Another trick islayer normalization, which takes a takes a vector and makes sure its elements are too big: def \\(\\LayerNorm(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): We first define an adapter function that takes a sequence model \\(f\\) and makes it \u201crobust\u201d: def \\(\\AddNorm(f: (\\R^{d \\times L} \\to \\R^{d \\times L}), \\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Finally, we can define the Transformer block succinctly as follows: def \\(\\TransformerBlock(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Positional embeddings. You might have noticed that as defined, the embedding of a token doesn\u2019t depend on where it occurs in the sequence, so \\(\\nl{mouse}\\) in both sentences would have the same embedding, which is not sensible. To fix this, we addpositional informationinto the embedding: def \\(\\EmbedTokenWithPosition(\\x: \\R^{d \\times L})\\): GPT-3. With all the pieces in place, we can now define roughly GPT-3 architecture in one line, just by stacking the Transformer block 96 times: Shape of the architecture (how the 175 billion parameters are allocated): These decisions are not necessarily optimal.Levine et al. 2020provide some theoretical justification, showing that the GPT-3 is too deep, which motivated the training of a deeper but wider Jurassic architecture. There are important but detailed differences between different versions of Transformers:",
            "Tokenization: Modeling: Decoder-only architectures: Encoder-only architectures: Encoder-decoder architectures:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/modeling/#split-by-spaces": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/#split-by-spaces": ""
        },
        "text_between_h2": [
            "Recall that a language model \\(p\\) is a probability distribution over asequence of tokenswhere each token comes from some vocabulary \\(\\sV\\): However, natural language doesn\u2019t come as a sequence of tokens, but as just a string (concretely, sequence of Unicode characters): Atokenizerconverts any string into a sequence of tokens. This is not necessarily the most glamorous part of language modeling, but plays a really important role in determining how well a model will work. The simplest solution is to do: Therefore, splitting by spaces by spaces to identify words is quite problematic. What makes a good tokenization? Sennrich et al, 2015applied thebyte pair encoding(BPE) algorithm, originally developed for data compression, to produce one of the most commonly used tokenizers. Learning the tokenizer. Intuition: start with each character as its own token and combine tokens that co-occur a lot. Example: The output of learning is: Applying the tokenizer. To tokenize a new string, apply the merges in the same order: Unicode. Rather than just splitting by frequency, a more \u201cprincipled\u201d approach is to define an objective function that captures what a good tokenization looks like. We now describe theunigram model(Kudo 2018). Given a sequence \\(x_{1:L}\\), a tokenization \\(T\\) is a set of Example: Algorithm: Impact: Examples of tokenizations for both GPT-3 and Jurassic (demo):",
            "Thus far, we have defined language models as a probability distribution over sequences of tokens \\(p(x_1, \\dots, x_L)\\), which as we saw was very elegant and powerful (via prompting, a language model can in principle do anything, as GPT-3 hints at). In practice, however, it can be more efficient for specialized tasks to avoid having to generatively model the entire sequence. Contextual embeddings. As a prerequisite, the main key development is to associate a sequence of tokens with a corresponding sequence of contextual embeddings: We will broaden our notion of language models to three types of models. Encoder-only(BERT, RoBERTa, etc.). These language models produce contextual embeddings but cannot be used directly to generate text. These contextual embeddings are generally used for classification tasks (sometimes boldly called natural language understanding tasks). Decoder-only(GPT-2, GPT-3, etc.). These are our standard autoregressive language models, which given a prompt \\(x_{1:i}\\) produces both contextual embeddings and a distribution over next tokens \\(x_{i+1}\\) (and recursively, over the entire completion \\(x_{i+1:L}\\)). Encoder-decoder(BART, T5, etc.). These models in some ways can the best of both worlds: they can use bidirectional contextual embeddings for the input \\(x_{1:L}\\) and can generate the output \\(y_{1:L}\\). We now describe the innards of the embedding function \\(\\phi : \\sV^L \\to \\R^{d \\times L}\\): We now introduce the model architectures for language model, with an emphasis on the ubiquitous Transformer architecture. Our exposition of the Transformer architecture will be based on theseslides from CS221 on differentiable programming, and will depart a bit from the standard presentation. The beauty of deep learning is being able to create building blocks, just like we build whole programs out of functions. So we want to be able to functions like the following to encapsulate the complexity: This function will have parameters which we will include in the body but elide in the function signature for simplicity. In what follows, we will define a library of building blocks until we get to the full Transformer. First, we have to convert sequences of tokens into sequences of vectors. \\(\\EmbedToken\\) does exactly this by looking up each token in an embedding matrix \\(E \\in \\R^{|\\sV| \\times d}\\) (a parameter that will be learned from data): def \\(\\EmbedToken(\\x: \\sV^L) \\to \\R^{d \\times L}\\): These are exactly the (context-independent) word embeddings of yore. We define an abstract \\(\\SequenceModel\\) function that takes thesecontext-independent embeddingsand maps them intocontextual embeddings. def \\(\\SequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The simplest type of sequence model is based on feedforward networks (Bengio et al., 2003) applied to afixed lengthcontext, just as in an n-gram model: def \\(\\FeedForwardSequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The first \u201creal\u201d sequence model is a recurrent neural network (RNN), which is a family of models that include simple RNNs, LSTMs, and GRUs. The basic form of an RNN simply computes a sequence ofhidden statesrecursively. def \\(\\SequenceRNN(x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The actual module that does the hard work is the \\(\\RNN\\), which analogous to a finite state machine, takes the current state \\(h\\), a new observation \\(x\\), and returns the updated state: def \\(\\RNN(h: \\R^d, x: \\R^d) \\to \\R^d\\): There are three ways to implement the \\(\\RNN\\). The earliest RNN is a simple RNNElman, 1990, which takes a linear combination of \\(h\\) and \\(x\\) and pushes it through an elementwise non-linear function \\(\\sigma\\) (e.g., logistic \\(\\sigma(z) = (1 + e^{-z})^{-1}\\) or more the modern ReLU \\(\\sigma(z) = \\max(0, z)\\)). def \\(\\SimpleRNN(h: \\R^d, x: \\R^d) \\to \\R^d\\): As defined RNNs only depend on the past, but we can them depend on the future two by running another RNN backwards. These models were used byELMoandULMFiT. def \\(\\BidirectionalSequenceRNN(\\x: \\R^{d \\times L}) \\to \\R^{2d \\times L}\\): Notes: We will not discuss these models in the interest of time. Now, we will discuss Transformers (Vaswani et al. 2017), the sequence model that is really responsible for the takeoff of large language models; they are the building blocks of decoder-only (GPT-2, GPT-3), encoder-only (BERT, RoBERTa), and decoder-encoder (BART, T5) models. There are great resources for learning about the Transformer: You are highly encouraged to read these references. In this lecture, I will strive to take a middle path which emphasizes pseudocode functions and interfaces. The crux of the Transformers are theattention mechanism, which was developed earlier for machine translation (Bahdananu et al. 2017). One can think of attention as a \u201csoft\u201d lookup table, where we have a query \\(y\\) that we want to match against each element in a sequence \\(x_{1:L} = [x_1, \\dots, x_L]\\): We can think of each \\(x_i\\) as representing a key-value pair via linear transformations: and forming the query via another linear transformation: The key and the query can be compared to give a score: These scores can be exponentiated and normalized to form a probability distribution over the token positions \\(\\{ 1, \\dots, L \\}\\): Then the final output is a weighted combination over the values: We can write this all succinctly in matrix form: def \\(\\Attention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d\\): We can think of there as being multiple aspects (e.g., syntax, semantics) that we would want to match on. To accommodate this, we can simultaneously have multipleattention headsand simply combine their outputs. def \\(\\MultiHeadedAttention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d:\\) Self-attention layer. Now we will substitute each \\(x_i\\) in for \\(y\\) as the query argument to produce: def \\(\\SelfAttention(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L})\\): Feedforward layer. Self-attention allows all the tokens to \u201ctalk\u201d to each other, whereas feedforward connections provide: def \\(\\FeedForward(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Improving trainability. We\u2019re almost done. We could in principle just take the \\(\\FeedForward \\circ \\SelfAttention\\) sequence model and iterate it 96 times to make GPT-3, but that network would be hard to optimize (for the same vanishing gradients problems that afflicted RNNs, now just along the depth direction). So we have to do two shenanigans to make sure that the network is trainable. Residual connections. One trick from computer vision is residual connections (ResNet). Instead of applying some function \\(f\\): we add a residual (skip) connection so that if \\(f\\)\u2019s gradients vanish, gradients can still flow through \\(\\x\\): Layer normalization. Another trick islayer normalization, which takes a takes a vector and makes sure its elements are too big: def \\(\\LayerNorm(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): We first define an adapter function that takes a sequence model \\(f\\) and makes it \u201crobust\u201d: def \\(\\AddNorm(f: (\\R^{d \\times L} \\to \\R^{d \\times L}), \\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Finally, we can define the Transformer block succinctly as follows: def \\(\\TransformerBlock(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Positional embeddings. You might have noticed that as defined, the embedding of a token doesn\u2019t depend on where it occurs in the sequence, so \\(\\nl{mouse}\\) in both sentences would have the same embedding, which is not sensible. To fix this, we addpositional informationinto the embedding: def \\(\\EmbedTokenWithPosition(\\x: \\R^{d \\times L})\\): GPT-3. With all the pieces in place, we can now define roughly GPT-3 architecture in one line, just by stacking the Transformer block 96 times: Shape of the architecture (how the 175 billion parameters are allocated): These decisions are not necessarily optimal.Levine et al. 2020provide some theoretical justification, showing that the GPT-3 is too deep, which motivated the training of a deeper but wider Jurassic architecture. There are important but detailed differences between different versions of Transformers:",
            "Tokenization: Modeling: Decoder-only architectures: Encoder-only architectures: Encoder-decoder architectures:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/modeling/#byte-pair-encoding": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/#byte-pair-encoding": ""
        },
        "text_between_h2": [
            "Recall that a language model \\(p\\) is a probability distribution over asequence of tokenswhere each token comes from some vocabulary \\(\\sV\\): However, natural language doesn\u2019t come as a sequence of tokens, but as just a string (concretely, sequence of Unicode characters): Atokenizerconverts any string into a sequence of tokens. This is not necessarily the most glamorous part of language modeling, but plays a really important role in determining how well a model will work. The simplest solution is to do: Therefore, splitting by spaces by spaces to identify words is quite problematic. What makes a good tokenization? Sennrich et al, 2015applied thebyte pair encoding(BPE) algorithm, originally developed for data compression, to produce one of the most commonly used tokenizers. Learning the tokenizer. Intuition: start with each character as its own token and combine tokens that co-occur a lot. Example: The output of learning is: Applying the tokenizer. To tokenize a new string, apply the merges in the same order: Unicode. Rather than just splitting by frequency, a more \u201cprincipled\u201d approach is to define an objective function that captures what a good tokenization looks like. We now describe theunigram model(Kudo 2018). Given a sequence \\(x_{1:L}\\), a tokenization \\(T\\) is a set of Example: Algorithm: Impact: Examples of tokenizations for both GPT-3 and Jurassic (demo):",
            "Thus far, we have defined language models as a probability distribution over sequences of tokens \\(p(x_1, \\dots, x_L)\\), which as we saw was very elegant and powerful (via prompting, a language model can in principle do anything, as GPT-3 hints at). In practice, however, it can be more efficient for specialized tasks to avoid having to generatively model the entire sequence. Contextual embeddings. As a prerequisite, the main key development is to associate a sequence of tokens with a corresponding sequence of contextual embeddings: We will broaden our notion of language models to three types of models. Encoder-only(BERT, RoBERTa, etc.). These language models produce contextual embeddings but cannot be used directly to generate text. These contextual embeddings are generally used for classification tasks (sometimes boldly called natural language understanding tasks). Decoder-only(GPT-2, GPT-3, etc.). These are our standard autoregressive language models, which given a prompt \\(x_{1:i}\\) produces both contextual embeddings and a distribution over next tokens \\(x_{i+1}\\) (and recursively, over the entire completion \\(x_{i+1:L}\\)). Encoder-decoder(BART, T5, etc.). These models in some ways can the best of both worlds: they can use bidirectional contextual embeddings for the input \\(x_{1:L}\\) and can generate the output \\(y_{1:L}\\). We now describe the innards of the embedding function \\(\\phi : \\sV^L \\to \\R^{d \\times L}\\): We now introduce the model architectures for language model, with an emphasis on the ubiquitous Transformer architecture. Our exposition of the Transformer architecture will be based on theseslides from CS221 on differentiable programming, and will depart a bit from the standard presentation. The beauty of deep learning is being able to create building blocks, just like we build whole programs out of functions. So we want to be able to functions like the following to encapsulate the complexity: This function will have parameters which we will include in the body but elide in the function signature for simplicity. In what follows, we will define a library of building blocks until we get to the full Transformer. First, we have to convert sequences of tokens into sequences of vectors. \\(\\EmbedToken\\) does exactly this by looking up each token in an embedding matrix \\(E \\in \\R^{|\\sV| \\times d}\\) (a parameter that will be learned from data): def \\(\\EmbedToken(\\x: \\sV^L) \\to \\R^{d \\times L}\\): These are exactly the (context-independent) word embeddings of yore. We define an abstract \\(\\SequenceModel\\) function that takes thesecontext-independent embeddingsand maps them intocontextual embeddings. def \\(\\SequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The simplest type of sequence model is based on feedforward networks (Bengio et al., 2003) applied to afixed lengthcontext, just as in an n-gram model: def \\(\\FeedForwardSequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The first \u201creal\u201d sequence model is a recurrent neural network (RNN), which is a family of models that include simple RNNs, LSTMs, and GRUs. The basic form of an RNN simply computes a sequence ofhidden statesrecursively. def \\(\\SequenceRNN(x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The actual module that does the hard work is the \\(\\RNN\\), which analogous to a finite state machine, takes the current state \\(h\\), a new observation \\(x\\), and returns the updated state: def \\(\\RNN(h: \\R^d, x: \\R^d) \\to \\R^d\\): There are three ways to implement the \\(\\RNN\\). The earliest RNN is a simple RNNElman, 1990, which takes a linear combination of \\(h\\) and \\(x\\) and pushes it through an elementwise non-linear function \\(\\sigma\\) (e.g., logistic \\(\\sigma(z) = (1 + e^{-z})^{-1}\\) or more the modern ReLU \\(\\sigma(z) = \\max(0, z)\\)). def \\(\\SimpleRNN(h: \\R^d, x: \\R^d) \\to \\R^d\\): As defined RNNs only depend on the past, but we can them depend on the future two by running another RNN backwards. These models were used byELMoandULMFiT. def \\(\\BidirectionalSequenceRNN(\\x: \\R^{d \\times L}) \\to \\R^{2d \\times L}\\): Notes: We will not discuss these models in the interest of time. Now, we will discuss Transformers (Vaswani et al. 2017), the sequence model that is really responsible for the takeoff of large language models; they are the building blocks of decoder-only (GPT-2, GPT-3), encoder-only (BERT, RoBERTa), and decoder-encoder (BART, T5) models. There are great resources for learning about the Transformer: You are highly encouraged to read these references. In this lecture, I will strive to take a middle path which emphasizes pseudocode functions and interfaces. The crux of the Transformers are theattention mechanism, which was developed earlier for machine translation (Bahdananu et al. 2017). One can think of attention as a \u201csoft\u201d lookup table, where we have a query \\(y\\) that we want to match against each element in a sequence \\(x_{1:L} = [x_1, \\dots, x_L]\\): We can think of each \\(x_i\\) as representing a key-value pair via linear transformations: and forming the query via another linear transformation: The key and the query can be compared to give a score: These scores can be exponentiated and normalized to form a probability distribution over the token positions \\(\\{ 1, \\dots, L \\}\\): Then the final output is a weighted combination over the values: We can write this all succinctly in matrix form: def \\(\\Attention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d\\): We can think of there as being multiple aspects (e.g., syntax, semantics) that we would want to match on. To accommodate this, we can simultaneously have multipleattention headsand simply combine their outputs. def \\(\\MultiHeadedAttention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d:\\) Self-attention layer. Now we will substitute each \\(x_i\\) in for \\(y\\) as the query argument to produce: def \\(\\SelfAttention(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L})\\): Feedforward layer. Self-attention allows all the tokens to \u201ctalk\u201d to each other, whereas feedforward connections provide: def \\(\\FeedForward(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Improving trainability. We\u2019re almost done. We could in principle just take the \\(\\FeedForward \\circ \\SelfAttention\\) sequence model and iterate it 96 times to make GPT-3, but that network would be hard to optimize (for the same vanishing gradients problems that afflicted RNNs, now just along the depth direction). So we have to do two shenanigans to make sure that the network is trainable. Residual connections. One trick from computer vision is residual connections (ResNet). Instead of applying some function \\(f\\): we add a residual (skip) connection so that if \\(f\\)\u2019s gradients vanish, gradients can still flow through \\(\\x\\): Layer normalization. Another trick islayer normalization, which takes a takes a vector and makes sure its elements are too big: def \\(\\LayerNorm(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): We first define an adapter function that takes a sequence model \\(f\\) and makes it \u201crobust\u201d: def \\(\\AddNorm(f: (\\R^{d \\times L} \\to \\R^{d \\times L}), \\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Finally, we can define the Transformer block succinctly as follows: def \\(\\TransformerBlock(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Positional embeddings. You might have noticed that as defined, the embedding of a token doesn\u2019t depend on where it occurs in the sequence, so \\(\\nl{mouse}\\) in both sentences would have the same embedding, which is not sensible. To fix this, we addpositional informationinto the embedding: def \\(\\EmbedTokenWithPosition(\\x: \\R^{d \\times L})\\): GPT-3. With all the pieces in place, we can now define roughly GPT-3 architecture in one line, just by stacking the Transformer block 96 times: Shape of the architecture (how the 175 billion parameters are allocated): These decisions are not necessarily optimal.Levine et al. 2020provide some theoretical justification, showing that the GPT-3 is too deep, which motivated the training of a deeper but wider Jurassic architecture. There are important but detailed differences between different versions of Transformers:",
            "Tokenization: Modeling: Decoder-only architectures: Encoder-only architectures: Encoder-decoder architectures:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/modeling/#unigram-model-sentencepiece": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/#unigram-model-sentencepiece": ""
        },
        "text_between_h2": [
            "Recall that a language model \\(p\\) is a probability distribution over asequence of tokenswhere each token comes from some vocabulary \\(\\sV\\): However, natural language doesn\u2019t come as a sequence of tokens, but as just a string (concretely, sequence of Unicode characters): Atokenizerconverts any string into a sequence of tokens. This is not necessarily the most glamorous part of language modeling, but plays a really important role in determining how well a model will work. The simplest solution is to do: Therefore, splitting by spaces by spaces to identify words is quite problematic. What makes a good tokenization? Sennrich et al, 2015applied thebyte pair encoding(BPE) algorithm, originally developed for data compression, to produce one of the most commonly used tokenizers. Learning the tokenizer. Intuition: start with each character as its own token and combine tokens that co-occur a lot. Example: The output of learning is: Applying the tokenizer. To tokenize a new string, apply the merges in the same order: Unicode. Rather than just splitting by frequency, a more \u201cprincipled\u201d approach is to define an objective function that captures what a good tokenization looks like. We now describe theunigram model(Kudo 2018). Given a sequence \\(x_{1:L}\\), a tokenization \\(T\\) is a set of Example: Algorithm: Impact: Examples of tokenizations for both GPT-3 and Jurassic (demo):",
            "Thus far, we have defined language models as a probability distribution over sequences of tokens \\(p(x_1, \\dots, x_L)\\), which as we saw was very elegant and powerful (via prompting, a language model can in principle do anything, as GPT-3 hints at). In practice, however, it can be more efficient for specialized tasks to avoid having to generatively model the entire sequence. Contextual embeddings. As a prerequisite, the main key development is to associate a sequence of tokens with a corresponding sequence of contextual embeddings: We will broaden our notion of language models to three types of models. Encoder-only(BERT, RoBERTa, etc.). These language models produce contextual embeddings but cannot be used directly to generate text. These contextual embeddings are generally used for classification tasks (sometimes boldly called natural language understanding tasks). Decoder-only(GPT-2, GPT-3, etc.). These are our standard autoregressive language models, which given a prompt \\(x_{1:i}\\) produces both contextual embeddings and a distribution over next tokens \\(x_{i+1}\\) (and recursively, over the entire completion \\(x_{i+1:L}\\)). Encoder-decoder(BART, T5, etc.). These models in some ways can the best of both worlds: they can use bidirectional contextual embeddings for the input \\(x_{1:L}\\) and can generate the output \\(y_{1:L}\\). We now describe the innards of the embedding function \\(\\phi : \\sV^L \\to \\R^{d \\times L}\\): We now introduce the model architectures for language model, with an emphasis on the ubiquitous Transformer architecture. Our exposition of the Transformer architecture will be based on theseslides from CS221 on differentiable programming, and will depart a bit from the standard presentation. The beauty of deep learning is being able to create building blocks, just like we build whole programs out of functions. So we want to be able to functions like the following to encapsulate the complexity: This function will have parameters which we will include in the body but elide in the function signature for simplicity. In what follows, we will define a library of building blocks until we get to the full Transformer. First, we have to convert sequences of tokens into sequences of vectors. \\(\\EmbedToken\\) does exactly this by looking up each token in an embedding matrix \\(E \\in \\R^{|\\sV| \\times d}\\) (a parameter that will be learned from data): def \\(\\EmbedToken(\\x: \\sV^L) \\to \\R^{d \\times L}\\): These are exactly the (context-independent) word embeddings of yore. We define an abstract \\(\\SequenceModel\\) function that takes thesecontext-independent embeddingsand maps them intocontextual embeddings. def \\(\\SequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The simplest type of sequence model is based on feedforward networks (Bengio et al., 2003) applied to afixed lengthcontext, just as in an n-gram model: def \\(\\FeedForwardSequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The first \u201creal\u201d sequence model is a recurrent neural network (RNN), which is a family of models that include simple RNNs, LSTMs, and GRUs. The basic form of an RNN simply computes a sequence ofhidden statesrecursively. def \\(\\SequenceRNN(x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The actual module that does the hard work is the \\(\\RNN\\), which analogous to a finite state machine, takes the current state \\(h\\), a new observation \\(x\\), and returns the updated state: def \\(\\RNN(h: \\R^d, x: \\R^d) \\to \\R^d\\): There are three ways to implement the \\(\\RNN\\). The earliest RNN is a simple RNNElman, 1990, which takes a linear combination of \\(h\\) and \\(x\\) and pushes it through an elementwise non-linear function \\(\\sigma\\) (e.g., logistic \\(\\sigma(z) = (1 + e^{-z})^{-1}\\) or more the modern ReLU \\(\\sigma(z) = \\max(0, z)\\)). def \\(\\SimpleRNN(h: \\R^d, x: \\R^d) \\to \\R^d\\): As defined RNNs only depend on the past, but we can them depend on the future two by running another RNN backwards. These models were used byELMoandULMFiT. def \\(\\BidirectionalSequenceRNN(\\x: \\R^{d \\times L}) \\to \\R^{2d \\times L}\\): Notes: We will not discuss these models in the interest of time. Now, we will discuss Transformers (Vaswani et al. 2017), the sequence model that is really responsible for the takeoff of large language models; they are the building blocks of decoder-only (GPT-2, GPT-3), encoder-only (BERT, RoBERTa), and decoder-encoder (BART, T5) models. There are great resources for learning about the Transformer: You are highly encouraged to read these references. In this lecture, I will strive to take a middle path which emphasizes pseudocode functions and interfaces. The crux of the Transformers are theattention mechanism, which was developed earlier for machine translation (Bahdananu et al. 2017). One can think of attention as a \u201csoft\u201d lookup table, where we have a query \\(y\\) that we want to match against each element in a sequence \\(x_{1:L} = [x_1, \\dots, x_L]\\): We can think of each \\(x_i\\) as representing a key-value pair via linear transformations: and forming the query via another linear transformation: The key and the query can be compared to give a score: These scores can be exponentiated and normalized to form a probability distribution over the token positions \\(\\{ 1, \\dots, L \\}\\): Then the final output is a weighted combination over the values: We can write this all succinctly in matrix form: def \\(\\Attention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d\\): We can think of there as being multiple aspects (e.g., syntax, semantics) that we would want to match on. To accommodate this, we can simultaneously have multipleattention headsand simply combine their outputs. def \\(\\MultiHeadedAttention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d:\\) Self-attention layer. Now we will substitute each \\(x_i\\) in for \\(y\\) as the query argument to produce: def \\(\\SelfAttention(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L})\\): Feedforward layer. Self-attention allows all the tokens to \u201ctalk\u201d to each other, whereas feedforward connections provide: def \\(\\FeedForward(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Improving trainability. We\u2019re almost done. We could in principle just take the \\(\\FeedForward \\circ \\SelfAttention\\) sequence model and iterate it 96 times to make GPT-3, but that network would be hard to optimize (for the same vanishing gradients problems that afflicted RNNs, now just along the depth direction). So we have to do two shenanigans to make sure that the network is trainable. Residual connections. One trick from computer vision is residual connections (ResNet). Instead of applying some function \\(f\\): we add a residual (skip) connection so that if \\(f\\)\u2019s gradients vanish, gradients can still flow through \\(\\x\\): Layer normalization. Another trick islayer normalization, which takes a takes a vector and makes sure its elements are too big: def \\(\\LayerNorm(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): We first define an adapter function that takes a sequence model \\(f\\) and makes it \u201crobust\u201d: def \\(\\AddNorm(f: (\\R^{d \\times L} \\to \\R^{d \\times L}), \\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Finally, we can define the Transformer block succinctly as follows: def \\(\\TransformerBlock(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Positional embeddings. You might have noticed that as defined, the embedding of a token doesn\u2019t depend on where it occurs in the sequence, so \\(\\nl{mouse}\\) in both sentences would have the same embedding, which is not sensible. To fix this, we addpositional informationinto the embedding: def \\(\\EmbedTokenWithPosition(\\x: \\R^{d \\times L})\\): GPT-3. With all the pieces in place, we can now define roughly GPT-3 architecture in one line, just by stacking the Transformer block 96 times: Shape of the architecture (how the 175 billion parameters are allocated): These decisions are not necessarily optimal.Levine et al. 2020provide some theoretical justification, showing that the GPT-3 is too deep, which motivated the training of a deeper but wider Jurassic architecture. There are important but detailed differences between different versions of Transformers:",
            "Tokenization: Modeling: Decoder-only architectures: Encoder-only architectures: Encoder-decoder architectures:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/modeling/#comparing-tokenizers": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/#comparing-tokenizers": ""
        },
        "text_between_h2": [
            "Recall that a language model \\(p\\) is a probability distribution over asequence of tokenswhere each token comes from some vocabulary \\(\\sV\\): However, natural language doesn\u2019t come as a sequence of tokens, but as just a string (concretely, sequence of Unicode characters): Atokenizerconverts any string into a sequence of tokens. This is not necessarily the most glamorous part of language modeling, but plays a really important role in determining how well a model will work. The simplest solution is to do: Therefore, splitting by spaces by spaces to identify words is quite problematic. What makes a good tokenization? Sennrich et al, 2015applied thebyte pair encoding(BPE) algorithm, originally developed for data compression, to produce one of the most commonly used tokenizers. Learning the tokenizer. Intuition: start with each character as its own token and combine tokens that co-occur a lot. Example: The output of learning is: Applying the tokenizer. To tokenize a new string, apply the merges in the same order: Unicode. Rather than just splitting by frequency, a more \u201cprincipled\u201d approach is to define an objective function that captures what a good tokenization looks like. We now describe theunigram model(Kudo 2018). Given a sequence \\(x_{1:L}\\), a tokenization \\(T\\) is a set of Example: Algorithm: Impact: Examples of tokenizations for both GPT-3 and Jurassic (demo):",
            "Thus far, we have defined language models as a probability distribution over sequences of tokens \\(p(x_1, \\dots, x_L)\\), which as we saw was very elegant and powerful (via prompting, a language model can in principle do anything, as GPT-3 hints at). In practice, however, it can be more efficient for specialized tasks to avoid having to generatively model the entire sequence. Contextual embeddings. As a prerequisite, the main key development is to associate a sequence of tokens with a corresponding sequence of contextual embeddings: We will broaden our notion of language models to three types of models. Encoder-only(BERT, RoBERTa, etc.). These language models produce contextual embeddings but cannot be used directly to generate text. These contextual embeddings are generally used for classification tasks (sometimes boldly called natural language understanding tasks). Decoder-only(GPT-2, GPT-3, etc.). These are our standard autoregressive language models, which given a prompt \\(x_{1:i}\\) produces both contextual embeddings and a distribution over next tokens \\(x_{i+1}\\) (and recursively, over the entire completion \\(x_{i+1:L}\\)). Encoder-decoder(BART, T5, etc.). These models in some ways can the best of both worlds: they can use bidirectional contextual embeddings for the input \\(x_{1:L}\\) and can generate the output \\(y_{1:L}\\). We now describe the innards of the embedding function \\(\\phi : \\sV^L \\to \\R^{d \\times L}\\): We now introduce the model architectures for language model, with an emphasis on the ubiquitous Transformer architecture. Our exposition of the Transformer architecture will be based on theseslides from CS221 on differentiable programming, and will depart a bit from the standard presentation. The beauty of deep learning is being able to create building blocks, just like we build whole programs out of functions. So we want to be able to functions like the following to encapsulate the complexity: This function will have parameters which we will include in the body but elide in the function signature for simplicity. In what follows, we will define a library of building blocks until we get to the full Transformer. First, we have to convert sequences of tokens into sequences of vectors. \\(\\EmbedToken\\) does exactly this by looking up each token in an embedding matrix \\(E \\in \\R^{|\\sV| \\times d}\\) (a parameter that will be learned from data): def \\(\\EmbedToken(\\x: \\sV^L) \\to \\R^{d \\times L}\\): These are exactly the (context-independent) word embeddings of yore. We define an abstract \\(\\SequenceModel\\) function that takes thesecontext-independent embeddingsand maps them intocontextual embeddings. def \\(\\SequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The simplest type of sequence model is based on feedforward networks (Bengio et al., 2003) applied to afixed lengthcontext, just as in an n-gram model: def \\(\\FeedForwardSequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The first \u201creal\u201d sequence model is a recurrent neural network (RNN), which is a family of models that include simple RNNs, LSTMs, and GRUs. The basic form of an RNN simply computes a sequence ofhidden statesrecursively. def \\(\\SequenceRNN(x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The actual module that does the hard work is the \\(\\RNN\\), which analogous to a finite state machine, takes the current state \\(h\\), a new observation \\(x\\), and returns the updated state: def \\(\\RNN(h: \\R^d, x: \\R^d) \\to \\R^d\\): There are three ways to implement the \\(\\RNN\\). The earliest RNN is a simple RNNElman, 1990, which takes a linear combination of \\(h\\) and \\(x\\) and pushes it through an elementwise non-linear function \\(\\sigma\\) (e.g., logistic \\(\\sigma(z) = (1 + e^{-z})^{-1}\\) or more the modern ReLU \\(\\sigma(z) = \\max(0, z)\\)). def \\(\\SimpleRNN(h: \\R^d, x: \\R^d) \\to \\R^d\\): As defined RNNs only depend on the past, but we can them depend on the future two by running another RNN backwards. These models were used byELMoandULMFiT. def \\(\\BidirectionalSequenceRNN(\\x: \\R^{d \\times L}) \\to \\R^{2d \\times L}\\): Notes: We will not discuss these models in the interest of time. Now, we will discuss Transformers (Vaswani et al. 2017), the sequence model that is really responsible for the takeoff of large language models; they are the building blocks of decoder-only (GPT-2, GPT-3), encoder-only (BERT, RoBERTa), and decoder-encoder (BART, T5) models. There are great resources for learning about the Transformer: You are highly encouraged to read these references. In this lecture, I will strive to take a middle path which emphasizes pseudocode functions and interfaces. The crux of the Transformers are theattention mechanism, which was developed earlier for machine translation (Bahdananu et al. 2017). One can think of attention as a \u201csoft\u201d lookup table, where we have a query \\(y\\) that we want to match against each element in a sequence \\(x_{1:L} = [x_1, \\dots, x_L]\\): We can think of each \\(x_i\\) as representing a key-value pair via linear transformations: and forming the query via another linear transformation: The key and the query can be compared to give a score: These scores can be exponentiated and normalized to form a probability distribution over the token positions \\(\\{ 1, \\dots, L \\}\\): Then the final output is a weighted combination over the values: We can write this all succinctly in matrix form: def \\(\\Attention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d\\): We can think of there as being multiple aspects (e.g., syntax, semantics) that we would want to match on. To accommodate this, we can simultaneously have multipleattention headsand simply combine their outputs. def \\(\\MultiHeadedAttention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d:\\) Self-attention layer. Now we will substitute each \\(x_i\\) in for \\(y\\) as the query argument to produce: def \\(\\SelfAttention(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L})\\): Feedforward layer. Self-attention allows all the tokens to \u201ctalk\u201d to each other, whereas feedforward connections provide: def \\(\\FeedForward(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Improving trainability. We\u2019re almost done. We could in principle just take the \\(\\FeedForward \\circ \\SelfAttention\\) sequence model and iterate it 96 times to make GPT-3, but that network would be hard to optimize (for the same vanishing gradients problems that afflicted RNNs, now just along the depth direction). So we have to do two shenanigans to make sure that the network is trainable. Residual connections. One trick from computer vision is residual connections (ResNet). Instead of applying some function \\(f\\): we add a residual (skip) connection so that if \\(f\\)\u2019s gradients vanish, gradients can still flow through \\(\\x\\): Layer normalization. Another trick islayer normalization, which takes a takes a vector and makes sure its elements are too big: def \\(\\LayerNorm(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): We first define an adapter function that takes a sequence model \\(f\\) and makes it \u201crobust\u201d: def \\(\\AddNorm(f: (\\R^{d \\times L} \\to \\R^{d \\times L}), \\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Finally, we can define the Transformer block succinctly as follows: def \\(\\TransformerBlock(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Positional embeddings. You might have noticed that as defined, the embedding of a token doesn\u2019t depend on where it occurs in the sequence, so \\(\\nl{mouse}\\) in both sentences would have the same embedding, which is not sensible. To fix this, we addpositional informationinto the embedding: def \\(\\EmbedTokenWithPosition(\\x: \\R^{d \\times L})\\): GPT-3. With all the pieces in place, we can now define roughly GPT-3 architecture in one line, just by stacking the Transformer block 96 times: Shape of the architecture (how the 175 billion parameters are allocated): These decisions are not necessarily optimal.Levine et al. 2020provide some theoretical justification, showing that the GPT-3 is too deep, which motivated the training of a deeper but wider Jurassic architecture. There are important but detailed differences between different versions of Transformers:",
            "Tokenization: Modeling: Decoder-only architectures: Encoder-only architectures: Encoder-decoder architectures:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/modeling/#models": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/#models": ""
        },
        "text_between_h2": [
            "Recall that a language model \\(p\\) is a probability distribution over asequence of tokenswhere each token comes from some vocabulary \\(\\sV\\): However, natural language doesn\u2019t come as a sequence of tokens, but as just a string (concretely, sequence of Unicode characters): Atokenizerconverts any string into a sequence of tokens. This is not necessarily the most glamorous part of language modeling, but plays a really important role in determining how well a model will work. The simplest solution is to do: Therefore, splitting by spaces by spaces to identify words is quite problematic. What makes a good tokenization? Sennrich et al, 2015applied thebyte pair encoding(BPE) algorithm, originally developed for data compression, to produce one of the most commonly used tokenizers. Learning the tokenizer. Intuition: start with each character as its own token and combine tokens that co-occur a lot. Example: The output of learning is: Applying the tokenizer. To tokenize a new string, apply the merges in the same order: Unicode. Rather than just splitting by frequency, a more \u201cprincipled\u201d approach is to define an objective function that captures what a good tokenization looks like. We now describe theunigram model(Kudo 2018). Given a sequence \\(x_{1:L}\\), a tokenization \\(T\\) is a set of Example: Algorithm: Impact: Examples of tokenizations for both GPT-3 and Jurassic (demo):",
            "Thus far, we have defined language models as a probability distribution over sequences of tokens \\(p(x_1, \\dots, x_L)\\), which as we saw was very elegant and powerful (via prompting, a language model can in principle do anything, as GPT-3 hints at). In practice, however, it can be more efficient for specialized tasks to avoid having to generatively model the entire sequence. Contextual embeddings. As a prerequisite, the main key development is to associate a sequence of tokens with a corresponding sequence of contextual embeddings: We will broaden our notion of language models to three types of models. Encoder-only(BERT, RoBERTa, etc.). These language models produce contextual embeddings but cannot be used directly to generate text. These contextual embeddings are generally used for classification tasks (sometimes boldly called natural language understanding tasks). Decoder-only(GPT-2, GPT-3, etc.). These are our standard autoregressive language models, which given a prompt \\(x_{1:i}\\) produces both contextual embeddings and a distribution over next tokens \\(x_{i+1}\\) (and recursively, over the entire completion \\(x_{i+1:L}\\)). Encoder-decoder(BART, T5, etc.). These models in some ways can the best of both worlds: they can use bidirectional contextual embeddings for the input \\(x_{1:L}\\) and can generate the output \\(y_{1:L}\\). We now describe the innards of the embedding function \\(\\phi : \\sV^L \\to \\R^{d \\times L}\\): We now introduce the model architectures for language model, with an emphasis on the ubiquitous Transformer architecture. Our exposition of the Transformer architecture will be based on theseslides from CS221 on differentiable programming, and will depart a bit from the standard presentation. The beauty of deep learning is being able to create building blocks, just like we build whole programs out of functions. So we want to be able to functions like the following to encapsulate the complexity: This function will have parameters which we will include in the body but elide in the function signature for simplicity. In what follows, we will define a library of building blocks until we get to the full Transformer. First, we have to convert sequences of tokens into sequences of vectors. \\(\\EmbedToken\\) does exactly this by looking up each token in an embedding matrix \\(E \\in \\R^{|\\sV| \\times d}\\) (a parameter that will be learned from data): def \\(\\EmbedToken(\\x: \\sV^L) \\to \\R^{d \\times L}\\): These are exactly the (context-independent) word embeddings of yore. We define an abstract \\(\\SequenceModel\\) function that takes thesecontext-independent embeddingsand maps them intocontextual embeddings. def \\(\\SequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The simplest type of sequence model is based on feedforward networks (Bengio et al., 2003) applied to afixed lengthcontext, just as in an n-gram model: def \\(\\FeedForwardSequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The first \u201creal\u201d sequence model is a recurrent neural network (RNN), which is a family of models that include simple RNNs, LSTMs, and GRUs. The basic form of an RNN simply computes a sequence ofhidden statesrecursively. def \\(\\SequenceRNN(x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The actual module that does the hard work is the \\(\\RNN\\), which analogous to a finite state machine, takes the current state \\(h\\), a new observation \\(x\\), and returns the updated state: def \\(\\RNN(h: \\R^d, x: \\R^d) \\to \\R^d\\): There are three ways to implement the \\(\\RNN\\). The earliest RNN is a simple RNNElman, 1990, which takes a linear combination of \\(h\\) and \\(x\\) and pushes it through an elementwise non-linear function \\(\\sigma\\) (e.g., logistic \\(\\sigma(z) = (1 + e^{-z})^{-1}\\) or more the modern ReLU \\(\\sigma(z) = \\max(0, z)\\)). def \\(\\SimpleRNN(h: \\R^d, x: \\R^d) \\to \\R^d\\): As defined RNNs only depend on the past, but we can them depend on the future two by running another RNN backwards. These models were used byELMoandULMFiT. def \\(\\BidirectionalSequenceRNN(\\x: \\R^{d \\times L}) \\to \\R^{2d \\times L}\\): Notes: We will not discuss these models in the interest of time. Now, we will discuss Transformers (Vaswani et al. 2017), the sequence model that is really responsible for the takeoff of large language models; they are the building blocks of decoder-only (GPT-2, GPT-3), encoder-only (BERT, RoBERTa), and decoder-encoder (BART, T5) models. There are great resources for learning about the Transformer: You are highly encouraged to read these references. In this lecture, I will strive to take a middle path which emphasizes pseudocode functions and interfaces. The crux of the Transformers are theattention mechanism, which was developed earlier for machine translation (Bahdananu et al. 2017). One can think of attention as a \u201csoft\u201d lookup table, where we have a query \\(y\\) that we want to match against each element in a sequence \\(x_{1:L} = [x_1, \\dots, x_L]\\): We can think of each \\(x_i\\) as representing a key-value pair via linear transformations: and forming the query via another linear transformation: The key and the query can be compared to give a score: These scores can be exponentiated and normalized to form a probability distribution over the token positions \\(\\{ 1, \\dots, L \\}\\): Then the final output is a weighted combination over the values: We can write this all succinctly in matrix form: def \\(\\Attention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d\\): We can think of there as being multiple aspects (e.g., syntax, semantics) that we would want to match on. To accommodate this, we can simultaneously have multipleattention headsand simply combine their outputs. def \\(\\MultiHeadedAttention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d:\\) Self-attention layer. Now we will substitute each \\(x_i\\) in for \\(y\\) as the query argument to produce: def \\(\\SelfAttention(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L})\\): Feedforward layer. Self-attention allows all the tokens to \u201ctalk\u201d to each other, whereas feedforward connections provide: def \\(\\FeedForward(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Improving trainability. We\u2019re almost done. We could in principle just take the \\(\\FeedForward \\circ \\SelfAttention\\) sequence model and iterate it 96 times to make GPT-3, but that network would be hard to optimize (for the same vanishing gradients problems that afflicted RNNs, now just along the depth direction). So we have to do two shenanigans to make sure that the network is trainable. Residual connections. One trick from computer vision is residual connections (ResNet). Instead of applying some function \\(f\\): we add a residual (skip) connection so that if \\(f\\)\u2019s gradients vanish, gradients can still flow through \\(\\x\\): Layer normalization. Another trick islayer normalization, which takes a takes a vector and makes sure its elements are too big: def \\(\\LayerNorm(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): We first define an adapter function that takes a sequence model \\(f\\) and makes it \u201crobust\u201d: def \\(\\AddNorm(f: (\\R^{d \\times L} \\to \\R^{d \\times L}), \\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Finally, we can define the Transformer block succinctly as follows: def \\(\\TransformerBlock(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Positional embeddings. You might have noticed that as defined, the embedding of a token doesn\u2019t depend on where it occurs in the sequence, so \\(\\nl{mouse}\\) in both sentences would have the same embedding, which is not sensible. To fix this, we addpositional informationinto the embedding: def \\(\\EmbedTokenWithPosition(\\x: \\R^{d \\times L})\\): GPT-3. With all the pieces in place, we can now define roughly GPT-3 architecture in one line, just by stacking the Transformer block 96 times: Shape of the architecture (how the 175 billion parameters are allocated): These decisions are not necessarily optimal.Levine et al. 2020provide some theoretical justification, showing that the GPT-3 is too deep, which motivated the training of a deeper but wider Jurassic architecture. There are important but detailed differences between different versions of Transformers:",
            "Tokenization: Modeling: Decoder-only architectures: Encoder-only architectures: Encoder-decoder architectures:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/modeling/#types-of-language-models": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/#types-of-language-models": ""
        },
        "text_between_h2": [
            "Recall that a language model \\(p\\) is a probability distribution over asequence of tokenswhere each token comes from some vocabulary \\(\\sV\\): However, natural language doesn\u2019t come as a sequence of tokens, but as just a string (concretely, sequence of Unicode characters): Atokenizerconverts any string into a sequence of tokens. This is not necessarily the most glamorous part of language modeling, but plays a really important role in determining how well a model will work. The simplest solution is to do: Therefore, splitting by spaces by spaces to identify words is quite problematic. What makes a good tokenization? Sennrich et al, 2015applied thebyte pair encoding(BPE) algorithm, originally developed for data compression, to produce one of the most commonly used tokenizers. Learning the tokenizer. Intuition: start with each character as its own token and combine tokens that co-occur a lot. Example: The output of learning is: Applying the tokenizer. To tokenize a new string, apply the merges in the same order: Unicode. Rather than just splitting by frequency, a more \u201cprincipled\u201d approach is to define an objective function that captures what a good tokenization looks like. We now describe theunigram model(Kudo 2018). Given a sequence \\(x_{1:L}\\), a tokenization \\(T\\) is a set of Example: Algorithm: Impact: Examples of tokenizations for both GPT-3 and Jurassic (demo):",
            "Thus far, we have defined language models as a probability distribution over sequences of tokens \\(p(x_1, \\dots, x_L)\\), which as we saw was very elegant and powerful (via prompting, a language model can in principle do anything, as GPT-3 hints at). In practice, however, it can be more efficient for specialized tasks to avoid having to generatively model the entire sequence. Contextual embeddings. As a prerequisite, the main key development is to associate a sequence of tokens with a corresponding sequence of contextual embeddings: We will broaden our notion of language models to three types of models. Encoder-only(BERT, RoBERTa, etc.). These language models produce contextual embeddings but cannot be used directly to generate text. These contextual embeddings are generally used for classification tasks (sometimes boldly called natural language understanding tasks). Decoder-only(GPT-2, GPT-3, etc.). These are our standard autoregressive language models, which given a prompt \\(x_{1:i}\\) produces both contextual embeddings and a distribution over next tokens \\(x_{i+1}\\) (and recursively, over the entire completion \\(x_{i+1:L}\\)). Encoder-decoder(BART, T5, etc.). These models in some ways can the best of both worlds: they can use bidirectional contextual embeddings for the input \\(x_{1:L}\\) and can generate the output \\(y_{1:L}\\). We now describe the innards of the embedding function \\(\\phi : \\sV^L \\to \\R^{d \\times L}\\): We now introduce the model architectures for language model, with an emphasis on the ubiquitous Transformer architecture. Our exposition of the Transformer architecture will be based on theseslides from CS221 on differentiable programming, and will depart a bit from the standard presentation. The beauty of deep learning is being able to create building blocks, just like we build whole programs out of functions. So we want to be able to functions like the following to encapsulate the complexity: This function will have parameters which we will include in the body but elide in the function signature for simplicity. In what follows, we will define a library of building blocks until we get to the full Transformer. First, we have to convert sequences of tokens into sequences of vectors. \\(\\EmbedToken\\) does exactly this by looking up each token in an embedding matrix \\(E \\in \\R^{|\\sV| \\times d}\\) (a parameter that will be learned from data): def \\(\\EmbedToken(\\x: \\sV^L) \\to \\R^{d \\times L}\\): These are exactly the (context-independent) word embeddings of yore. We define an abstract \\(\\SequenceModel\\) function that takes thesecontext-independent embeddingsand maps them intocontextual embeddings. def \\(\\SequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The simplest type of sequence model is based on feedforward networks (Bengio et al., 2003) applied to afixed lengthcontext, just as in an n-gram model: def \\(\\FeedForwardSequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The first \u201creal\u201d sequence model is a recurrent neural network (RNN), which is a family of models that include simple RNNs, LSTMs, and GRUs. The basic form of an RNN simply computes a sequence ofhidden statesrecursively. def \\(\\SequenceRNN(x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The actual module that does the hard work is the \\(\\RNN\\), which analogous to a finite state machine, takes the current state \\(h\\), a new observation \\(x\\), and returns the updated state: def \\(\\RNN(h: \\R^d, x: \\R^d) \\to \\R^d\\): There are three ways to implement the \\(\\RNN\\). The earliest RNN is a simple RNNElman, 1990, which takes a linear combination of \\(h\\) and \\(x\\) and pushes it through an elementwise non-linear function \\(\\sigma\\) (e.g., logistic \\(\\sigma(z) = (1 + e^{-z})^{-1}\\) or more the modern ReLU \\(\\sigma(z) = \\max(0, z)\\)). def \\(\\SimpleRNN(h: \\R^d, x: \\R^d) \\to \\R^d\\): As defined RNNs only depend on the past, but we can them depend on the future two by running another RNN backwards. These models were used byELMoandULMFiT. def \\(\\BidirectionalSequenceRNN(\\x: \\R^{d \\times L}) \\to \\R^{2d \\times L}\\): Notes: We will not discuss these models in the interest of time. Now, we will discuss Transformers (Vaswani et al. 2017), the sequence model that is really responsible for the takeoff of large language models; they are the building blocks of decoder-only (GPT-2, GPT-3), encoder-only (BERT, RoBERTa), and decoder-encoder (BART, T5) models. There are great resources for learning about the Transformer: You are highly encouraged to read these references. In this lecture, I will strive to take a middle path which emphasizes pseudocode functions and interfaces. The crux of the Transformers are theattention mechanism, which was developed earlier for machine translation (Bahdananu et al. 2017). One can think of attention as a \u201csoft\u201d lookup table, where we have a query \\(y\\) that we want to match against each element in a sequence \\(x_{1:L} = [x_1, \\dots, x_L]\\): We can think of each \\(x_i\\) as representing a key-value pair via linear transformations: and forming the query via another linear transformation: The key and the query can be compared to give a score: These scores can be exponentiated and normalized to form a probability distribution over the token positions \\(\\{ 1, \\dots, L \\}\\): Then the final output is a weighted combination over the values: We can write this all succinctly in matrix form: def \\(\\Attention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d\\): We can think of there as being multiple aspects (e.g., syntax, semantics) that we would want to match on. To accommodate this, we can simultaneously have multipleattention headsand simply combine their outputs. def \\(\\MultiHeadedAttention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d:\\) Self-attention layer. Now we will substitute each \\(x_i\\) in for \\(y\\) as the query argument to produce: def \\(\\SelfAttention(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L})\\): Feedforward layer. Self-attention allows all the tokens to \u201ctalk\u201d to each other, whereas feedforward connections provide: def \\(\\FeedForward(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Improving trainability. We\u2019re almost done. We could in principle just take the \\(\\FeedForward \\circ \\SelfAttention\\) sequence model and iterate it 96 times to make GPT-3, but that network would be hard to optimize (for the same vanishing gradients problems that afflicted RNNs, now just along the depth direction). So we have to do two shenanigans to make sure that the network is trainable. Residual connections. One trick from computer vision is residual connections (ResNet). Instead of applying some function \\(f\\): we add a residual (skip) connection so that if \\(f\\)\u2019s gradients vanish, gradients can still flow through \\(\\x\\): Layer normalization. Another trick islayer normalization, which takes a takes a vector and makes sure its elements are too big: def \\(\\LayerNorm(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): We first define an adapter function that takes a sequence model \\(f\\) and makes it \u201crobust\u201d: def \\(\\AddNorm(f: (\\R^{d \\times L} \\to \\R^{d \\times L}), \\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Finally, we can define the Transformer block succinctly as follows: def \\(\\TransformerBlock(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Positional embeddings. You might have noticed that as defined, the embedding of a token doesn\u2019t depend on where it occurs in the sequence, so \\(\\nl{mouse}\\) in both sentences would have the same embedding, which is not sensible. To fix this, we addpositional informationinto the embedding: def \\(\\EmbedTokenWithPosition(\\x: \\R^{d \\times L})\\): GPT-3. With all the pieces in place, we can now define roughly GPT-3 architecture in one line, just by stacking the Transformer block 96 times: Shape of the architecture (how the 175 billion parameters are allocated): These decisions are not necessarily optimal.Levine et al. 2020provide some theoretical justification, showing that the GPT-3 is too deep, which motivated the training of a deeper but wider Jurassic architecture. There are important but detailed differences between different versions of Transformers:",
            "Tokenization: Modeling: Decoder-only architectures: Encoder-only architectures: Encoder-decoder architectures:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/modeling/#preliminaries": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/#preliminaries": "",
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/#preliminaries-1": ""
        },
        "text_between_h2": [
            "Recall that a language model \\(p\\) is a probability distribution over asequence of tokenswhere each token comes from some vocabulary \\(\\sV\\): However, natural language doesn\u2019t come as a sequence of tokens, but as just a string (concretely, sequence of Unicode characters): Atokenizerconverts any string into a sequence of tokens. This is not necessarily the most glamorous part of language modeling, but plays a really important role in determining how well a model will work. The simplest solution is to do: Therefore, splitting by spaces by spaces to identify words is quite problematic. What makes a good tokenization? Sennrich et al, 2015applied thebyte pair encoding(BPE) algorithm, originally developed for data compression, to produce one of the most commonly used tokenizers. Learning the tokenizer. Intuition: start with each character as its own token and combine tokens that co-occur a lot. Example: The output of learning is: Applying the tokenizer. To tokenize a new string, apply the merges in the same order: Unicode. Rather than just splitting by frequency, a more \u201cprincipled\u201d approach is to define an objective function that captures what a good tokenization looks like. We now describe theunigram model(Kudo 2018). Given a sequence \\(x_{1:L}\\), a tokenization \\(T\\) is a set of Example: Algorithm: Impact: Examples of tokenizations for both GPT-3 and Jurassic (demo):",
            "Thus far, we have defined language models as a probability distribution over sequences of tokens \\(p(x_1, \\dots, x_L)\\), which as we saw was very elegant and powerful (via prompting, a language model can in principle do anything, as GPT-3 hints at). In practice, however, it can be more efficient for specialized tasks to avoid having to generatively model the entire sequence. Contextual embeddings. As a prerequisite, the main key development is to associate a sequence of tokens with a corresponding sequence of contextual embeddings: We will broaden our notion of language models to three types of models. Encoder-only(BERT, RoBERTa, etc.). These language models produce contextual embeddings but cannot be used directly to generate text. These contextual embeddings are generally used for classification tasks (sometimes boldly called natural language understanding tasks). Decoder-only(GPT-2, GPT-3, etc.). These are our standard autoregressive language models, which given a prompt \\(x_{1:i}\\) produces both contextual embeddings and a distribution over next tokens \\(x_{i+1}\\) (and recursively, over the entire completion \\(x_{i+1:L}\\)). Encoder-decoder(BART, T5, etc.). These models in some ways can the best of both worlds: they can use bidirectional contextual embeddings for the input \\(x_{1:L}\\) and can generate the output \\(y_{1:L}\\). We now describe the innards of the embedding function \\(\\phi : \\sV^L \\to \\R^{d \\times L}\\): We now introduce the model architectures for language model, with an emphasis on the ubiquitous Transformer architecture. Our exposition of the Transformer architecture will be based on theseslides from CS221 on differentiable programming, and will depart a bit from the standard presentation. The beauty of deep learning is being able to create building blocks, just like we build whole programs out of functions. So we want to be able to functions like the following to encapsulate the complexity: This function will have parameters which we will include in the body but elide in the function signature for simplicity. In what follows, we will define a library of building blocks until we get to the full Transformer. First, we have to convert sequences of tokens into sequences of vectors. \\(\\EmbedToken\\) does exactly this by looking up each token in an embedding matrix \\(E \\in \\R^{|\\sV| \\times d}\\) (a parameter that will be learned from data): def \\(\\EmbedToken(\\x: \\sV^L) \\to \\R^{d \\times L}\\): These are exactly the (context-independent) word embeddings of yore. We define an abstract \\(\\SequenceModel\\) function that takes thesecontext-independent embeddingsand maps them intocontextual embeddings. def \\(\\SequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The simplest type of sequence model is based on feedforward networks (Bengio et al., 2003) applied to afixed lengthcontext, just as in an n-gram model: def \\(\\FeedForwardSequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The first \u201creal\u201d sequence model is a recurrent neural network (RNN), which is a family of models that include simple RNNs, LSTMs, and GRUs. The basic form of an RNN simply computes a sequence ofhidden statesrecursively. def \\(\\SequenceRNN(x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The actual module that does the hard work is the \\(\\RNN\\), which analogous to a finite state machine, takes the current state \\(h\\), a new observation \\(x\\), and returns the updated state: def \\(\\RNN(h: \\R^d, x: \\R^d) \\to \\R^d\\): There are three ways to implement the \\(\\RNN\\). The earliest RNN is a simple RNNElman, 1990, which takes a linear combination of \\(h\\) and \\(x\\) and pushes it through an elementwise non-linear function \\(\\sigma\\) (e.g., logistic \\(\\sigma(z) = (1 + e^{-z})^{-1}\\) or more the modern ReLU \\(\\sigma(z) = \\max(0, z)\\)). def \\(\\SimpleRNN(h: \\R^d, x: \\R^d) \\to \\R^d\\): As defined RNNs only depend on the past, but we can them depend on the future two by running another RNN backwards. These models were used byELMoandULMFiT. def \\(\\BidirectionalSequenceRNN(\\x: \\R^{d \\times L}) \\to \\R^{2d \\times L}\\): Notes: We will not discuss these models in the interest of time. Now, we will discuss Transformers (Vaswani et al. 2017), the sequence model that is really responsible for the takeoff of large language models; they are the building blocks of decoder-only (GPT-2, GPT-3), encoder-only (BERT, RoBERTa), and decoder-encoder (BART, T5) models. There are great resources for learning about the Transformer: You are highly encouraged to read these references. In this lecture, I will strive to take a middle path which emphasizes pseudocode functions and interfaces. The crux of the Transformers are theattention mechanism, which was developed earlier for machine translation (Bahdananu et al. 2017). One can think of attention as a \u201csoft\u201d lookup table, where we have a query \\(y\\) that we want to match against each element in a sequence \\(x_{1:L} = [x_1, \\dots, x_L]\\): We can think of each \\(x_i\\) as representing a key-value pair via linear transformations: and forming the query via another linear transformation: The key and the query can be compared to give a score: These scores can be exponentiated and normalized to form a probability distribution over the token positions \\(\\{ 1, \\dots, L \\}\\): Then the final output is a weighted combination over the values: We can write this all succinctly in matrix form: def \\(\\Attention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d\\): We can think of there as being multiple aspects (e.g., syntax, semantics) that we would want to match on. To accommodate this, we can simultaneously have multipleattention headsand simply combine their outputs. def \\(\\MultiHeadedAttention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d:\\) Self-attention layer. Now we will substitute each \\(x_i\\) in for \\(y\\) as the query argument to produce: def \\(\\SelfAttention(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L})\\): Feedforward layer. Self-attention allows all the tokens to \u201ctalk\u201d to each other, whereas feedforward connections provide: def \\(\\FeedForward(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Improving trainability. We\u2019re almost done. We could in principle just take the \\(\\FeedForward \\circ \\SelfAttention\\) sequence model and iterate it 96 times to make GPT-3, but that network would be hard to optimize (for the same vanishing gradients problems that afflicted RNNs, now just along the depth direction). So we have to do two shenanigans to make sure that the network is trainable. Residual connections. One trick from computer vision is residual connections (ResNet). Instead of applying some function \\(f\\): we add a residual (skip) connection so that if \\(f\\)\u2019s gradients vanish, gradients can still flow through \\(\\x\\): Layer normalization. Another trick islayer normalization, which takes a takes a vector and makes sure its elements are too big: def \\(\\LayerNorm(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): We first define an adapter function that takes a sequence model \\(f\\) and makes it \u201crobust\u201d: def \\(\\AddNorm(f: (\\R^{d \\times L} \\to \\R^{d \\times L}), \\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Finally, we can define the Transformer block succinctly as follows: def \\(\\TransformerBlock(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Positional embeddings. You might have noticed that as defined, the embedding of a token doesn\u2019t depend on where it occurs in the sequence, so \\(\\nl{mouse}\\) in both sentences would have the same embedding, which is not sensible. To fix this, we addpositional informationinto the embedding: def \\(\\EmbedTokenWithPosition(\\x: \\R^{d \\times L})\\): GPT-3. With all the pieces in place, we can now define roughly GPT-3 architecture in one line, just by stacking the Transformer block 96 times: Shape of the architecture (how the 175 billion parameters are allocated): These decisions are not necessarily optimal.Levine et al. 2020provide some theoretical justification, showing that the GPT-3 is too deep, which motivated the training of a deeper but wider Jurassic architecture. There are important but detailed differences between different versions of Transformers:",
            "Tokenization: Modeling: Decoder-only architectures: Encoder-only architectures: Encoder-decoder architectures:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/modeling/#preliminaries-1": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/#preliminaries-1": ""
        },
        "text_between_h2": [
            "Recall that a language model \\(p\\) is a probability distribution over asequence of tokenswhere each token comes from some vocabulary \\(\\sV\\): However, natural language doesn\u2019t come as a sequence of tokens, but as just a string (concretely, sequence of Unicode characters): Atokenizerconverts any string into a sequence of tokens. This is not necessarily the most glamorous part of language modeling, but plays a really important role in determining how well a model will work. The simplest solution is to do: Therefore, splitting by spaces by spaces to identify words is quite problematic. What makes a good tokenization? Sennrich et al, 2015applied thebyte pair encoding(BPE) algorithm, originally developed for data compression, to produce one of the most commonly used tokenizers. Learning the tokenizer. Intuition: start with each character as its own token and combine tokens that co-occur a lot. Example: The output of learning is: Applying the tokenizer. To tokenize a new string, apply the merges in the same order: Unicode. Rather than just splitting by frequency, a more \u201cprincipled\u201d approach is to define an objective function that captures what a good tokenization looks like. We now describe theunigram model(Kudo 2018). Given a sequence \\(x_{1:L}\\), a tokenization \\(T\\) is a set of Example: Algorithm: Impact: Examples of tokenizations for both GPT-3 and Jurassic (demo):",
            "Thus far, we have defined language models as a probability distribution over sequences of tokens \\(p(x_1, \\dots, x_L)\\), which as we saw was very elegant and powerful (via prompting, a language model can in principle do anything, as GPT-3 hints at). In practice, however, it can be more efficient for specialized tasks to avoid having to generatively model the entire sequence. Contextual embeddings. As a prerequisite, the main key development is to associate a sequence of tokens with a corresponding sequence of contextual embeddings: We will broaden our notion of language models to three types of models. Encoder-only(BERT, RoBERTa, etc.). These language models produce contextual embeddings but cannot be used directly to generate text. These contextual embeddings are generally used for classification tasks (sometimes boldly called natural language understanding tasks). Decoder-only(GPT-2, GPT-3, etc.). These are our standard autoregressive language models, which given a prompt \\(x_{1:i}\\) produces both contextual embeddings and a distribution over next tokens \\(x_{i+1}\\) (and recursively, over the entire completion \\(x_{i+1:L}\\)). Encoder-decoder(BART, T5, etc.). These models in some ways can the best of both worlds: they can use bidirectional contextual embeddings for the input \\(x_{1:L}\\) and can generate the output \\(y_{1:L}\\). We now describe the innards of the embedding function \\(\\phi : \\sV^L \\to \\R^{d \\times L}\\): We now introduce the model architectures for language model, with an emphasis on the ubiquitous Transformer architecture. Our exposition of the Transformer architecture will be based on theseslides from CS221 on differentiable programming, and will depart a bit from the standard presentation. The beauty of deep learning is being able to create building blocks, just like we build whole programs out of functions. So we want to be able to functions like the following to encapsulate the complexity: This function will have parameters which we will include in the body but elide in the function signature for simplicity. In what follows, we will define a library of building blocks until we get to the full Transformer. First, we have to convert sequences of tokens into sequences of vectors. \\(\\EmbedToken\\) does exactly this by looking up each token in an embedding matrix \\(E \\in \\R^{|\\sV| \\times d}\\) (a parameter that will be learned from data): def \\(\\EmbedToken(\\x: \\sV^L) \\to \\R^{d \\times L}\\): These are exactly the (context-independent) word embeddings of yore. We define an abstract \\(\\SequenceModel\\) function that takes thesecontext-independent embeddingsand maps them intocontextual embeddings. def \\(\\SequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The simplest type of sequence model is based on feedforward networks (Bengio et al., 2003) applied to afixed lengthcontext, just as in an n-gram model: def \\(\\FeedForwardSequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The first \u201creal\u201d sequence model is a recurrent neural network (RNN), which is a family of models that include simple RNNs, LSTMs, and GRUs. The basic form of an RNN simply computes a sequence ofhidden statesrecursively. def \\(\\SequenceRNN(x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The actual module that does the hard work is the \\(\\RNN\\), which analogous to a finite state machine, takes the current state \\(h\\), a new observation \\(x\\), and returns the updated state: def \\(\\RNN(h: \\R^d, x: \\R^d) \\to \\R^d\\): There are three ways to implement the \\(\\RNN\\). The earliest RNN is a simple RNNElman, 1990, which takes a linear combination of \\(h\\) and \\(x\\) and pushes it through an elementwise non-linear function \\(\\sigma\\) (e.g., logistic \\(\\sigma(z) = (1 + e^{-z})^{-1}\\) or more the modern ReLU \\(\\sigma(z) = \\max(0, z)\\)). def \\(\\SimpleRNN(h: \\R^d, x: \\R^d) \\to \\R^d\\): As defined RNNs only depend on the past, but we can them depend on the future two by running another RNN backwards. These models were used byELMoandULMFiT. def \\(\\BidirectionalSequenceRNN(\\x: \\R^{d \\times L}) \\to \\R^{2d \\times L}\\): Notes: We will not discuss these models in the interest of time. Now, we will discuss Transformers (Vaswani et al. 2017), the sequence model that is really responsible for the takeoff of large language models; they are the building blocks of decoder-only (GPT-2, GPT-3), encoder-only (BERT, RoBERTa), and decoder-encoder (BART, T5) models. There are great resources for learning about the Transformer: You are highly encouraged to read these references. In this lecture, I will strive to take a middle path which emphasizes pseudocode functions and interfaces. The crux of the Transformers are theattention mechanism, which was developed earlier for machine translation (Bahdananu et al. 2017). One can think of attention as a \u201csoft\u201d lookup table, where we have a query \\(y\\) that we want to match against each element in a sequence \\(x_{1:L} = [x_1, \\dots, x_L]\\): We can think of each \\(x_i\\) as representing a key-value pair via linear transformations: and forming the query via another linear transformation: The key and the query can be compared to give a score: These scores can be exponentiated and normalized to form a probability distribution over the token positions \\(\\{ 1, \\dots, L \\}\\): Then the final output is a weighted combination over the values: We can write this all succinctly in matrix form: def \\(\\Attention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d\\): We can think of there as being multiple aspects (e.g., syntax, semantics) that we would want to match on. To accommodate this, we can simultaneously have multipleattention headsand simply combine their outputs. def \\(\\MultiHeadedAttention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d:\\) Self-attention layer. Now we will substitute each \\(x_i\\) in for \\(y\\) as the query argument to produce: def \\(\\SelfAttention(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L})\\): Feedforward layer. Self-attention allows all the tokens to \u201ctalk\u201d to each other, whereas feedforward connections provide: def \\(\\FeedForward(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Improving trainability. We\u2019re almost done. We could in principle just take the \\(\\FeedForward \\circ \\SelfAttention\\) sequence model and iterate it 96 times to make GPT-3, but that network would be hard to optimize (for the same vanishing gradients problems that afflicted RNNs, now just along the depth direction). So we have to do two shenanigans to make sure that the network is trainable. Residual connections. One trick from computer vision is residual connections (ResNet). Instead of applying some function \\(f\\): we add a residual (skip) connection so that if \\(f\\)\u2019s gradients vanish, gradients can still flow through \\(\\x\\): Layer normalization. Another trick islayer normalization, which takes a takes a vector and makes sure its elements are too big: def \\(\\LayerNorm(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): We first define an adapter function that takes a sequence model \\(f\\) and makes it \u201crobust\u201d: def \\(\\AddNorm(f: (\\R^{d \\times L} \\to \\R^{d \\times L}), \\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Finally, we can define the Transformer block succinctly as follows: def \\(\\TransformerBlock(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Positional embeddings. You might have noticed that as defined, the embedding of a token doesn\u2019t depend on where it occurs in the sequence, so \\(\\nl{mouse}\\) in both sentences would have the same embedding, which is not sensible. To fix this, we addpositional informationinto the embedding: def \\(\\EmbedTokenWithPosition(\\x: \\R^{d \\times L})\\): GPT-3. With all the pieces in place, we can now define roughly GPT-3 architecture in one line, just by stacking the Transformer block 96 times: Shape of the architecture (how the 175 billion parameters are allocated): These decisions are not necessarily optimal.Levine et al. 2020provide some theoretical justification, showing that the GPT-3 is too deep, which motivated the training of a deeper but wider Jurassic architecture. There are important but detailed differences between different versions of Transformers:",
            "Tokenization: Modeling: Decoder-only architectures: Encoder-only architectures: Encoder-decoder architectures:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/modeling/#recurrent-neural-networks": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/#recurrent-neural-networks": ""
        },
        "text_between_h2": [
            "Recall that a language model \\(p\\) is a probability distribution over asequence of tokenswhere each token comes from some vocabulary \\(\\sV\\): However, natural language doesn\u2019t come as a sequence of tokens, but as just a string (concretely, sequence of Unicode characters): Atokenizerconverts any string into a sequence of tokens. This is not necessarily the most glamorous part of language modeling, but plays a really important role in determining how well a model will work. The simplest solution is to do: Therefore, splitting by spaces by spaces to identify words is quite problematic. What makes a good tokenization? Sennrich et al, 2015applied thebyte pair encoding(BPE) algorithm, originally developed for data compression, to produce one of the most commonly used tokenizers. Learning the tokenizer. Intuition: start with each character as its own token and combine tokens that co-occur a lot. Example: The output of learning is: Applying the tokenizer. To tokenize a new string, apply the merges in the same order: Unicode. Rather than just splitting by frequency, a more \u201cprincipled\u201d approach is to define an objective function that captures what a good tokenization looks like. We now describe theunigram model(Kudo 2018). Given a sequence \\(x_{1:L}\\), a tokenization \\(T\\) is a set of Example: Algorithm: Impact: Examples of tokenizations for both GPT-3 and Jurassic (demo):",
            "Thus far, we have defined language models as a probability distribution over sequences of tokens \\(p(x_1, \\dots, x_L)\\), which as we saw was very elegant and powerful (via prompting, a language model can in principle do anything, as GPT-3 hints at). In practice, however, it can be more efficient for specialized tasks to avoid having to generatively model the entire sequence. Contextual embeddings. As a prerequisite, the main key development is to associate a sequence of tokens with a corresponding sequence of contextual embeddings: We will broaden our notion of language models to three types of models. Encoder-only(BERT, RoBERTa, etc.). These language models produce contextual embeddings but cannot be used directly to generate text. These contextual embeddings are generally used for classification tasks (sometimes boldly called natural language understanding tasks). Decoder-only(GPT-2, GPT-3, etc.). These are our standard autoregressive language models, which given a prompt \\(x_{1:i}\\) produces both contextual embeddings and a distribution over next tokens \\(x_{i+1}\\) (and recursively, over the entire completion \\(x_{i+1:L}\\)). Encoder-decoder(BART, T5, etc.). These models in some ways can the best of both worlds: they can use bidirectional contextual embeddings for the input \\(x_{1:L}\\) and can generate the output \\(y_{1:L}\\). We now describe the innards of the embedding function \\(\\phi : \\sV^L \\to \\R^{d \\times L}\\): We now introduce the model architectures for language model, with an emphasis on the ubiquitous Transformer architecture. Our exposition of the Transformer architecture will be based on theseslides from CS221 on differentiable programming, and will depart a bit from the standard presentation. The beauty of deep learning is being able to create building blocks, just like we build whole programs out of functions. So we want to be able to functions like the following to encapsulate the complexity: This function will have parameters which we will include in the body but elide in the function signature for simplicity. In what follows, we will define a library of building blocks until we get to the full Transformer. First, we have to convert sequences of tokens into sequences of vectors. \\(\\EmbedToken\\) does exactly this by looking up each token in an embedding matrix \\(E \\in \\R^{|\\sV| \\times d}\\) (a parameter that will be learned from data): def \\(\\EmbedToken(\\x: \\sV^L) \\to \\R^{d \\times L}\\): These are exactly the (context-independent) word embeddings of yore. We define an abstract \\(\\SequenceModel\\) function that takes thesecontext-independent embeddingsand maps them intocontextual embeddings. def \\(\\SequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The simplest type of sequence model is based on feedforward networks (Bengio et al., 2003) applied to afixed lengthcontext, just as in an n-gram model: def \\(\\FeedForwardSequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The first \u201creal\u201d sequence model is a recurrent neural network (RNN), which is a family of models that include simple RNNs, LSTMs, and GRUs. The basic form of an RNN simply computes a sequence ofhidden statesrecursively. def \\(\\SequenceRNN(x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The actual module that does the hard work is the \\(\\RNN\\), which analogous to a finite state machine, takes the current state \\(h\\), a new observation \\(x\\), and returns the updated state: def \\(\\RNN(h: \\R^d, x: \\R^d) \\to \\R^d\\): There are three ways to implement the \\(\\RNN\\). The earliest RNN is a simple RNNElman, 1990, which takes a linear combination of \\(h\\) and \\(x\\) and pushes it through an elementwise non-linear function \\(\\sigma\\) (e.g., logistic \\(\\sigma(z) = (1 + e^{-z})^{-1}\\) or more the modern ReLU \\(\\sigma(z) = \\max(0, z)\\)). def \\(\\SimpleRNN(h: \\R^d, x: \\R^d) \\to \\R^d\\): As defined RNNs only depend on the past, but we can them depend on the future two by running another RNN backwards. These models were used byELMoandULMFiT. def \\(\\BidirectionalSequenceRNN(\\x: \\R^{d \\times L}) \\to \\R^{2d \\times L}\\): Notes: We will not discuss these models in the interest of time. Now, we will discuss Transformers (Vaswani et al. 2017), the sequence model that is really responsible for the takeoff of large language models; they are the building blocks of decoder-only (GPT-2, GPT-3), encoder-only (BERT, RoBERTa), and decoder-encoder (BART, T5) models. There are great resources for learning about the Transformer: You are highly encouraged to read these references. In this lecture, I will strive to take a middle path which emphasizes pseudocode functions and interfaces. The crux of the Transformers are theattention mechanism, which was developed earlier for machine translation (Bahdananu et al. 2017). One can think of attention as a \u201csoft\u201d lookup table, where we have a query \\(y\\) that we want to match against each element in a sequence \\(x_{1:L} = [x_1, \\dots, x_L]\\): We can think of each \\(x_i\\) as representing a key-value pair via linear transformations: and forming the query via another linear transformation: The key and the query can be compared to give a score: These scores can be exponentiated and normalized to form a probability distribution over the token positions \\(\\{ 1, \\dots, L \\}\\): Then the final output is a weighted combination over the values: We can write this all succinctly in matrix form: def \\(\\Attention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d\\): We can think of there as being multiple aspects (e.g., syntax, semantics) that we would want to match on. To accommodate this, we can simultaneously have multipleattention headsand simply combine their outputs. def \\(\\MultiHeadedAttention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d:\\) Self-attention layer. Now we will substitute each \\(x_i\\) in for \\(y\\) as the query argument to produce: def \\(\\SelfAttention(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L})\\): Feedforward layer. Self-attention allows all the tokens to \u201ctalk\u201d to each other, whereas feedforward connections provide: def \\(\\FeedForward(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Improving trainability. We\u2019re almost done. We could in principle just take the \\(\\FeedForward \\circ \\SelfAttention\\) sequence model and iterate it 96 times to make GPT-3, but that network would be hard to optimize (for the same vanishing gradients problems that afflicted RNNs, now just along the depth direction). So we have to do two shenanigans to make sure that the network is trainable. Residual connections. One trick from computer vision is residual connections (ResNet). Instead of applying some function \\(f\\): we add a residual (skip) connection so that if \\(f\\)\u2019s gradients vanish, gradients can still flow through \\(\\x\\): Layer normalization. Another trick islayer normalization, which takes a takes a vector and makes sure its elements are too big: def \\(\\LayerNorm(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): We first define an adapter function that takes a sequence model \\(f\\) and makes it \u201crobust\u201d: def \\(\\AddNorm(f: (\\R^{d \\times L} \\to \\R^{d \\times L}), \\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Finally, we can define the Transformer block succinctly as follows: def \\(\\TransformerBlock(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Positional embeddings. You might have noticed that as defined, the embedding of a token doesn\u2019t depend on where it occurs in the sequence, so \\(\\nl{mouse}\\) in both sentences would have the same embedding, which is not sensible. To fix this, we addpositional informationinto the embedding: def \\(\\EmbedTokenWithPosition(\\x: \\R^{d \\times L})\\): GPT-3. With all the pieces in place, we can now define roughly GPT-3 architecture in one line, just by stacking the Transformer block 96 times: Shape of the architecture (how the 175 billion parameters are allocated): These decisions are not necessarily optimal.Levine et al. 2020provide some theoretical justification, showing that the GPT-3 is too deep, which motivated the training of a deeper but wider Jurassic architecture. There are important but detailed differences between different versions of Transformers:",
            "Tokenization: Modeling: Decoder-only architectures: Encoder-only architectures: Encoder-decoder architectures:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/modeling/#transformers": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/#transformers": ""
        },
        "text_between_h2": [
            "Recall that a language model \\(p\\) is a probability distribution over asequence of tokenswhere each token comes from some vocabulary \\(\\sV\\): However, natural language doesn\u2019t come as a sequence of tokens, but as just a string (concretely, sequence of Unicode characters): Atokenizerconverts any string into a sequence of tokens. This is not necessarily the most glamorous part of language modeling, but plays a really important role in determining how well a model will work. The simplest solution is to do: Therefore, splitting by spaces by spaces to identify words is quite problematic. What makes a good tokenization? Sennrich et al, 2015applied thebyte pair encoding(BPE) algorithm, originally developed for data compression, to produce one of the most commonly used tokenizers. Learning the tokenizer. Intuition: start with each character as its own token and combine tokens that co-occur a lot. Example: The output of learning is: Applying the tokenizer. To tokenize a new string, apply the merges in the same order: Unicode. Rather than just splitting by frequency, a more \u201cprincipled\u201d approach is to define an objective function that captures what a good tokenization looks like. We now describe theunigram model(Kudo 2018). Given a sequence \\(x_{1:L}\\), a tokenization \\(T\\) is a set of Example: Algorithm: Impact: Examples of tokenizations for both GPT-3 and Jurassic (demo):",
            "Thus far, we have defined language models as a probability distribution over sequences of tokens \\(p(x_1, \\dots, x_L)\\), which as we saw was very elegant and powerful (via prompting, a language model can in principle do anything, as GPT-3 hints at). In practice, however, it can be more efficient for specialized tasks to avoid having to generatively model the entire sequence. Contextual embeddings. As a prerequisite, the main key development is to associate a sequence of tokens with a corresponding sequence of contextual embeddings: We will broaden our notion of language models to three types of models. Encoder-only(BERT, RoBERTa, etc.). These language models produce contextual embeddings but cannot be used directly to generate text. These contextual embeddings are generally used for classification tasks (sometimes boldly called natural language understanding tasks). Decoder-only(GPT-2, GPT-3, etc.). These are our standard autoregressive language models, which given a prompt \\(x_{1:i}\\) produces both contextual embeddings and a distribution over next tokens \\(x_{i+1}\\) (and recursively, over the entire completion \\(x_{i+1:L}\\)). Encoder-decoder(BART, T5, etc.). These models in some ways can the best of both worlds: they can use bidirectional contextual embeddings for the input \\(x_{1:L}\\) and can generate the output \\(y_{1:L}\\). We now describe the innards of the embedding function \\(\\phi : \\sV^L \\to \\R^{d \\times L}\\): We now introduce the model architectures for language model, with an emphasis on the ubiquitous Transformer architecture. Our exposition of the Transformer architecture will be based on theseslides from CS221 on differentiable programming, and will depart a bit from the standard presentation. The beauty of deep learning is being able to create building blocks, just like we build whole programs out of functions. So we want to be able to functions like the following to encapsulate the complexity: This function will have parameters which we will include in the body but elide in the function signature for simplicity. In what follows, we will define a library of building blocks until we get to the full Transformer. First, we have to convert sequences of tokens into sequences of vectors. \\(\\EmbedToken\\) does exactly this by looking up each token in an embedding matrix \\(E \\in \\R^{|\\sV| \\times d}\\) (a parameter that will be learned from data): def \\(\\EmbedToken(\\x: \\sV^L) \\to \\R^{d \\times L}\\): These are exactly the (context-independent) word embeddings of yore. We define an abstract \\(\\SequenceModel\\) function that takes thesecontext-independent embeddingsand maps them intocontextual embeddings. def \\(\\SequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The simplest type of sequence model is based on feedforward networks (Bengio et al., 2003) applied to afixed lengthcontext, just as in an n-gram model: def \\(\\FeedForwardSequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The first \u201creal\u201d sequence model is a recurrent neural network (RNN), which is a family of models that include simple RNNs, LSTMs, and GRUs. The basic form of an RNN simply computes a sequence ofhidden statesrecursively. def \\(\\SequenceRNN(x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The actual module that does the hard work is the \\(\\RNN\\), which analogous to a finite state machine, takes the current state \\(h\\), a new observation \\(x\\), and returns the updated state: def \\(\\RNN(h: \\R^d, x: \\R^d) \\to \\R^d\\): There are three ways to implement the \\(\\RNN\\). The earliest RNN is a simple RNNElman, 1990, which takes a linear combination of \\(h\\) and \\(x\\) and pushes it through an elementwise non-linear function \\(\\sigma\\) (e.g., logistic \\(\\sigma(z) = (1 + e^{-z})^{-1}\\) or more the modern ReLU \\(\\sigma(z) = \\max(0, z)\\)). def \\(\\SimpleRNN(h: \\R^d, x: \\R^d) \\to \\R^d\\): As defined RNNs only depend on the past, but we can them depend on the future two by running another RNN backwards. These models were used byELMoandULMFiT. def \\(\\BidirectionalSequenceRNN(\\x: \\R^{d \\times L}) \\to \\R^{2d \\times L}\\): Notes: We will not discuss these models in the interest of time. Now, we will discuss Transformers (Vaswani et al. 2017), the sequence model that is really responsible for the takeoff of large language models; they are the building blocks of decoder-only (GPT-2, GPT-3), encoder-only (BERT, RoBERTa), and decoder-encoder (BART, T5) models. There are great resources for learning about the Transformer: You are highly encouraged to read these references. In this lecture, I will strive to take a middle path which emphasizes pseudocode functions and interfaces. The crux of the Transformers are theattention mechanism, which was developed earlier for machine translation (Bahdananu et al. 2017). One can think of attention as a \u201csoft\u201d lookup table, where we have a query \\(y\\) that we want to match against each element in a sequence \\(x_{1:L} = [x_1, \\dots, x_L]\\): We can think of each \\(x_i\\) as representing a key-value pair via linear transformations: and forming the query via another linear transformation: The key and the query can be compared to give a score: These scores can be exponentiated and normalized to form a probability distribution over the token positions \\(\\{ 1, \\dots, L \\}\\): Then the final output is a weighted combination over the values: We can write this all succinctly in matrix form: def \\(\\Attention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d\\): We can think of there as being multiple aspects (e.g., syntax, semantics) that we would want to match on. To accommodate this, we can simultaneously have multipleattention headsand simply combine their outputs. def \\(\\MultiHeadedAttention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d:\\) Self-attention layer. Now we will substitute each \\(x_i\\) in for \\(y\\) as the query argument to produce: def \\(\\SelfAttention(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L})\\): Feedforward layer. Self-attention allows all the tokens to \u201ctalk\u201d to each other, whereas feedforward connections provide: def \\(\\FeedForward(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Improving trainability. We\u2019re almost done. We could in principle just take the \\(\\FeedForward \\circ \\SelfAttention\\) sequence model and iterate it 96 times to make GPT-3, but that network would be hard to optimize (for the same vanishing gradients problems that afflicted RNNs, now just along the depth direction). So we have to do two shenanigans to make sure that the network is trainable. Residual connections. One trick from computer vision is residual connections (ResNet). Instead of applying some function \\(f\\): we add a residual (skip) connection so that if \\(f\\)\u2019s gradients vanish, gradients can still flow through \\(\\x\\): Layer normalization. Another trick islayer normalization, which takes a takes a vector and makes sure its elements are too big: def \\(\\LayerNorm(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): We first define an adapter function that takes a sequence model \\(f\\) and makes it \u201crobust\u201d: def \\(\\AddNorm(f: (\\R^{d \\times L} \\to \\R^{d \\times L}), \\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Finally, we can define the Transformer block succinctly as follows: def \\(\\TransformerBlock(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Positional embeddings. You might have noticed that as defined, the embedding of a token doesn\u2019t depend on where it occurs in the sequence, so \\(\\nl{mouse}\\) in both sentences would have the same embedding, which is not sensible. To fix this, we addpositional informationinto the embedding: def \\(\\EmbedTokenWithPosition(\\x: \\R^{d \\times L})\\): GPT-3. With all the pieces in place, we can now define roughly GPT-3 architecture in one line, just by stacking the Transformer block 96 times: Shape of the architecture (how the 175 billion parameters are allocated): These decisions are not necessarily optimal.Levine et al. 2020provide some theoretical justification, showing that the GPT-3 is too deep, which motivated the training of a deeper but wider Jurassic architecture. There are important but detailed differences between different versions of Transformers:",
            "Tokenization: Modeling: Decoder-only architectures: Encoder-only architectures: Encoder-decoder architectures:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/modeling/#further-reading": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/modeling/#further-reading": ""
        },
        "text_between_h2": [
            "Recall that a language model \\(p\\) is a probability distribution over asequence of tokenswhere each token comes from some vocabulary \\(\\sV\\): However, natural language doesn\u2019t come as a sequence of tokens, but as just a string (concretely, sequence of Unicode characters): Atokenizerconverts any string into a sequence of tokens. This is not necessarily the most glamorous part of language modeling, but plays a really important role in determining how well a model will work. The simplest solution is to do: Therefore, splitting by spaces by spaces to identify words is quite problematic. What makes a good tokenization? Sennrich et al, 2015applied thebyte pair encoding(BPE) algorithm, originally developed for data compression, to produce one of the most commonly used tokenizers. Learning the tokenizer. Intuition: start with each character as its own token and combine tokens that co-occur a lot. Example: The output of learning is: Applying the tokenizer. To tokenize a new string, apply the merges in the same order: Unicode. Rather than just splitting by frequency, a more \u201cprincipled\u201d approach is to define an objective function that captures what a good tokenization looks like. We now describe theunigram model(Kudo 2018). Given a sequence \\(x_{1:L}\\), a tokenization \\(T\\) is a set of Example: Algorithm: Impact: Examples of tokenizations for both GPT-3 and Jurassic (demo):",
            "Thus far, we have defined language models as a probability distribution over sequences of tokens \\(p(x_1, \\dots, x_L)\\), which as we saw was very elegant and powerful (via prompting, a language model can in principle do anything, as GPT-3 hints at). In practice, however, it can be more efficient for specialized tasks to avoid having to generatively model the entire sequence. Contextual embeddings. As a prerequisite, the main key development is to associate a sequence of tokens with a corresponding sequence of contextual embeddings: We will broaden our notion of language models to three types of models. Encoder-only(BERT, RoBERTa, etc.). These language models produce contextual embeddings but cannot be used directly to generate text. These contextual embeddings are generally used for classification tasks (sometimes boldly called natural language understanding tasks). Decoder-only(GPT-2, GPT-3, etc.). These are our standard autoregressive language models, which given a prompt \\(x_{1:i}\\) produces both contextual embeddings and a distribution over next tokens \\(x_{i+1}\\) (and recursively, over the entire completion \\(x_{i+1:L}\\)). Encoder-decoder(BART, T5, etc.). These models in some ways can the best of both worlds: they can use bidirectional contextual embeddings for the input \\(x_{1:L}\\) and can generate the output \\(y_{1:L}\\). We now describe the innards of the embedding function \\(\\phi : \\sV^L \\to \\R^{d \\times L}\\): We now introduce the model architectures for language model, with an emphasis on the ubiquitous Transformer architecture. Our exposition of the Transformer architecture will be based on theseslides from CS221 on differentiable programming, and will depart a bit from the standard presentation. The beauty of deep learning is being able to create building blocks, just like we build whole programs out of functions. So we want to be able to functions like the following to encapsulate the complexity: This function will have parameters which we will include in the body but elide in the function signature for simplicity. In what follows, we will define a library of building blocks until we get to the full Transformer. First, we have to convert sequences of tokens into sequences of vectors. \\(\\EmbedToken\\) does exactly this by looking up each token in an embedding matrix \\(E \\in \\R^{|\\sV| \\times d}\\) (a parameter that will be learned from data): def \\(\\EmbedToken(\\x: \\sV^L) \\to \\R^{d \\times L}\\): These are exactly the (context-independent) word embeddings of yore. We define an abstract \\(\\SequenceModel\\) function that takes thesecontext-independent embeddingsand maps them intocontextual embeddings. def \\(\\SequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The simplest type of sequence model is based on feedforward networks (Bengio et al., 2003) applied to afixed lengthcontext, just as in an n-gram model: def \\(\\FeedForwardSequenceModel(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The first \u201creal\u201d sequence model is a recurrent neural network (RNN), which is a family of models that include simple RNNs, LSTMs, and GRUs. The basic form of an RNN simply computes a sequence ofhidden statesrecursively. def \\(\\SequenceRNN(x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): The actual module that does the hard work is the \\(\\RNN\\), which analogous to a finite state machine, takes the current state \\(h\\), a new observation \\(x\\), and returns the updated state: def \\(\\RNN(h: \\R^d, x: \\R^d) \\to \\R^d\\): There are three ways to implement the \\(\\RNN\\). The earliest RNN is a simple RNNElman, 1990, which takes a linear combination of \\(h\\) and \\(x\\) and pushes it through an elementwise non-linear function \\(\\sigma\\) (e.g., logistic \\(\\sigma(z) = (1 + e^{-z})^{-1}\\) or more the modern ReLU \\(\\sigma(z) = \\max(0, z)\\)). def \\(\\SimpleRNN(h: \\R^d, x: \\R^d) \\to \\R^d\\): As defined RNNs only depend on the past, but we can them depend on the future two by running another RNN backwards. These models were used byELMoandULMFiT. def \\(\\BidirectionalSequenceRNN(\\x: \\R^{d \\times L}) \\to \\R^{2d \\times L}\\): Notes: We will not discuss these models in the interest of time. Now, we will discuss Transformers (Vaswani et al. 2017), the sequence model that is really responsible for the takeoff of large language models; they are the building blocks of decoder-only (GPT-2, GPT-3), encoder-only (BERT, RoBERTa), and decoder-encoder (BART, T5) models. There are great resources for learning about the Transformer: You are highly encouraged to read these references. In this lecture, I will strive to take a middle path which emphasizes pseudocode functions and interfaces. The crux of the Transformers are theattention mechanism, which was developed earlier for machine translation (Bahdananu et al. 2017). One can think of attention as a \u201csoft\u201d lookup table, where we have a query \\(y\\) that we want to match against each element in a sequence \\(x_{1:L} = [x_1, \\dots, x_L]\\): We can think of each \\(x_i\\) as representing a key-value pair via linear transformations: and forming the query via another linear transformation: The key and the query can be compared to give a score: These scores can be exponentiated and normalized to form a probability distribution over the token positions \\(\\{ 1, \\dots, L \\}\\): Then the final output is a weighted combination over the values: We can write this all succinctly in matrix form: def \\(\\Attention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d\\): We can think of there as being multiple aspects (e.g., syntax, semantics) that we would want to match on. To accommodate this, we can simultaneously have multipleattention headsand simply combine their outputs. def \\(\\MultiHeadedAttention(\\x: \\R^{d \\times L}, y: \\R^d) \\to \\R^d:\\) Self-attention layer. Now we will substitute each \\(x_i\\) in for \\(y\\) as the query argument to produce: def \\(\\SelfAttention(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L})\\): Feedforward layer. Self-attention allows all the tokens to \u201ctalk\u201d to each other, whereas feedforward connections provide: def \\(\\FeedForward(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Improving trainability. We\u2019re almost done. We could in principle just take the \\(\\FeedForward \\circ \\SelfAttention\\) sequence model and iterate it 96 times to make GPT-3, but that network would be hard to optimize (for the same vanishing gradients problems that afflicted RNNs, now just along the depth direction). So we have to do two shenanigans to make sure that the network is trainable. Residual connections. One trick from computer vision is residual connections (ResNet). Instead of applying some function \\(f\\): we add a residual (skip) connection so that if \\(f\\)\u2019s gradients vanish, gradients can still flow through \\(\\x\\): Layer normalization. Another trick islayer normalization, which takes a takes a vector and makes sure its elements are too big: def \\(\\LayerNorm(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): We first define an adapter function that takes a sequence model \\(f\\) and makes it \u201crobust\u201d: def \\(\\AddNorm(f: (\\R^{d \\times L} \\to \\R^{d \\times L}), \\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Finally, we can define the Transformer block succinctly as follows: def \\(\\TransformerBlock(\\x: \\R^{d \\times L}) \\to \\R^{d \\times L}\\): Positional embeddings. You might have noticed that as defined, the embedding of a token doesn\u2019t depend on where it occurs in the sequence, so \\(\\nl{mouse}\\) in both sentences would have the same embedding, which is not sensible. To fix this, we addpositional informationinto the embedding: def \\(\\EmbedTokenWithPosition(\\x: \\R^{d \\times L})\\): GPT-3. With all the pieces in place, we can now define roughly GPT-3 architecture in one line, just by stacking the Transformer block 96 times: Shape of the architecture (how the 175 billion parameters are allocated): These decisions are not necessarily optimal.Levine et al. 2020provide some theoretical justification, showing that the GPT-3 is too deep, which motivated the training of a deeper but wider Jurassic architecture. There are important but detailed differences between different versions of Transformers:",
            "Tokenization: Modeling: Decoder-only architectures: Encoder-only architectures: Encoder-decoder architectures:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/training/": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/training/": "Training",
            "https://stanford-cs324.github.io/winter2022/lectures/training/#objective-functions": "",
            "https://stanford-cs324.github.io/winter2022/lectures/training/#optimization-algorithms": "",
            "https://stanford-cs324.github.io/winter2022/lectures/training/#decoder-only-models": "",
            "https://stanford-cs324.github.io/winter2022/lectures/training/#encoder-only-models": "",
            "https://stanford-cs324.github.io/winter2022/lectures/training/#encoder-decoder-models": "",
            "https://stanford-cs324.github.io/winter2022/lectures/training/#further-reading": ""
        },
        "text_between_h2": [
            "We will consider objective functions for the three types of language models: We can use any model that maps token sequences into contextual embeddings (e.g., LSTMs, Transformers): Recall that an autoregressive language model defines a conditional distribution: We define it as follows: Succinctly: Maximum likelihood. Let \\(\\theta\\) be all the parameters of large language models. Let \\(\\sD\\) be thetraining dataconsisting of a set of sequences. We can then follow the maximum likelihood principle and define the following negative log-likelihood objective function: There\u2019s more to say about how to efficiently optimize this function, but that\u2019s all there is for the objective. Unidirectional to bidirectional. A decoder-only model trained using maximum likelihood above also produces (unidirectional) contextual embeddings, but we can provide stronger bidirectional contextual embeddings given that we don\u2019t need to generate. BERT. We will first present theBERTobjective function, which contains two terms: Take the example sequence for natural language inference (predict entailment, contradiction, or neutral): There are two special tokens: Using our notation from the previous lecture, the BERT model is defined as: where \\(\\SentenceEmbedding(\\x)\\) returns one of 2 vectors depending on the sequence:  BERT-large has \\(n_\\text{heads} = 16\\) attention heads, and a \\(d_\\text{model} = 1024\\) dimensional model, resulting in 355M parameters. Masked language modeling. The basic idea of the masked language model is to train on the prediction problem: More more generally, we can think of this as similar to a denoising autoencoder where we map a noisy / incomplete version \\(\\tx\\) and try to reconstruct the original \\(\\x\\). Model. We first define the model distribution that takes \\(\\tx\\) and predicts each tokenindependentlygiven the contextual embedding: Masking function. We define a (stochastic) noising function \\(A(\\tx \\mid \\x)\\) that: Here\u2019s how \\(A\\) is defined: Reducing distribution shift. If we were to always replace chosen tokens in \\(I\\) with \\(\\MASK\\), then: Next sentence prediction. Recall that BERT is trained on pairs of sentences concatenated. The goal of next sentence prediction is to predict whether the second sentence follows from the first or not. \\([\\CLS, \\nl{the}, \\nl{mouse}, \\nl{ate}, \\nl{the}, \\nl{cheese}, \\SEP, \\nl{it}, \\nl{was}, \\nl{full}] \\Rightarrow 1\\). \\([\\CLS, \\nl{the}, \\nl{mouse}, \\nl{ate}, \\nl{the}, \\nl{cheese}, \\SEP, \\nl{hello}, \\nl{world}] \\Rightarrow 0\\). We will use the embedding of the \\(\\CLS\\) token to make this binary classification decision. Dataset. Let \\(\\sD\\) be a set of examples \\((\\x, c)\\) constructed as follows: Objective. Then the BERT objective is: We will talk about training later, but a few quick notes about BERT: RoBERTamakes the following changes to BERT: Example task (table-to-text generation): Recall that encoder-decoder models (e.g., BART, T5): BART (Bidirectional Auto-Regressive Transformers). BART (Lewis et al. 2019) is a Transformer-based encoder-decoder model. BART considers the following transformations \\(A(\\tx \\mid \\x)\\):Based on BERT-scaled experiments, they decided on the following transformations for the final model: They demonstrated strong results on both classification and generation tasks using fine-tuning. T5 (Text-to-Text Transfer Transformer). T5 (Raffel et al., 2020) is another Transformer-based encoder-decoder model. Tasks: This paper experimented with many different unsupervised objectives:and found that the \u201ci.i.d. noise, replace spans\u201d worked well (though many objectives were similar). They also cast all classical NLP tasks in a uniform framework as \u201ctext-to-text\u201d tasks:Note the difference in approach to classification tasks: Notes:",
            "Now we turn our attention to how to optimize the objective. For simplicity, let\u2019s take autogressive language modeling: Stochastic gradient descent (SGD). A first cut is just to do stochastic gradient descent with mini-batches: The key concerns in optimization are: There are several levels that we can approach optimization: ADAM (adaptive moment estimation).ADAMincorporates two ideas: Updating parameters. Memory. Using Adam increases the amount of storage from \\(2(\\text{num-params})\\) (from \\(\\theta_t,g_t\\)) to \\(4(\\text{num-params})\\) (from \\(\\theta_t,g_t,m_t,v_t\\)). AdaFactor (Shazeer & Stern, 2018) was proposed as a way to reduce this memory footprint. Mixed-precision training is another method for reducing memory (Narang et al., 2018).  Learning rates. Initialization. For GPT-3:",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/training/#objective-functions": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/training/#objective-functions": ""
        },
        "text_between_h2": [
            "We will consider objective functions for the three types of language models: We can use any model that maps token sequences into contextual embeddings (e.g., LSTMs, Transformers): Recall that an autoregressive language model defines a conditional distribution: We define it as follows: Succinctly: Maximum likelihood. Let \\(\\theta\\) be all the parameters of large language models. Let \\(\\sD\\) be thetraining dataconsisting of a set of sequences. We can then follow the maximum likelihood principle and define the following negative log-likelihood objective function: There\u2019s more to say about how to efficiently optimize this function, but that\u2019s all there is for the objective. Unidirectional to bidirectional. A decoder-only model trained using maximum likelihood above also produces (unidirectional) contextual embeddings, but we can provide stronger bidirectional contextual embeddings given that we don\u2019t need to generate. BERT. We will first present theBERTobjective function, which contains two terms: Take the example sequence for natural language inference (predict entailment, contradiction, or neutral): There are two special tokens: Using our notation from the previous lecture, the BERT model is defined as: where \\(\\SentenceEmbedding(\\x)\\) returns one of 2 vectors depending on the sequence:  BERT-large has \\(n_\\text{heads} = 16\\) attention heads, and a \\(d_\\text{model} = 1024\\) dimensional model, resulting in 355M parameters. Masked language modeling. The basic idea of the masked language model is to train on the prediction problem: More more generally, we can think of this as similar to a denoising autoencoder where we map a noisy / incomplete version \\(\\tx\\) and try to reconstruct the original \\(\\x\\). Model. We first define the model distribution that takes \\(\\tx\\) and predicts each tokenindependentlygiven the contextual embedding: Masking function. We define a (stochastic) noising function \\(A(\\tx \\mid \\x)\\) that: Here\u2019s how \\(A\\) is defined: Reducing distribution shift. If we were to always replace chosen tokens in \\(I\\) with \\(\\MASK\\), then: Next sentence prediction. Recall that BERT is trained on pairs of sentences concatenated. The goal of next sentence prediction is to predict whether the second sentence follows from the first or not. \\([\\CLS, \\nl{the}, \\nl{mouse}, \\nl{ate}, \\nl{the}, \\nl{cheese}, \\SEP, \\nl{it}, \\nl{was}, \\nl{full}] \\Rightarrow 1\\). \\([\\CLS, \\nl{the}, \\nl{mouse}, \\nl{ate}, \\nl{the}, \\nl{cheese}, \\SEP, \\nl{hello}, \\nl{world}] \\Rightarrow 0\\). We will use the embedding of the \\(\\CLS\\) token to make this binary classification decision. Dataset. Let \\(\\sD\\) be a set of examples \\((\\x, c)\\) constructed as follows: Objective. Then the BERT objective is: We will talk about training later, but a few quick notes about BERT: RoBERTamakes the following changes to BERT: Example task (table-to-text generation): Recall that encoder-decoder models (e.g., BART, T5): BART (Bidirectional Auto-Regressive Transformers). BART (Lewis et al. 2019) is a Transformer-based encoder-decoder model. BART considers the following transformations \\(A(\\tx \\mid \\x)\\):Based on BERT-scaled experiments, they decided on the following transformations for the final model: They demonstrated strong results on both classification and generation tasks using fine-tuning. T5 (Text-to-Text Transfer Transformer). T5 (Raffel et al., 2020) is another Transformer-based encoder-decoder model. Tasks: This paper experimented with many different unsupervised objectives:and found that the \u201ci.i.d. noise, replace spans\u201d worked well (though many objectives were similar). They also cast all classical NLP tasks in a uniform framework as \u201ctext-to-text\u201d tasks:Note the difference in approach to classification tasks: Notes:",
            "Now we turn our attention to how to optimize the objective. For simplicity, let\u2019s take autogressive language modeling: Stochastic gradient descent (SGD). A first cut is just to do stochastic gradient descent with mini-batches: The key concerns in optimization are: There are several levels that we can approach optimization: ADAM (adaptive moment estimation).ADAMincorporates two ideas: Updating parameters. Memory. Using Adam increases the amount of storage from \\(2(\\text{num-params})\\) (from \\(\\theta_t,g_t\\)) to \\(4(\\text{num-params})\\) (from \\(\\theta_t,g_t,m_t,v_t\\)). AdaFactor (Shazeer & Stern, 2018) was proposed as a way to reduce this memory footprint. Mixed-precision training is another method for reducing memory (Narang et al., 2018).  Learning rates. Initialization. For GPT-3:",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/training/#optimization-algorithms": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/training/#optimization-algorithms": ""
        },
        "text_between_h2": [
            "We will consider objective functions for the three types of language models: We can use any model that maps token sequences into contextual embeddings (e.g., LSTMs, Transformers): Recall that an autoregressive language model defines a conditional distribution: We define it as follows: Succinctly: Maximum likelihood. Let \\(\\theta\\) be all the parameters of large language models. Let \\(\\sD\\) be thetraining dataconsisting of a set of sequences. We can then follow the maximum likelihood principle and define the following negative log-likelihood objective function: There\u2019s more to say about how to efficiently optimize this function, but that\u2019s all there is for the objective. Unidirectional to bidirectional. A decoder-only model trained using maximum likelihood above also produces (unidirectional) contextual embeddings, but we can provide stronger bidirectional contextual embeddings given that we don\u2019t need to generate. BERT. We will first present theBERTobjective function, which contains two terms: Take the example sequence for natural language inference (predict entailment, contradiction, or neutral): There are two special tokens: Using our notation from the previous lecture, the BERT model is defined as: where \\(\\SentenceEmbedding(\\x)\\) returns one of 2 vectors depending on the sequence:  BERT-large has \\(n_\\text{heads} = 16\\) attention heads, and a \\(d_\\text{model} = 1024\\) dimensional model, resulting in 355M parameters. Masked language modeling. The basic idea of the masked language model is to train on the prediction problem: More more generally, we can think of this as similar to a denoising autoencoder where we map a noisy / incomplete version \\(\\tx\\) and try to reconstruct the original \\(\\x\\). Model. We first define the model distribution that takes \\(\\tx\\) and predicts each tokenindependentlygiven the contextual embedding: Masking function. We define a (stochastic) noising function \\(A(\\tx \\mid \\x)\\) that: Here\u2019s how \\(A\\) is defined: Reducing distribution shift. If we were to always replace chosen tokens in \\(I\\) with \\(\\MASK\\), then: Next sentence prediction. Recall that BERT is trained on pairs of sentences concatenated. The goal of next sentence prediction is to predict whether the second sentence follows from the first or not. \\([\\CLS, \\nl{the}, \\nl{mouse}, \\nl{ate}, \\nl{the}, \\nl{cheese}, \\SEP, \\nl{it}, \\nl{was}, \\nl{full}] \\Rightarrow 1\\). \\([\\CLS, \\nl{the}, \\nl{mouse}, \\nl{ate}, \\nl{the}, \\nl{cheese}, \\SEP, \\nl{hello}, \\nl{world}] \\Rightarrow 0\\). We will use the embedding of the \\(\\CLS\\) token to make this binary classification decision. Dataset. Let \\(\\sD\\) be a set of examples \\((\\x, c)\\) constructed as follows: Objective. Then the BERT objective is: We will talk about training later, but a few quick notes about BERT: RoBERTamakes the following changes to BERT: Example task (table-to-text generation): Recall that encoder-decoder models (e.g., BART, T5): BART (Bidirectional Auto-Regressive Transformers). BART (Lewis et al. 2019) is a Transformer-based encoder-decoder model. BART considers the following transformations \\(A(\\tx \\mid \\x)\\):Based on BERT-scaled experiments, they decided on the following transformations for the final model: They demonstrated strong results on both classification and generation tasks using fine-tuning. T5 (Text-to-Text Transfer Transformer). T5 (Raffel et al., 2020) is another Transformer-based encoder-decoder model. Tasks: This paper experimented with many different unsupervised objectives:and found that the \u201ci.i.d. noise, replace spans\u201d worked well (though many objectives were similar). They also cast all classical NLP tasks in a uniform framework as \u201ctext-to-text\u201d tasks:Note the difference in approach to classification tasks: Notes:",
            "Now we turn our attention to how to optimize the objective. For simplicity, let\u2019s take autogressive language modeling: Stochastic gradient descent (SGD). A first cut is just to do stochastic gradient descent with mini-batches: The key concerns in optimization are: There are several levels that we can approach optimization: ADAM (adaptive moment estimation).ADAMincorporates two ideas: Updating parameters. Memory. Using Adam increases the amount of storage from \\(2(\\text{num-params})\\) (from \\(\\theta_t,g_t\\)) to \\(4(\\text{num-params})\\) (from \\(\\theta_t,g_t,m_t,v_t\\)). AdaFactor (Shazeer & Stern, 2018) was proposed as a way to reduce this memory footprint. Mixed-precision training is another method for reducing memory (Narang et al., 2018).  Learning rates. Initialization. For GPT-3:",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/training/#decoder-only-models": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/training/#decoder-only-models": ""
        },
        "text_between_h2": [
            "We will consider objective functions for the three types of language models: We can use any model that maps token sequences into contextual embeddings (e.g., LSTMs, Transformers): Recall that an autoregressive language model defines a conditional distribution: We define it as follows: Succinctly: Maximum likelihood. Let \\(\\theta\\) be all the parameters of large language models. Let \\(\\sD\\) be thetraining dataconsisting of a set of sequences. We can then follow the maximum likelihood principle and define the following negative log-likelihood objective function: There\u2019s more to say about how to efficiently optimize this function, but that\u2019s all there is for the objective. Unidirectional to bidirectional. A decoder-only model trained using maximum likelihood above also produces (unidirectional) contextual embeddings, but we can provide stronger bidirectional contextual embeddings given that we don\u2019t need to generate. BERT. We will first present theBERTobjective function, which contains two terms: Take the example sequence for natural language inference (predict entailment, contradiction, or neutral): There are two special tokens: Using our notation from the previous lecture, the BERT model is defined as: where \\(\\SentenceEmbedding(\\x)\\) returns one of 2 vectors depending on the sequence:  BERT-large has \\(n_\\text{heads} = 16\\) attention heads, and a \\(d_\\text{model} = 1024\\) dimensional model, resulting in 355M parameters. Masked language modeling. The basic idea of the masked language model is to train on the prediction problem: More more generally, we can think of this as similar to a denoising autoencoder where we map a noisy / incomplete version \\(\\tx\\) and try to reconstruct the original \\(\\x\\). Model. We first define the model distribution that takes \\(\\tx\\) and predicts each tokenindependentlygiven the contextual embedding: Masking function. We define a (stochastic) noising function \\(A(\\tx \\mid \\x)\\) that: Here\u2019s how \\(A\\) is defined: Reducing distribution shift. If we were to always replace chosen tokens in \\(I\\) with \\(\\MASK\\), then: Next sentence prediction. Recall that BERT is trained on pairs of sentences concatenated. The goal of next sentence prediction is to predict whether the second sentence follows from the first or not. \\([\\CLS, \\nl{the}, \\nl{mouse}, \\nl{ate}, \\nl{the}, \\nl{cheese}, \\SEP, \\nl{it}, \\nl{was}, \\nl{full}] \\Rightarrow 1\\). \\([\\CLS, \\nl{the}, \\nl{mouse}, \\nl{ate}, \\nl{the}, \\nl{cheese}, \\SEP, \\nl{hello}, \\nl{world}] \\Rightarrow 0\\). We will use the embedding of the \\(\\CLS\\) token to make this binary classification decision. Dataset. Let \\(\\sD\\) be a set of examples \\((\\x, c)\\) constructed as follows: Objective. Then the BERT objective is: We will talk about training later, but a few quick notes about BERT: RoBERTamakes the following changes to BERT: Example task (table-to-text generation): Recall that encoder-decoder models (e.g., BART, T5): BART (Bidirectional Auto-Regressive Transformers). BART (Lewis et al. 2019) is a Transformer-based encoder-decoder model. BART considers the following transformations \\(A(\\tx \\mid \\x)\\):Based on BERT-scaled experiments, they decided on the following transformations for the final model: They demonstrated strong results on both classification and generation tasks using fine-tuning. T5 (Text-to-Text Transfer Transformer). T5 (Raffel et al., 2020) is another Transformer-based encoder-decoder model. Tasks: This paper experimented with many different unsupervised objectives:and found that the \u201ci.i.d. noise, replace spans\u201d worked well (though many objectives were similar). They also cast all classical NLP tasks in a uniform framework as \u201ctext-to-text\u201d tasks:Note the difference in approach to classification tasks: Notes:",
            "Now we turn our attention to how to optimize the objective. For simplicity, let\u2019s take autogressive language modeling: Stochastic gradient descent (SGD). A first cut is just to do stochastic gradient descent with mini-batches: The key concerns in optimization are: There are several levels that we can approach optimization: ADAM (adaptive moment estimation).ADAMincorporates two ideas: Updating parameters. Memory. Using Adam increases the amount of storage from \\(2(\\text{num-params})\\) (from \\(\\theta_t,g_t\\)) to \\(4(\\text{num-params})\\) (from \\(\\theta_t,g_t,m_t,v_t\\)). AdaFactor (Shazeer & Stern, 2018) was proposed as a way to reduce this memory footprint. Mixed-precision training is another method for reducing memory (Narang et al., 2018).  Learning rates. Initialization. For GPT-3:",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/training/#encoder-only-models": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/training/#encoder-only-models": ""
        },
        "text_between_h2": [
            "We will consider objective functions for the three types of language models: We can use any model that maps token sequences into contextual embeddings (e.g., LSTMs, Transformers): Recall that an autoregressive language model defines a conditional distribution: We define it as follows: Succinctly: Maximum likelihood. Let \\(\\theta\\) be all the parameters of large language models. Let \\(\\sD\\) be thetraining dataconsisting of a set of sequences. We can then follow the maximum likelihood principle and define the following negative log-likelihood objective function: There\u2019s more to say about how to efficiently optimize this function, but that\u2019s all there is for the objective. Unidirectional to bidirectional. A decoder-only model trained using maximum likelihood above also produces (unidirectional) contextual embeddings, but we can provide stronger bidirectional contextual embeddings given that we don\u2019t need to generate. BERT. We will first present theBERTobjective function, which contains two terms: Take the example sequence for natural language inference (predict entailment, contradiction, or neutral): There are two special tokens: Using our notation from the previous lecture, the BERT model is defined as: where \\(\\SentenceEmbedding(\\x)\\) returns one of 2 vectors depending on the sequence:  BERT-large has \\(n_\\text{heads} = 16\\) attention heads, and a \\(d_\\text{model} = 1024\\) dimensional model, resulting in 355M parameters. Masked language modeling. The basic idea of the masked language model is to train on the prediction problem: More more generally, we can think of this as similar to a denoising autoencoder where we map a noisy / incomplete version \\(\\tx\\) and try to reconstruct the original \\(\\x\\). Model. We first define the model distribution that takes \\(\\tx\\) and predicts each tokenindependentlygiven the contextual embedding: Masking function. We define a (stochastic) noising function \\(A(\\tx \\mid \\x)\\) that: Here\u2019s how \\(A\\) is defined: Reducing distribution shift. If we were to always replace chosen tokens in \\(I\\) with \\(\\MASK\\), then: Next sentence prediction. Recall that BERT is trained on pairs of sentences concatenated. The goal of next sentence prediction is to predict whether the second sentence follows from the first or not. \\([\\CLS, \\nl{the}, \\nl{mouse}, \\nl{ate}, \\nl{the}, \\nl{cheese}, \\SEP, \\nl{it}, \\nl{was}, \\nl{full}] \\Rightarrow 1\\). \\([\\CLS, \\nl{the}, \\nl{mouse}, \\nl{ate}, \\nl{the}, \\nl{cheese}, \\SEP, \\nl{hello}, \\nl{world}] \\Rightarrow 0\\). We will use the embedding of the \\(\\CLS\\) token to make this binary classification decision. Dataset. Let \\(\\sD\\) be a set of examples \\((\\x, c)\\) constructed as follows: Objective. Then the BERT objective is: We will talk about training later, but a few quick notes about BERT: RoBERTamakes the following changes to BERT: Example task (table-to-text generation): Recall that encoder-decoder models (e.g., BART, T5): BART (Bidirectional Auto-Regressive Transformers). BART (Lewis et al. 2019) is a Transformer-based encoder-decoder model. BART considers the following transformations \\(A(\\tx \\mid \\x)\\):Based on BERT-scaled experiments, they decided on the following transformations for the final model: They demonstrated strong results on both classification and generation tasks using fine-tuning. T5 (Text-to-Text Transfer Transformer). T5 (Raffel et al., 2020) is another Transformer-based encoder-decoder model. Tasks: This paper experimented with many different unsupervised objectives:and found that the \u201ci.i.d. noise, replace spans\u201d worked well (though many objectives were similar). They also cast all classical NLP tasks in a uniform framework as \u201ctext-to-text\u201d tasks:Note the difference in approach to classification tasks: Notes:",
            "Now we turn our attention to how to optimize the objective. For simplicity, let\u2019s take autogressive language modeling: Stochastic gradient descent (SGD). A first cut is just to do stochastic gradient descent with mini-batches: The key concerns in optimization are: There are several levels that we can approach optimization: ADAM (adaptive moment estimation).ADAMincorporates two ideas: Updating parameters. Memory. Using Adam increases the amount of storage from \\(2(\\text{num-params})\\) (from \\(\\theta_t,g_t\\)) to \\(4(\\text{num-params})\\) (from \\(\\theta_t,g_t,m_t,v_t\\)). AdaFactor (Shazeer & Stern, 2018) was proposed as a way to reduce this memory footprint. Mixed-precision training is another method for reducing memory (Narang et al., 2018).  Learning rates. Initialization. For GPT-3:",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/training/#encoder-decoder-models": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/training/#encoder-decoder-models": ""
        },
        "text_between_h2": [
            "We will consider objective functions for the three types of language models: We can use any model that maps token sequences into contextual embeddings (e.g., LSTMs, Transformers): Recall that an autoregressive language model defines a conditional distribution: We define it as follows: Succinctly: Maximum likelihood. Let \\(\\theta\\) be all the parameters of large language models. Let \\(\\sD\\) be thetraining dataconsisting of a set of sequences. We can then follow the maximum likelihood principle and define the following negative log-likelihood objective function: There\u2019s more to say about how to efficiently optimize this function, but that\u2019s all there is for the objective. Unidirectional to bidirectional. A decoder-only model trained using maximum likelihood above also produces (unidirectional) contextual embeddings, but we can provide stronger bidirectional contextual embeddings given that we don\u2019t need to generate. BERT. We will first present theBERTobjective function, which contains two terms: Take the example sequence for natural language inference (predict entailment, contradiction, or neutral): There are two special tokens: Using our notation from the previous lecture, the BERT model is defined as: where \\(\\SentenceEmbedding(\\x)\\) returns one of 2 vectors depending on the sequence:  BERT-large has \\(n_\\text{heads} = 16\\) attention heads, and a \\(d_\\text{model} = 1024\\) dimensional model, resulting in 355M parameters. Masked language modeling. The basic idea of the masked language model is to train on the prediction problem: More more generally, we can think of this as similar to a denoising autoencoder where we map a noisy / incomplete version \\(\\tx\\) and try to reconstruct the original \\(\\x\\). Model. We first define the model distribution that takes \\(\\tx\\) and predicts each tokenindependentlygiven the contextual embedding: Masking function. We define a (stochastic) noising function \\(A(\\tx \\mid \\x)\\) that: Here\u2019s how \\(A\\) is defined: Reducing distribution shift. If we were to always replace chosen tokens in \\(I\\) with \\(\\MASK\\), then: Next sentence prediction. Recall that BERT is trained on pairs of sentences concatenated. The goal of next sentence prediction is to predict whether the second sentence follows from the first or not. \\([\\CLS, \\nl{the}, \\nl{mouse}, \\nl{ate}, \\nl{the}, \\nl{cheese}, \\SEP, \\nl{it}, \\nl{was}, \\nl{full}] \\Rightarrow 1\\). \\([\\CLS, \\nl{the}, \\nl{mouse}, \\nl{ate}, \\nl{the}, \\nl{cheese}, \\SEP, \\nl{hello}, \\nl{world}] \\Rightarrow 0\\). We will use the embedding of the \\(\\CLS\\) token to make this binary classification decision. Dataset. Let \\(\\sD\\) be a set of examples \\((\\x, c)\\) constructed as follows: Objective. Then the BERT objective is: We will talk about training later, but a few quick notes about BERT: RoBERTamakes the following changes to BERT: Example task (table-to-text generation): Recall that encoder-decoder models (e.g., BART, T5): BART (Bidirectional Auto-Regressive Transformers). BART (Lewis et al. 2019) is a Transformer-based encoder-decoder model. BART considers the following transformations \\(A(\\tx \\mid \\x)\\):Based on BERT-scaled experiments, they decided on the following transformations for the final model: They demonstrated strong results on both classification and generation tasks using fine-tuning. T5 (Text-to-Text Transfer Transformer). T5 (Raffel et al., 2020) is another Transformer-based encoder-decoder model. Tasks: This paper experimented with many different unsupervised objectives:and found that the \u201ci.i.d. noise, replace spans\u201d worked well (though many objectives were similar). They also cast all classical NLP tasks in a uniform framework as \u201ctext-to-text\u201d tasks:Note the difference in approach to classification tasks: Notes:",
            "Now we turn our attention to how to optimize the objective. For simplicity, let\u2019s take autogressive language modeling: Stochastic gradient descent (SGD). A first cut is just to do stochastic gradient descent with mini-batches: The key concerns in optimization are: There are several levels that we can approach optimization: ADAM (adaptive moment estimation).ADAMincorporates two ideas: Updating parameters. Memory. Using Adam increases the amount of storage from \\(2(\\text{num-params})\\) (from \\(\\theta_t,g_t\\)) to \\(4(\\text{num-params})\\) (from \\(\\theta_t,g_t,m_t,v_t\\)). AdaFactor (Shazeer & Stern, 2018) was proposed as a way to reduce this memory footprint. Mixed-precision training is another method for reducing memory (Narang et al., 2018).  Learning rates. Initialization. For GPT-3:",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/training/#further-reading": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/training/#further-reading": ""
        },
        "text_between_h2": [
            "We will consider objective functions for the three types of language models: We can use any model that maps token sequences into contextual embeddings (e.g., LSTMs, Transformers): Recall that an autoregressive language model defines a conditional distribution: We define it as follows: Succinctly: Maximum likelihood. Let \\(\\theta\\) be all the parameters of large language models. Let \\(\\sD\\) be thetraining dataconsisting of a set of sequences. We can then follow the maximum likelihood principle and define the following negative log-likelihood objective function: There\u2019s more to say about how to efficiently optimize this function, but that\u2019s all there is for the objective. Unidirectional to bidirectional. A decoder-only model trained using maximum likelihood above also produces (unidirectional) contextual embeddings, but we can provide stronger bidirectional contextual embeddings given that we don\u2019t need to generate. BERT. We will first present theBERTobjective function, which contains two terms: Take the example sequence for natural language inference (predict entailment, contradiction, or neutral): There are two special tokens: Using our notation from the previous lecture, the BERT model is defined as: where \\(\\SentenceEmbedding(\\x)\\) returns one of 2 vectors depending on the sequence:  BERT-large has \\(n_\\text{heads} = 16\\) attention heads, and a \\(d_\\text{model} = 1024\\) dimensional model, resulting in 355M parameters. Masked language modeling. The basic idea of the masked language model is to train on the prediction problem: More more generally, we can think of this as similar to a denoising autoencoder where we map a noisy / incomplete version \\(\\tx\\) and try to reconstruct the original \\(\\x\\). Model. We first define the model distribution that takes \\(\\tx\\) and predicts each tokenindependentlygiven the contextual embedding: Masking function. We define a (stochastic) noising function \\(A(\\tx \\mid \\x)\\) that: Here\u2019s how \\(A\\) is defined: Reducing distribution shift. If we were to always replace chosen tokens in \\(I\\) with \\(\\MASK\\), then: Next sentence prediction. Recall that BERT is trained on pairs of sentences concatenated. The goal of next sentence prediction is to predict whether the second sentence follows from the first or not. \\([\\CLS, \\nl{the}, \\nl{mouse}, \\nl{ate}, \\nl{the}, \\nl{cheese}, \\SEP, \\nl{it}, \\nl{was}, \\nl{full}] \\Rightarrow 1\\). \\([\\CLS, \\nl{the}, \\nl{mouse}, \\nl{ate}, \\nl{the}, \\nl{cheese}, \\SEP, \\nl{hello}, \\nl{world}] \\Rightarrow 0\\). We will use the embedding of the \\(\\CLS\\) token to make this binary classification decision. Dataset. Let \\(\\sD\\) be a set of examples \\((\\x, c)\\) constructed as follows: Objective. Then the BERT objective is: We will talk about training later, but a few quick notes about BERT: RoBERTamakes the following changes to BERT: Example task (table-to-text generation): Recall that encoder-decoder models (e.g., BART, T5): BART (Bidirectional Auto-Regressive Transformers). BART (Lewis et al. 2019) is a Transformer-based encoder-decoder model. BART considers the following transformations \\(A(\\tx \\mid \\x)\\):Based on BERT-scaled experiments, they decided on the following transformations for the final model: They demonstrated strong results on both classification and generation tasks using fine-tuning. T5 (Text-to-Text Transfer Transformer). T5 (Raffel et al., 2020) is another Transformer-based encoder-decoder model. Tasks: This paper experimented with many different unsupervised objectives:and found that the \u201ci.i.d. noise, replace spans\u201d worked well (though many objectives were similar). They also cast all classical NLP tasks in a uniform framework as \u201ctext-to-text\u201d tasks:Note the difference in approach to classification tasks: Notes:",
            "Now we turn our attention to how to optimize the objective. For simplicity, let\u2019s take autogressive language modeling: Stochastic gradient descent (SGD). A first cut is just to do stochastic gradient descent with mini-batches: The key concerns in optimization are: There are several levels that we can approach optimization: ADAM (adaptive moment estimation).ADAMincorporates two ideas: Updating parameters. Memory. Using Adam increases the amount of storage from \\(2(\\text{num-params})\\) (from \\(\\theta_t,g_t\\)) to \\(4(\\text{num-params})\\) (from \\(\\theta_t,g_t,m_t,v_t\\)). AdaFactor (Shazeer & Stern, 2018) was proposed as a way to reduce this memory footprint. Mixed-precision training is another method for reducing memory (Narang et al., 2018).  Learning rates. Initialization. For GPT-3:",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/parallelism/": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/parallelism/": "Parallelism",
            "https://stanford-cs324.github.io/winter2022/lectures/parallelism/#further-reading": ""
        },
        "text_between_h2": [
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/parallelism/#further-reading": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/parallelism/#further-reading": ""
        },
        "text_between_h2": [
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/scaling-laws/": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/scaling-laws/": "Scaling laws",
            "https://stanford-cs324.github.io/winter2022/lectures/scaling-laws/#further-reading": ""
        },
        "text_between_h2": [
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/scaling-laws/#further-reading": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/scaling-laws/#further-reading": ""
        },
        "text_between_h2": [
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/": "Selective architectures",
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#mixture-of-experts": "",
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#sparsely-gated-mixture-of-experts-lepikhin-et-al-2021": "",
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#switch-transformer-fedus-et-al-2021": "",
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#balanced-assignment-of-sparse-experts-base-layers-lewis-et-al-2021": "",
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#generalist-language-model-glam-du-et-al-2021": "",
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#facebookmoe-artetxe-et-al-2021": "",
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#decentralized-mixture-of-experts-ryabinin--gusev-2020": "",
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#summary": "",
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#retrieval-based-models": "",
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#retrieval-augmented-generation-rag-lewis-et-al-2020": "",
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#retro-borgeaud-et-al-2021": "",
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#discussion": "",
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#summary-1": "",
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#further-reading": ""
        },
        "text_between_h2": [
            "Basics. The idea of mixture of experts goes back toJacobs et al. (1991).  To introduce the basic idea, suppose we are solving a prediction problem: Let us start out by learning a feedforward (ReLU) neural network: where the parameters are \\(\\theta = (W_1, W_2)\\). But themixture-of-expertsapproach is to: Example. Consider \\(d = 2\\) and each expert being a linear classifier (source): Training. We can learn a mixture-of-experts model by normal backpropagation. Applying the product rule yields: Notice that the gradient is proportional to \\(g_e(x)\\) and updates both the gating function and the experts. Saving compute. Balancing experts. Parallelism.  We define thetop-2 expertsapproximate gating function as follows: Notation: Balancing experts. For example, we can take \\(\\lambda = \\frac{0.01}{B}\\). Example. Here is an example with \\(B = 2\\) tokens and \\(E = 4\\) experts: The counter would be: We would try to push down on the gating function on expert 2 to discourage its use. Experimental setup:  BASE requires more compute to optimize the assignment \\(a\\), but is more stable. Summary and next steps. Specification: Other upgrades: Results:  Results on WinoGender: Setup:  Results onStereoSet: Motivation: Main considerations: Distributed hash tables:  Experiments from the paper: Diskin et al., 2021:",
            "We now turn to another class of language models,retrieval-based(or retrieval-augmented, memory-augmented models), that can help us push past the scaling ceiling of a dense Transformer. Encoder-decoder. Let us first focus on sequence-to-sequence tasks using an encoder-decoder framework: Example (open-book question answering): Recall thatBARTandT5are examples of encoder-decoder models: that are trained on denoising objectives; for example: Retrieval. Let us assume that we have astore\\(S\\), which is a set of sequences (usually, documents or passages). Intuitively, a retrieval-based model generates: Example (open-book question answering): Nearest neighborsas a special case:  Formally, the RAG-Sequence model is defined as follows: In practice, the summation \\(z \\in S\\) is replaced by thetop-k(analogous to choosing the top 1 or 2 experts for mixture of experts). Retriever: Dense Passage Retrieval (DPR)(Karpukhin et al., 2020). Generator. Training. Experiments.  Results:",
            "",
            "Mixture of experts: Retrieval-based models:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#mixture-of-experts": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#mixture-of-experts": ""
        },
        "text_between_h2": [
            "Basics. The idea of mixture of experts goes back toJacobs et al. (1991).  To introduce the basic idea, suppose we are solving a prediction problem: Let us start out by learning a feedforward (ReLU) neural network: where the parameters are \\(\\theta = (W_1, W_2)\\). But themixture-of-expertsapproach is to: Example. Consider \\(d = 2\\) and each expert being a linear classifier (source): Training. We can learn a mixture-of-experts model by normal backpropagation. Applying the product rule yields: Notice that the gradient is proportional to \\(g_e(x)\\) and updates both the gating function and the experts. Saving compute. Balancing experts. Parallelism.  We define thetop-2 expertsapproximate gating function as follows: Notation: Balancing experts. For example, we can take \\(\\lambda = \\frac{0.01}{B}\\). Example. Here is an example with \\(B = 2\\) tokens and \\(E = 4\\) experts: The counter would be: We would try to push down on the gating function on expert 2 to discourage its use. Experimental setup:  BASE requires more compute to optimize the assignment \\(a\\), but is more stable. Summary and next steps. Specification: Other upgrades: Results:  Results on WinoGender: Setup:  Results onStereoSet: Motivation: Main considerations: Distributed hash tables:  Experiments from the paper: Diskin et al., 2021:",
            "We now turn to another class of language models,retrieval-based(or retrieval-augmented, memory-augmented models), that can help us push past the scaling ceiling of a dense Transformer. Encoder-decoder. Let us first focus on sequence-to-sequence tasks using an encoder-decoder framework: Example (open-book question answering): Recall thatBARTandT5are examples of encoder-decoder models: that are trained on denoising objectives; for example: Retrieval. Let us assume that we have astore\\(S\\), which is a set of sequences (usually, documents or passages). Intuitively, a retrieval-based model generates: Example (open-book question answering): Nearest neighborsas a special case:  Formally, the RAG-Sequence model is defined as follows: In practice, the summation \\(z \\in S\\) is replaced by thetop-k(analogous to choosing the top 1 or 2 experts for mixture of experts). Retriever: Dense Passage Retrieval (DPR)(Karpukhin et al., 2020). Generator. Training. Experiments.  Results:",
            "",
            "Mixture of experts: Retrieval-based models:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#sparsely-gated-mixture-of-experts-lepikhin-et-al-2021": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#sparsely-gated-mixture-of-experts-lepikhin-et-al-2021": ""
        },
        "text_between_h2": [
            "Basics. The idea of mixture of experts goes back toJacobs et al. (1991).  To introduce the basic idea, suppose we are solving a prediction problem: Let us start out by learning a feedforward (ReLU) neural network: where the parameters are \\(\\theta = (W_1, W_2)\\). But themixture-of-expertsapproach is to: Example. Consider \\(d = 2\\) and each expert being a linear classifier (source): Training. We can learn a mixture-of-experts model by normal backpropagation. Applying the product rule yields: Notice that the gradient is proportional to \\(g_e(x)\\) and updates both the gating function and the experts. Saving compute. Balancing experts. Parallelism.  We define thetop-2 expertsapproximate gating function as follows: Notation: Balancing experts. For example, we can take \\(\\lambda = \\frac{0.01}{B}\\). Example. Here is an example with \\(B = 2\\) tokens and \\(E = 4\\) experts: The counter would be: We would try to push down on the gating function on expert 2 to discourage its use. Experimental setup:  BASE requires more compute to optimize the assignment \\(a\\), but is more stable. Summary and next steps. Specification: Other upgrades: Results:  Results on WinoGender: Setup:  Results onStereoSet: Motivation: Main considerations: Distributed hash tables:  Experiments from the paper: Diskin et al., 2021:",
            "We now turn to another class of language models,retrieval-based(or retrieval-augmented, memory-augmented models), that can help us push past the scaling ceiling of a dense Transformer. Encoder-decoder. Let us first focus on sequence-to-sequence tasks using an encoder-decoder framework: Example (open-book question answering): Recall thatBARTandT5are examples of encoder-decoder models: that are trained on denoising objectives; for example: Retrieval. Let us assume that we have astore\\(S\\), which is a set of sequences (usually, documents or passages). Intuitively, a retrieval-based model generates: Example (open-book question answering): Nearest neighborsas a special case:  Formally, the RAG-Sequence model is defined as follows: In practice, the summation \\(z \\in S\\) is replaced by thetop-k(analogous to choosing the top 1 or 2 experts for mixture of experts). Retriever: Dense Passage Retrieval (DPR)(Karpukhin et al., 2020). Generator. Training. Experiments.  Results:",
            "",
            "Mixture of experts: Retrieval-based models:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#switch-transformer-fedus-et-al-2021": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#switch-transformer-fedus-et-al-2021": ""
        },
        "text_between_h2": [
            "Basics. The idea of mixture of experts goes back toJacobs et al. (1991).  To introduce the basic idea, suppose we are solving a prediction problem: Let us start out by learning a feedforward (ReLU) neural network: where the parameters are \\(\\theta = (W_1, W_2)\\). But themixture-of-expertsapproach is to: Example. Consider \\(d = 2\\) and each expert being a linear classifier (source): Training. We can learn a mixture-of-experts model by normal backpropagation. Applying the product rule yields: Notice that the gradient is proportional to \\(g_e(x)\\) and updates both the gating function and the experts. Saving compute. Balancing experts. Parallelism.  We define thetop-2 expertsapproximate gating function as follows: Notation: Balancing experts. For example, we can take \\(\\lambda = \\frac{0.01}{B}\\). Example. Here is an example with \\(B = 2\\) tokens and \\(E = 4\\) experts: The counter would be: We would try to push down on the gating function on expert 2 to discourage its use. Experimental setup:  BASE requires more compute to optimize the assignment \\(a\\), but is more stable. Summary and next steps. Specification: Other upgrades: Results:  Results on WinoGender: Setup:  Results onStereoSet: Motivation: Main considerations: Distributed hash tables:  Experiments from the paper: Diskin et al., 2021:",
            "We now turn to another class of language models,retrieval-based(or retrieval-augmented, memory-augmented models), that can help us push past the scaling ceiling of a dense Transformer. Encoder-decoder. Let us first focus on sequence-to-sequence tasks using an encoder-decoder framework: Example (open-book question answering): Recall thatBARTandT5are examples of encoder-decoder models: that are trained on denoising objectives; for example: Retrieval. Let us assume that we have astore\\(S\\), which is a set of sequences (usually, documents or passages). Intuitively, a retrieval-based model generates: Example (open-book question answering): Nearest neighborsas a special case:  Formally, the RAG-Sequence model is defined as follows: In practice, the summation \\(z \\in S\\) is replaced by thetop-k(analogous to choosing the top 1 or 2 experts for mixture of experts). Retriever: Dense Passage Retrieval (DPR)(Karpukhin et al., 2020). Generator. Training. Experiments.  Results:",
            "",
            "Mixture of experts: Retrieval-based models:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#balanced-assignment-of-sparse-experts-base-layers-lewis-et-al-2021": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#balanced-assignment-of-sparse-experts-base-layers-lewis-et-al-2021": ""
        },
        "text_between_h2": [
            "Basics. The idea of mixture of experts goes back toJacobs et al. (1991).  To introduce the basic idea, suppose we are solving a prediction problem: Let us start out by learning a feedforward (ReLU) neural network: where the parameters are \\(\\theta = (W_1, W_2)\\). But themixture-of-expertsapproach is to: Example. Consider \\(d = 2\\) and each expert being a linear classifier (source): Training. We can learn a mixture-of-experts model by normal backpropagation. Applying the product rule yields: Notice that the gradient is proportional to \\(g_e(x)\\) and updates both the gating function and the experts. Saving compute. Balancing experts. Parallelism.  We define thetop-2 expertsapproximate gating function as follows: Notation: Balancing experts. For example, we can take \\(\\lambda = \\frac{0.01}{B}\\). Example. Here is an example with \\(B = 2\\) tokens and \\(E = 4\\) experts: The counter would be: We would try to push down on the gating function on expert 2 to discourage its use. Experimental setup:  BASE requires more compute to optimize the assignment \\(a\\), but is more stable. Summary and next steps. Specification: Other upgrades: Results:  Results on WinoGender: Setup:  Results onStereoSet: Motivation: Main considerations: Distributed hash tables:  Experiments from the paper: Diskin et al., 2021:",
            "We now turn to another class of language models,retrieval-based(or retrieval-augmented, memory-augmented models), that can help us push past the scaling ceiling of a dense Transformer. Encoder-decoder. Let us first focus on sequence-to-sequence tasks using an encoder-decoder framework: Example (open-book question answering): Recall thatBARTandT5are examples of encoder-decoder models: that are trained on denoising objectives; for example: Retrieval. Let us assume that we have astore\\(S\\), which is a set of sequences (usually, documents or passages). Intuitively, a retrieval-based model generates: Example (open-book question answering): Nearest neighborsas a special case:  Formally, the RAG-Sequence model is defined as follows: In practice, the summation \\(z \\in S\\) is replaced by thetop-k(analogous to choosing the top 1 or 2 experts for mixture of experts). Retriever: Dense Passage Retrieval (DPR)(Karpukhin et al., 2020). Generator. Training. Experiments.  Results:",
            "",
            "Mixture of experts: Retrieval-based models:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#generalist-language-model-glam-du-et-al-2021": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#generalist-language-model-glam-du-et-al-2021": ""
        },
        "text_between_h2": [
            "Basics. The idea of mixture of experts goes back toJacobs et al. (1991).  To introduce the basic idea, suppose we are solving a prediction problem: Let us start out by learning a feedforward (ReLU) neural network: where the parameters are \\(\\theta = (W_1, W_2)\\). But themixture-of-expertsapproach is to: Example. Consider \\(d = 2\\) and each expert being a linear classifier (source): Training. We can learn a mixture-of-experts model by normal backpropagation. Applying the product rule yields: Notice that the gradient is proportional to \\(g_e(x)\\) and updates both the gating function and the experts. Saving compute. Balancing experts. Parallelism.  We define thetop-2 expertsapproximate gating function as follows: Notation: Balancing experts. For example, we can take \\(\\lambda = \\frac{0.01}{B}\\). Example. Here is an example with \\(B = 2\\) tokens and \\(E = 4\\) experts: The counter would be: We would try to push down on the gating function on expert 2 to discourage its use. Experimental setup:  BASE requires more compute to optimize the assignment \\(a\\), but is more stable. Summary and next steps. Specification: Other upgrades: Results:  Results on WinoGender: Setup:  Results onStereoSet: Motivation: Main considerations: Distributed hash tables:  Experiments from the paper: Diskin et al., 2021:",
            "We now turn to another class of language models,retrieval-based(or retrieval-augmented, memory-augmented models), that can help us push past the scaling ceiling of a dense Transformer. Encoder-decoder. Let us first focus on sequence-to-sequence tasks using an encoder-decoder framework: Example (open-book question answering): Recall thatBARTandT5are examples of encoder-decoder models: that are trained on denoising objectives; for example: Retrieval. Let us assume that we have astore\\(S\\), which is a set of sequences (usually, documents or passages). Intuitively, a retrieval-based model generates: Example (open-book question answering): Nearest neighborsas a special case:  Formally, the RAG-Sequence model is defined as follows: In practice, the summation \\(z \\in S\\) is replaced by thetop-k(analogous to choosing the top 1 or 2 experts for mixture of experts). Retriever: Dense Passage Retrieval (DPR)(Karpukhin et al., 2020). Generator. Training. Experiments.  Results:",
            "",
            "Mixture of experts: Retrieval-based models:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#facebookmoe-artetxe-et-al-2021": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#facebookmoe-artetxe-et-al-2021": ""
        },
        "text_between_h2": [
            "Basics. The idea of mixture of experts goes back toJacobs et al. (1991).  To introduce the basic idea, suppose we are solving a prediction problem: Let us start out by learning a feedforward (ReLU) neural network: where the parameters are \\(\\theta = (W_1, W_2)\\). But themixture-of-expertsapproach is to: Example. Consider \\(d = 2\\) and each expert being a linear classifier (source): Training. We can learn a mixture-of-experts model by normal backpropagation. Applying the product rule yields: Notice that the gradient is proportional to \\(g_e(x)\\) and updates both the gating function and the experts. Saving compute. Balancing experts. Parallelism.  We define thetop-2 expertsapproximate gating function as follows: Notation: Balancing experts. For example, we can take \\(\\lambda = \\frac{0.01}{B}\\). Example. Here is an example with \\(B = 2\\) tokens and \\(E = 4\\) experts: The counter would be: We would try to push down on the gating function on expert 2 to discourage its use. Experimental setup:  BASE requires more compute to optimize the assignment \\(a\\), but is more stable. Summary and next steps. Specification: Other upgrades: Results:  Results on WinoGender: Setup:  Results onStereoSet: Motivation: Main considerations: Distributed hash tables:  Experiments from the paper: Diskin et al., 2021:",
            "We now turn to another class of language models,retrieval-based(or retrieval-augmented, memory-augmented models), that can help us push past the scaling ceiling of a dense Transformer. Encoder-decoder. Let us first focus on sequence-to-sequence tasks using an encoder-decoder framework: Example (open-book question answering): Recall thatBARTandT5are examples of encoder-decoder models: that are trained on denoising objectives; for example: Retrieval. Let us assume that we have astore\\(S\\), which is a set of sequences (usually, documents or passages). Intuitively, a retrieval-based model generates: Example (open-book question answering): Nearest neighborsas a special case:  Formally, the RAG-Sequence model is defined as follows: In practice, the summation \\(z \\in S\\) is replaced by thetop-k(analogous to choosing the top 1 or 2 experts for mixture of experts). Retriever: Dense Passage Retrieval (DPR)(Karpukhin et al., 2020). Generator. Training. Experiments.  Results:",
            "",
            "Mixture of experts: Retrieval-based models:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#decentralized-mixture-of-experts-ryabinin--gusev-2020": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#decentralized-mixture-of-experts-ryabinin--gusev-2020": ""
        },
        "text_between_h2": [
            "Basics. The idea of mixture of experts goes back toJacobs et al. (1991).  To introduce the basic idea, suppose we are solving a prediction problem: Let us start out by learning a feedforward (ReLU) neural network: where the parameters are \\(\\theta = (W_1, W_2)\\). But themixture-of-expertsapproach is to: Example. Consider \\(d = 2\\) and each expert being a linear classifier (source): Training. We can learn a mixture-of-experts model by normal backpropagation. Applying the product rule yields: Notice that the gradient is proportional to \\(g_e(x)\\) and updates both the gating function and the experts. Saving compute. Balancing experts. Parallelism.  We define thetop-2 expertsapproximate gating function as follows: Notation: Balancing experts. For example, we can take \\(\\lambda = \\frac{0.01}{B}\\). Example. Here is an example with \\(B = 2\\) tokens and \\(E = 4\\) experts: The counter would be: We would try to push down on the gating function on expert 2 to discourage its use. Experimental setup:  BASE requires more compute to optimize the assignment \\(a\\), but is more stable. Summary and next steps. Specification: Other upgrades: Results:  Results on WinoGender: Setup:  Results onStereoSet: Motivation: Main considerations: Distributed hash tables:  Experiments from the paper: Diskin et al., 2021:",
            "We now turn to another class of language models,retrieval-based(or retrieval-augmented, memory-augmented models), that can help us push past the scaling ceiling of a dense Transformer. Encoder-decoder. Let us first focus on sequence-to-sequence tasks using an encoder-decoder framework: Example (open-book question answering): Recall thatBARTandT5are examples of encoder-decoder models: that are trained on denoising objectives; for example: Retrieval. Let us assume that we have astore\\(S\\), which is a set of sequences (usually, documents or passages). Intuitively, a retrieval-based model generates: Example (open-book question answering): Nearest neighborsas a special case:  Formally, the RAG-Sequence model is defined as follows: In practice, the summation \\(z \\in S\\) is replaced by thetop-k(analogous to choosing the top 1 or 2 experts for mixture of experts). Retriever: Dense Passage Retrieval (DPR)(Karpukhin et al., 2020). Generator. Training. Experiments.  Results:",
            "",
            "Mixture of experts: Retrieval-based models:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#summary": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#summary": "",
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#summary-1": ""
        },
        "text_between_h2": [
            "Basics. The idea of mixture of experts goes back toJacobs et al. (1991).  To introduce the basic idea, suppose we are solving a prediction problem: Let us start out by learning a feedforward (ReLU) neural network: where the parameters are \\(\\theta = (W_1, W_2)\\). But themixture-of-expertsapproach is to: Example. Consider \\(d = 2\\) and each expert being a linear classifier (source): Training. We can learn a mixture-of-experts model by normal backpropagation. Applying the product rule yields: Notice that the gradient is proportional to \\(g_e(x)\\) and updates both the gating function and the experts. Saving compute. Balancing experts. Parallelism.  We define thetop-2 expertsapproximate gating function as follows: Notation: Balancing experts. For example, we can take \\(\\lambda = \\frac{0.01}{B}\\). Example. Here is an example with \\(B = 2\\) tokens and \\(E = 4\\) experts: The counter would be: We would try to push down on the gating function on expert 2 to discourage its use. Experimental setup:  BASE requires more compute to optimize the assignment \\(a\\), but is more stable. Summary and next steps. Specification: Other upgrades: Results:  Results on WinoGender: Setup:  Results onStereoSet: Motivation: Main considerations: Distributed hash tables:  Experiments from the paper: Diskin et al., 2021:",
            "We now turn to another class of language models,retrieval-based(or retrieval-augmented, memory-augmented models), that can help us push past the scaling ceiling of a dense Transformer. Encoder-decoder. Let us first focus on sequence-to-sequence tasks using an encoder-decoder framework: Example (open-book question answering): Recall thatBARTandT5are examples of encoder-decoder models: that are trained on denoising objectives; for example: Retrieval. Let us assume that we have astore\\(S\\), which is a set of sequences (usually, documents or passages). Intuitively, a retrieval-based model generates: Example (open-book question answering): Nearest neighborsas a special case:  Formally, the RAG-Sequence model is defined as follows: In practice, the summation \\(z \\in S\\) is replaced by thetop-k(analogous to choosing the top 1 or 2 experts for mixture of experts). Retriever: Dense Passage Retrieval (DPR)(Karpukhin et al., 2020). Generator. Training. Experiments.  Results:",
            "",
            "Mixture of experts: Retrieval-based models:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#summary-1": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#summary-1": ""
        },
        "text_between_h2": [
            "Basics. The idea of mixture of experts goes back toJacobs et al. (1991).  To introduce the basic idea, suppose we are solving a prediction problem: Let us start out by learning a feedforward (ReLU) neural network: where the parameters are \\(\\theta = (W_1, W_2)\\). But themixture-of-expertsapproach is to: Example. Consider \\(d = 2\\) and each expert being a linear classifier (source): Training. We can learn a mixture-of-experts model by normal backpropagation. Applying the product rule yields: Notice that the gradient is proportional to \\(g_e(x)\\) and updates both the gating function and the experts. Saving compute. Balancing experts. Parallelism.  We define thetop-2 expertsapproximate gating function as follows: Notation: Balancing experts. For example, we can take \\(\\lambda = \\frac{0.01}{B}\\). Example. Here is an example with \\(B = 2\\) tokens and \\(E = 4\\) experts: The counter would be: We would try to push down on the gating function on expert 2 to discourage its use. Experimental setup:  BASE requires more compute to optimize the assignment \\(a\\), but is more stable. Summary and next steps. Specification: Other upgrades: Results:  Results on WinoGender: Setup:  Results onStereoSet: Motivation: Main considerations: Distributed hash tables:  Experiments from the paper: Diskin et al., 2021:",
            "We now turn to another class of language models,retrieval-based(or retrieval-augmented, memory-augmented models), that can help us push past the scaling ceiling of a dense Transformer. Encoder-decoder. Let us first focus on sequence-to-sequence tasks using an encoder-decoder framework: Example (open-book question answering): Recall thatBARTandT5are examples of encoder-decoder models: that are trained on denoising objectives; for example: Retrieval. Let us assume that we have astore\\(S\\), which is a set of sequences (usually, documents or passages). Intuitively, a retrieval-based model generates: Example (open-book question answering): Nearest neighborsas a special case:  Formally, the RAG-Sequence model is defined as follows: In practice, the summation \\(z \\in S\\) is replaced by thetop-k(analogous to choosing the top 1 or 2 experts for mixture of experts). Retriever: Dense Passage Retrieval (DPR)(Karpukhin et al., 2020). Generator. Training. Experiments.  Results:",
            "",
            "Mixture of experts: Retrieval-based models:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#retrieval-based-models": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#retrieval-based-models": ""
        },
        "text_between_h2": [
            "Basics. The idea of mixture of experts goes back toJacobs et al. (1991).  To introduce the basic idea, suppose we are solving a prediction problem: Let us start out by learning a feedforward (ReLU) neural network: where the parameters are \\(\\theta = (W_1, W_2)\\). But themixture-of-expertsapproach is to: Example. Consider \\(d = 2\\) and each expert being a linear classifier (source): Training. We can learn a mixture-of-experts model by normal backpropagation. Applying the product rule yields: Notice that the gradient is proportional to \\(g_e(x)\\) and updates both the gating function and the experts. Saving compute. Balancing experts. Parallelism.  We define thetop-2 expertsapproximate gating function as follows: Notation: Balancing experts. For example, we can take \\(\\lambda = \\frac{0.01}{B}\\). Example. Here is an example with \\(B = 2\\) tokens and \\(E = 4\\) experts: The counter would be: We would try to push down on the gating function on expert 2 to discourage its use. Experimental setup:  BASE requires more compute to optimize the assignment \\(a\\), but is more stable. Summary and next steps. Specification: Other upgrades: Results:  Results on WinoGender: Setup:  Results onStereoSet: Motivation: Main considerations: Distributed hash tables:  Experiments from the paper: Diskin et al., 2021:",
            "We now turn to another class of language models,retrieval-based(or retrieval-augmented, memory-augmented models), that can help us push past the scaling ceiling of a dense Transformer. Encoder-decoder. Let us first focus on sequence-to-sequence tasks using an encoder-decoder framework: Example (open-book question answering): Recall thatBARTandT5are examples of encoder-decoder models: that are trained on denoising objectives; for example: Retrieval. Let us assume that we have astore\\(S\\), which is a set of sequences (usually, documents or passages). Intuitively, a retrieval-based model generates: Example (open-book question answering): Nearest neighborsas a special case:  Formally, the RAG-Sequence model is defined as follows: In practice, the summation \\(z \\in S\\) is replaced by thetop-k(analogous to choosing the top 1 or 2 experts for mixture of experts). Retriever: Dense Passage Retrieval (DPR)(Karpukhin et al., 2020). Generator. Training. Experiments.  Results:",
            "",
            "Mixture of experts: Retrieval-based models:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#retrieval-augmented-generation-rag-lewis-et-al-2020": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#retrieval-augmented-generation-rag-lewis-et-al-2020": ""
        },
        "text_between_h2": [
            "Basics. The idea of mixture of experts goes back toJacobs et al. (1991).  To introduce the basic idea, suppose we are solving a prediction problem: Let us start out by learning a feedforward (ReLU) neural network: where the parameters are \\(\\theta = (W_1, W_2)\\). But themixture-of-expertsapproach is to: Example. Consider \\(d = 2\\) and each expert being a linear classifier (source): Training. We can learn a mixture-of-experts model by normal backpropagation. Applying the product rule yields: Notice that the gradient is proportional to \\(g_e(x)\\) and updates both the gating function and the experts. Saving compute. Balancing experts. Parallelism.  We define thetop-2 expertsapproximate gating function as follows: Notation: Balancing experts. For example, we can take \\(\\lambda = \\frac{0.01}{B}\\). Example. Here is an example with \\(B = 2\\) tokens and \\(E = 4\\) experts: The counter would be: We would try to push down on the gating function on expert 2 to discourage its use. Experimental setup:  BASE requires more compute to optimize the assignment \\(a\\), but is more stable. Summary and next steps. Specification: Other upgrades: Results:  Results on WinoGender: Setup:  Results onStereoSet: Motivation: Main considerations: Distributed hash tables:  Experiments from the paper: Diskin et al., 2021:",
            "We now turn to another class of language models,retrieval-based(or retrieval-augmented, memory-augmented models), that can help us push past the scaling ceiling of a dense Transformer. Encoder-decoder. Let us first focus on sequence-to-sequence tasks using an encoder-decoder framework: Example (open-book question answering): Recall thatBARTandT5are examples of encoder-decoder models: that are trained on denoising objectives; for example: Retrieval. Let us assume that we have astore\\(S\\), which is a set of sequences (usually, documents or passages). Intuitively, a retrieval-based model generates: Example (open-book question answering): Nearest neighborsas a special case:  Formally, the RAG-Sequence model is defined as follows: In practice, the summation \\(z \\in S\\) is replaced by thetop-k(analogous to choosing the top 1 or 2 experts for mixture of experts). Retriever: Dense Passage Retrieval (DPR)(Karpukhin et al., 2020). Generator. Training. Experiments.  Results:",
            "",
            "Mixture of experts: Retrieval-based models:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#retro-borgeaud-et-al-2021": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#retro-borgeaud-et-al-2021": ""
        },
        "text_between_h2": [
            "Basics. The idea of mixture of experts goes back toJacobs et al. (1991).  To introduce the basic idea, suppose we are solving a prediction problem: Let us start out by learning a feedforward (ReLU) neural network: where the parameters are \\(\\theta = (W_1, W_2)\\). But themixture-of-expertsapproach is to: Example. Consider \\(d = 2\\) and each expert being a linear classifier (source): Training. We can learn a mixture-of-experts model by normal backpropagation. Applying the product rule yields: Notice that the gradient is proportional to \\(g_e(x)\\) and updates both the gating function and the experts. Saving compute. Balancing experts. Parallelism.  We define thetop-2 expertsapproximate gating function as follows: Notation: Balancing experts. For example, we can take \\(\\lambda = \\frac{0.01}{B}\\). Example. Here is an example with \\(B = 2\\) tokens and \\(E = 4\\) experts: The counter would be: We would try to push down on the gating function on expert 2 to discourage its use. Experimental setup:  BASE requires more compute to optimize the assignment \\(a\\), but is more stable. Summary and next steps. Specification: Other upgrades: Results:  Results on WinoGender: Setup:  Results onStereoSet: Motivation: Main considerations: Distributed hash tables:  Experiments from the paper: Diskin et al., 2021:",
            "We now turn to another class of language models,retrieval-based(or retrieval-augmented, memory-augmented models), that can help us push past the scaling ceiling of a dense Transformer. Encoder-decoder. Let us first focus on sequence-to-sequence tasks using an encoder-decoder framework: Example (open-book question answering): Recall thatBARTandT5are examples of encoder-decoder models: that are trained on denoising objectives; for example: Retrieval. Let us assume that we have astore\\(S\\), which is a set of sequences (usually, documents or passages). Intuitively, a retrieval-based model generates: Example (open-book question answering): Nearest neighborsas a special case:  Formally, the RAG-Sequence model is defined as follows: In practice, the summation \\(z \\in S\\) is replaced by thetop-k(analogous to choosing the top 1 or 2 experts for mixture of experts). Retriever: Dense Passage Retrieval (DPR)(Karpukhin et al., 2020). Generator. Training. Experiments.  Results:",
            "",
            "Mixture of experts: Retrieval-based models:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#discussion": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#discussion": ""
        },
        "text_between_h2": [
            "Basics. The idea of mixture of experts goes back toJacobs et al. (1991).  To introduce the basic idea, suppose we are solving a prediction problem: Let us start out by learning a feedforward (ReLU) neural network: where the parameters are \\(\\theta = (W_1, W_2)\\). But themixture-of-expertsapproach is to: Example. Consider \\(d = 2\\) and each expert being a linear classifier (source): Training. We can learn a mixture-of-experts model by normal backpropagation. Applying the product rule yields: Notice that the gradient is proportional to \\(g_e(x)\\) and updates both the gating function and the experts. Saving compute. Balancing experts. Parallelism.  We define thetop-2 expertsapproximate gating function as follows: Notation: Balancing experts. For example, we can take \\(\\lambda = \\frac{0.01}{B}\\). Example. Here is an example with \\(B = 2\\) tokens and \\(E = 4\\) experts: The counter would be: We would try to push down on the gating function on expert 2 to discourage its use. Experimental setup:  BASE requires more compute to optimize the assignment \\(a\\), but is more stable. Summary and next steps. Specification: Other upgrades: Results:  Results on WinoGender: Setup:  Results onStereoSet: Motivation: Main considerations: Distributed hash tables:  Experiments from the paper: Diskin et al., 2021:",
            "We now turn to another class of language models,retrieval-based(or retrieval-augmented, memory-augmented models), that can help us push past the scaling ceiling of a dense Transformer. Encoder-decoder. Let us first focus on sequence-to-sequence tasks using an encoder-decoder framework: Example (open-book question answering): Recall thatBARTandT5are examples of encoder-decoder models: that are trained on denoising objectives; for example: Retrieval. Let us assume that we have astore\\(S\\), which is a set of sequences (usually, documents or passages). Intuitively, a retrieval-based model generates: Example (open-book question answering): Nearest neighborsas a special case:  Formally, the RAG-Sequence model is defined as follows: In practice, the summation \\(z \\in S\\) is replaced by thetop-k(analogous to choosing the top 1 or 2 experts for mixture of experts). Retriever: Dense Passage Retrieval (DPR)(Karpukhin et al., 2020). Generator. Training. Experiments.  Results:",
            "",
            "Mixture of experts: Retrieval-based models:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#further-reading": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/selective-architectures/#further-reading": ""
        },
        "text_between_h2": [
            "Basics. The idea of mixture of experts goes back toJacobs et al. (1991).  To introduce the basic idea, suppose we are solving a prediction problem: Let us start out by learning a feedforward (ReLU) neural network: where the parameters are \\(\\theta = (W_1, W_2)\\). But themixture-of-expertsapproach is to: Example. Consider \\(d = 2\\) and each expert being a linear classifier (source): Training. We can learn a mixture-of-experts model by normal backpropagation. Applying the product rule yields: Notice that the gradient is proportional to \\(g_e(x)\\) and updates both the gating function and the experts. Saving compute. Balancing experts. Parallelism.  We define thetop-2 expertsapproximate gating function as follows: Notation: Balancing experts. For example, we can take \\(\\lambda = \\frac{0.01}{B}\\). Example. Here is an example with \\(B = 2\\) tokens and \\(E = 4\\) experts: The counter would be: We would try to push down on the gating function on expert 2 to discourage its use. Experimental setup:  BASE requires more compute to optimize the assignment \\(a\\), but is more stable. Summary and next steps. Specification: Other upgrades: Results:  Results on WinoGender: Setup:  Results onStereoSet: Motivation: Main considerations: Distributed hash tables:  Experiments from the paper: Diskin et al., 2021:",
            "We now turn to another class of language models,retrieval-based(or retrieval-augmented, memory-augmented models), that can help us push past the scaling ceiling of a dense Transformer. Encoder-decoder. Let us first focus on sequence-to-sequence tasks using an encoder-decoder framework: Example (open-book question answering): Recall thatBARTandT5are examples of encoder-decoder models: that are trained on denoising objectives; for example: Retrieval. Let us assume that we have astore\\(S\\), which is a set of sequences (usually, documents or passages). Intuitively, a retrieval-based model generates: Example (open-book question answering): Nearest neighborsas a special case:  Formally, the RAG-Sequence model is defined as follows: In practice, the summation \\(z \\in S\\) is replaced by thetop-k(analogous to choosing the top 1 or 2 experts for mixture of experts). Retriever: Dense Passage Retrieval (DPR)(Karpukhin et al., 2020). Generator. Training. Experiments.  Results:",
            "",
            "Mixture of experts: Retrieval-based models:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/adaptation/": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/": "Adaptation",
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#why-adapt-the-language-model": "",
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#probing": "",
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#fine-tuning": "",
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#lightweight-fine-tuning": "",
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#ways-downstream-tasks-can-be-different": "",
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#general-adaptation-setup": "",
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#strategies-for-fixed-length-representations": "",
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#summary": "",
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#fine-tuning-for-zero-shot-performance": "",
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#fine-tuning-for-human-aligned-language-models": "",
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#summary-1": "",
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#prompt-tuning-lester-et-al-2021-": "",
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#prefix-tuning-li-and-liang-2021-": "",
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#adapter-tuning-houlsby-et-al-2019": "",
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#parallelization-over-prefixesprompts": "",
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#robustness-of-lightweight-fine-tuning": "",
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#summary-2": "",
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#overall-summary": "",
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#further-reading": ""
        },
        "text_between_h2": [
            "The format of such a task may not be very natural for the model.",
            "",
            "A Transformer encoder maps a sequence of \\(L\\) tokens to \\(L\\) embeddings. Many tasks (e.g., classification) have fixed-length outputs. How do we get 1 embedding vector from a Transformer encoder?",
            "",
            "where for self-attention, we set \\(L'=L\\) and define \\(K = W_{\\text{key}}\\x\\), \\(V = W_{\\text{value}}\\x\\) and \\(Q = W_{\\text{query}}\\x\\), where \\(W_{\\text{key}}, W_{ \\text{value}}, W_{\\text{query}}\\) are learned weight matrices. where \\(Q = W_{\\text{query}}\\x\\) as in regular self-attention. Trainable parameters at all layers helps where \\(W_{\\text{down}}\\in \\R^{r \\times d}\\) and \\(W_{\\text{up}}\\in \\R^{d \\times r}\\) are learned weights that project \\(x\\) down to a bottleneck dimension \\(r\\) and back up to dimension \\(d\\), and \\(\\sigma\\) is a non-linear activation function. The result \\(\\text{Adapter}(x)\\) is a vector in \\(\\R^d\\) with the same dimensionality as \\(x\\).",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#why-adapt-the-language-model": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#why-adapt-the-language-model": ""
        },
        "text_between_h2": [
            "The format of such a task may not be very natural for the model.",
            "",
            "A Transformer encoder maps a sequence of \\(L\\) tokens to \\(L\\) embeddings. Many tasks (e.g., classification) have fixed-length outputs. How do we get 1 embedding vector from a Transformer encoder?",
            "",
            "where for self-attention, we set \\(L'=L\\) and define \\(K = W_{\\text{key}}\\x\\), \\(V = W_{\\text{value}}\\x\\) and \\(Q = W_{\\text{query}}\\x\\), where \\(W_{\\text{key}}, W_{ \\text{value}}, W_{\\text{query}}\\) are learned weight matrices. where \\(Q = W_{\\text{query}}\\x\\) as in regular self-attention. Trainable parameters at all layers helps where \\(W_{\\text{down}}\\in \\R^{r \\times d}\\) and \\(W_{\\text{up}}\\in \\R^{d \\times r}\\) are learned weights that project \\(x\\) down to a bottleneck dimension \\(r\\) and back up to dimension \\(d\\), and \\(\\sigma\\) is a non-linear activation function. The result \\(\\text{Adapter}(x)\\) is a vector in \\(\\R^d\\) with the same dimensionality as \\(x\\).",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#probing": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#probing": ""
        },
        "text_between_h2": [
            "The format of such a task may not be very natural for the model.",
            "",
            "A Transformer encoder maps a sequence of \\(L\\) tokens to \\(L\\) embeddings. Many tasks (e.g., classification) have fixed-length outputs. How do we get 1 embedding vector from a Transformer encoder?",
            "",
            "where for self-attention, we set \\(L'=L\\) and define \\(K = W_{\\text{key}}\\x\\), \\(V = W_{\\text{value}}\\x\\) and \\(Q = W_{\\text{query}}\\x\\), where \\(W_{\\text{key}}, W_{ \\text{value}}, W_{\\text{query}}\\) are learned weight matrices. where \\(Q = W_{\\text{query}}\\x\\) as in regular self-attention. Trainable parameters at all layers helps where \\(W_{\\text{down}}\\in \\R^{r \\times d}\\) and \\(W_{\\text{up}}\\in \\R^{d \\times r}\\) are learned weights that project \\(x\\) down to a bottleneck dimension \\(r\\) and back up to dimension \\(d\\), and \\(\\sigma\\) is a non-linear activation function. The result \\(\\text{Adapter}(x)\\) is a vector in \\(\\R^d\\) with the same dimensionality as \\(x\\).",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#fine-tuning": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#fine-tuning": "",
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#fine-tuning-for-zero-shot-performance": "",
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#fine-tuning-for-human-aligned-language-models": ""
        },
        "text_between_h2": [
            "The format of such a task may not be very natural for the model.",
            "",
            "A Transformer encoder maps a sequence of \\(L\\) tokens to \\(L\\) embeddings. Many tasks (e.g., classification) have fixed-length outputs. How do we get 1 embedding vector from a Transformer encoder?",
            "",
            "where for self-attention, we set \\(L'=L\\) and define \\(K = W_{\\text{key}}\\x\\), \\(V = W_{\\text{value}}\\x\\) and \\(Q = W_{\\text{query}}\\x\\), where \\(W_{\\text{key}}, W_{ \\text{value}}, W_{\\text{query}}\\) are learned weight matrices. where \\(Q = W_{\\text{query}}\\x\\) as in regular self-attention. Trainable parameters at all layers helps where \\(W_{\\text{down}}\\in \\R^{r \\times d}\\) and \\(W_{\\text{up}}\\in \\R^{d \\times r}\\) are learned weights that project \\(x\\) down to a bottleneck dimension \\(r\\) and back up to dimension \\(d\\), and \\(\\sigma\\) is a non-linear activation function. The result \\(\\text{Adapter}(x)\\) is a vector in \\(\\R^d\\) with the same dimensionality as \\(x\\).",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#fine-tuning-for-zero-shot-performance": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#fine-tuning-for-zero-shot-performance": ""
        },
        "text_between_h2": [
            "The format of such a task may not be very natural for the model.",
            "",
            "A Transformer encoder maps a sequence of \\(L\\) tokens to \\(L\\) embeddings. Many tasks (e.g., classification) have fixed-length outputs. How do we get 1 embedding vector from a Transformer encoder?",
            "",
            "where for self-attention, we set \\(L'=L\\) and define \\(K = W_{\\text{key}}\\x\\), \\(V = W_{\\text{value}}\\x\\) and \\(Q = W_{\\text{query}}\\x\\), where \\(W_{\\text{key}}, W_{ \\text{value}}, W_{\\text{query}}\\) are learned weight matrices. where \\(Q = W_{\\text{query}}\\x\\) as in regular self-attention. Trainable parameters at all layers helps where \\(W_{\\text{down}}\\in \\R^{r \\times d}\\) and \\(W_{\\text{up}}\\in \\R^{d \\times r}\\) are learned weights that project \\(x\\) down to a bottleneck dimension \\(r\\) and back up to dimension \\(d\\), and \\(\\sigma\\) is a non-linear activation function. The result \\(\\text{Adapter}(x)\\) is a vector in \\(\\R^d\\) with the same dimensionality as \\(x\\).",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#fine-tuning-for-human-aligned-language-models": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#fine-tuning-for-human-aligned-language-models": ""
        },
        "text_between_h2": [
            "The format of such a task may not be very natural for the model.",
            "",
            "A Transformer encoder maps a sequence of \\(L\\) tokens to \\(L\\) embeddings. Many tasks (e.g., classification) have fixed-length outputs. How do we get 1 embedding vector from a Transformer encoder?",
            "",
            "where for self-attention, we set \\(L'=L\\) and define \\(K = W_{\\text{key}}\\x\\), \\(V = W_{\\text{value}}\\x\\) and \\(Q = W_{\\text{query}}\\x\\), where \\(W_{\\text{key}}, W_{ \\text{value}}, W_{\\text{query}}\\) are learned weight matrices. where \\(Q = W_{\\text{query}}\\x\\) as in regular self-attention. Trainable parameters at all layers helps where \\(W_{\\text{down}}\\in \\R^{r \\times d}\\) and \\(W_{\\text{up}}\\in \\R^{d \\times r}\\) are learned weights that project \\(x\\) down to a bottleneck dimension \\(r\\) and back up to dimension \\(d\\), and \\(\\sigma\\) is a non-linear activation function. The result \\(\\text{Adapter}(x)\\) is a vector in \\(\\R^d\\) with the same dimensionality as \\(x\\).",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#lightweight-fine-tuning": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#lightweight-fine-tuning": ""
        },
        "text_between_h2": [
            "The format of such a task may not be very natural for the model.",
            "",
            "A Transformer encoder maps a sequence of \\(L\\) tokens to \\(L\\) embeddings. Many tasks (e.g., classification) have fixed-length outputs. How do we get 1 embedding vector from a Transformer encoder?",
            "",
            "where for self-attention, we set \\(L'=L\\) and define \\(K = W_{\\text{key}}\\x\\), \\(V = W_{\\text{value}}\\x\\) and \\(Q = W_{\\text{query}}\\x\\), where \\(W_{\\text{key}}, W_{ \\text{value}}, W_{\\text{query}}\\) are learned weight matrices. where \\(Q = W_{\\text{query}}\\x\\) as in regular self-attention. Trainable parameters at all layers helps where \\(W_{\\text{down}}\\in \\R^{r \\times d}\\) and \\(W_{\\text{up}}\\in \\R^{d \\times r}\\) are learned weights that project \\(x\\) down to a bottleneck dimension \\(r\\) and back up to dimension \\(d\\), and \\(\\sigma\\) is a non-linear activation function. The result \\(\\text{Adapter}(x)\\) is a vector in \\(\\R^d\\) with the same dimensionality as \\(x\\).",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#ways-downstream-tasks-can-be-different": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#ways-downstream-tasks-can-be-different": ""
        },
        "text_between_h2": [
            "The format of such a task may not be very natural for the model.",
            "",
            "A Transformer encoder maps a sequence of \\(L\\) tokens to \\(L\\) embeddings. Many tasks (e.g., classification) have fixed-length outputs. How do we get 1 embedding vector from a Transformer encoder?",
            "",
            "where for self-attention, we set \\(L'=L\\) and define \\(K = W_{\\text{key}}\\x\\), \\(V = W_{\\text{value}}\\x\\) and \\(Q = W_{\\text{query}}\\x\\), where \\(W_{\\text{key}}, W_{ \\text{value}}, W_{\\text{query}}\\) are learned weight matrices. where \\(Q = W_{\\text{query}}\\x\\) as in regular self-attention. Trainable parameters at all layers helps where \\(W_{\\text{down}}\\in \\R^{r \\times d}\\) and \\(W_{\\text{up}}\\in \\R^{d \\times r}\\) are learned weights that project \\(x\\) down to a bottleneck dimension \\(r\\) and back up to dimension \\(d\\), and \\(\\sigma\\) is a non-linear activation function. The result \\(\\text{Adapter}(x)\\) is a vector in \\(\\R^d\\) with the same dimensionality as \\(x\\).",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#general-adaptation-setup": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#general-adaptation-setup": ""
        },
        "text_between_h2": [
            "The format of such a task may not be very natural for the model.",
            "",
            "A Transformer encoder maps a sequence of \\(L\\) tokens to \\(L\\) embeddings. Many tasks (e.g., classification) have fixed-length outputs. How do we get 1 embedding vector from a Transformer encoder?",
            "",
            "where for self-attention, we set \\(L'=L\\) and define \\(K = W_{\\text{key}}\\x\\), \\(V = W_{\\text{value}}\\x\\) and \\(Q = W_{\\text{query}}\\x\\), where \\(W_{\\text{key}}, W_{ \\text{value}}, W_{\\text{query}}\\) are learned weight matrices. where \\(Q = W_{\\text{query}}\\x\\) as in regular self-attention. Trainable parameters at all layers helps where \\(W_{\\text{down}}\\in \\R^{r \\times d}\\) and \\(W_{\\text{up}}\\in \\R^{d \\times r}\\) are learned weights that project \\(x\\) down to a bottleneck dimension \\(r\\) and back up to dimension \\(d\\), and \\(\\sigma\\) is a non-linear activation function. The result \\(\\text{Adapter}(x)\\) is a vector in \\(\\R^d\\) with the same dimensionality as \\(x\\).",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#strategies-for-fixed-length-representations": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#strategies-for-fixed-length-representations": ""
        },
        "text_between_h2": [
            "The format of such a task may not be very natural for the model.",
            "",
            "A Transformer encoder maps a sequence of \\(L\\) tokens to \\(L\\) embeddings. Many tasks (e.g., classification) have fixed-length outputs. How do we get 1 embedding vector from a Transformer encoder?",
            "",
            "where for self-attention, we set \\(L'=L\\) and define \\(K = W_{\\text{key}}\\x\\), \\(V = W_{\\text{value}}\\x\\) and \\(Q = W_{\\text{query}}\\x\\), where \\(W_{\\text{key}}, W_{ \\text{value}}, W_{\\text{query}}\\) are learned weight matrices. where \\(Q = W_{\\text{query}}\\x\\) as in regular self-attention. Trainable parameters at all layers helps where \\(W_{\\text{down}}\\in \\R^{r \\times d}\\) and \\(W_{\\text{up}}\\in \\R^{d \\times r}\\) are learned weights that project \\(x\\) down to a bottleneck dimension \\(r\\) and back up to dimension \\(d\\), and \\(\\sigma\\) is a non-linear activation function. The result \\(\\text{Adapter}(x)\\) is a vector in \\(\\R^d\\) with the same dimensionality as \\(x\\).",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#summary": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#summary": "",
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#summary-1": "",
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#summary-2": ""
        },
        "text_between_h2": [
            "The format of such a task may not be very natural for the model.",
            "",
            "A Transformer encoder maps a sequence of \\(L\\) tokens to \\(L\\) embeddings. Many tasks (e.g., classification) have fixed-length outputs. How do we get 1 embedding vector from a Transformer encoder?",
            "",
            "where for self-attention, we set \\(L'=L\\) and define \\(K = W_{\\text{key}}\\x\\), \\(V = W_{\\text{value}}\\x\\) and \\(Q = W_{\\text{query}}\\x\\), where \\(W_{\\text{key}}, W_{ \\text{value}}, W_{\\text{query}}\\) are learned weight matrices. where \\(Q = W_{\\text{query}}\\x\\) as in regular self-attention. Trainable parameters at all layers helps where \\(W_{\\text{down}}\\in \\R^{r \\times d}\\) and \\(W_{\\text{up}}\\in \\R^{d \\times r}\\) are learned weights that project \\(x\\) down to a bottleneck dimension \\(r\\) and back up to dimension \\(d\\), and \\(\\sigma\\) is a non-linear activation function. The result \\(\\text{Adapter}(x)\\) is a vector in \\(\\R^d\\) with the same dimensionality as \\(x\\).",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#summary-1": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#summary-1": ""
        },
        "text_between_h2": [
            "The format of such a task may not be very natural for the model.",
            "",
            "A Transformer encoder maps a sequence of \\(L\\) tokens to \\(L\\) embeddings. Many tasks (e.g., classification) have fixed-length outputs. How do we get 1 embedding vector from a Transformer encoder?",
            "",
            "where for self-attention, we set \\(L'=L\\) and define \\(K = W_{\\text{key}}\\x\\), \\(V = W_{\\text{value}}\\x\\) and \\(Q = W_{\\text{query}}\\x\\), where \\(W_{\\text{key}}, W_{ \\text{value}}, W_{\\text{query}}\\) are learned weight matrices. where \\(Q = W_{\\text{query}}\\x\\) as in regular self-attention. Trainable parameters at all layers helps where \\(W_{\\text{down}}\\in \\R^{r \\times d}\\) and \\(W_{\\text{up}}\\in \\R^{d \\times r}\\) are learned weights that project \\(x\\) down to a bottleneck dimension \\(r\\) and back up to dimension \\(d\\), and \\(\\sigma\\) is a non-linear activation function. The result \\(\\text{Adapter}(x)\\) is a vector in \\(\\R^d\\) with the same dimensionality as \\(x\\).",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#summary-2": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#summary-2": ""
        },
        "text_between_h2": [
            "The format of such a task may not be very natural for the model.",
            "",
            "A Transformer encoder maps a sequence of \\(L\\) tokens to \\(L\\) embeddings. Many tasks (e.g., classification) have fixed-length outputs. How do we get 1 embedding vector from a Transformer encoder?",
            "",
            "where for self-attention, we set \\(L'=L\\) and define \\(K = W_{\\text{key}}\\x\\), \\(V = W_{\\text{value}}\\x\\) and \\(Q = W_{\\text{query}}\\x\\), where \\(W_{\\text{key}}, W_{ \\text{value}}, W_{\\text{query}}\\) are learned weight matrices. where \\(Q = W_{\\text{query}}\\x\\) as in regular self-attention. Trainable parameters at all layers helps where \\(W_{\\text{down}}\\in \\R^{r \\times d}\\) and \\(W_{\\text{up}}\\in \\R^{d \\times r}\\) are learned weights that project \\(x\\) down to a bottleneck dimension \\(r\\) and back up to dimension \\(d\\), and \\(\\sigma\\) is a non-linear activation function. The result \\(\\text{Adapter}(x)\\) is a vector in \\(\\R^d\\) with the same dimensionality as \\(x\\).",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#prompt-tuning-lester-et-al-2021-": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#prompt-tuning-lester-et-al-2021-": ""
        },
        "text_between_h2": [
            "The format of such a task may not be very natural for the model.",
            "",
            "A Transformer encoder maps a sequence of \\(L\\) tokens to \\(L\\) embeddings. Many tasks (e.g., classification) have fixed-length outputs. How do we get 1 embedding vector from a Transformer encoder?",
            "",
            "where for self-attention, we set \\(L'=L\\) and define \\(K = W_{\\text{key}}\\x\\), \\(V = W_{\\text{value}}\\x\\) and \\(Q = W_{\\text{query}}\\x\\), where \\(W_{\\text{key}}, W_{ \\text{value}}, W_{\\text{query}}\\) are learned weight matrices. where \\(Q = W_{\\text{query}}\\x\\) as in regular self-attention. Trainable parameters at all layers helps where \\(W_{\\text{down}}\\in \\R^{r \\times d}\\) and \\(W_{\\text{up}}\\in \\R^{d \\times r}\\) are learned weights that project \\(x\\) down to a bottleneck dimension \\(r\\) and back up to dimension \\(d\\), and \\(\\sigma\\) is a non-linear activation function. The result \\(\\text{Adapter}(x)\\) is a vector in \\(\\R^d\\) with the same dimensionality as \\(x\\).",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#prefix-tuning-li-and-liang-2021-": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#prefix-tuning-li-and-liang-2021-": ""
        },
        "text_between_h2": [
            "The format of such a task may not be very natural for the model.",
            "",
            "A Transformer encoder maps a sequence of \\(L\\) tokens to \\(L\\) embeddings. Many tasks (e.g., classification) have fixed-length outputs. How do we get 1 embedding vector from a Transformer encoder?",
            "",
            "where for self-attention, we set \\(L'=L\\) and define \\(K = W_{\\text{key}}\\x\\), \\(V = W_{\\text{value}}\\x\\) and \\(Q = W_{\\text{query}}\\x\\), where \\(W_{\\text{key}}, W_{ \\text{value}}, W_{\\text{query}}\\) are learned weight matrices. where \\(Q = W_{\\text{query}}\\x\\) as in regular self-attention. Trainable parameters at all layers helps where \\(W_{\\text{down}}\\in \\R^{r \\times d}\\) and \\(W_{\\text{up}}\\in \\R^{d \\times r}\\) are learned weights that project \\(x\\) down to a bottleneck dimension \\(r\\) and back up to dimension \\(d\\), and \\(\\sigma\\) is a non-linear activation function. The result \\(\\text{Adapter}(x)\\) is a vector in \\(\\R^d\\) with the same dimensionality as \\(x\\).",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#adapter-tuning-houlsby-et-al-2019": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#adapter-tuning-houlsby-et-al-2019": ""
        },
        "text_between_h2": [
            "The format of such a task may not be very natural for the model.",
            "",
            "A Transformer encoder maps a sequence of \\(L\\) tokens to \\(L\\) embeddings. Many tasks (e.g., classification) have fixed-length outputs. How do we get 1 embedding vector from a Transformer encoder?",
            "",
            "where for self-attention, we set \\(L'=L\\) and define \\(K = W_{\\text{key}}\\x\\), \\(V = W_{\\text{value}}\\x\\) and \\(Q = W_{\\text{query}}\\x\\), where \\(W_{\\text{key}}, W_{ \\text{value}}, W_{\\text{query}}\\) are learned weight matrices. where \\(Q = W_{\\text{query}}\\x\\) as in regular self-attention. Trainable parameters at all layers helps where \\(W_{\\text{down}}\\in \\R^{r \\times d}\\) and \\(W_{\\text{up}}\\in \\R^{d \\times r}\\) are learned weights that project \\(x\\) down to a bottleneck dimension \\(r\\) and back up to dimension \\(d\\), and \\(\\sigma\\) is a non-linear activation function. The result \\(\\text{Adapter}(x)\\) is a vector in \\(\\R^d\\) with the same dimensionality as \\(x\\).",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#parallelization-over-prefixesprompts": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#parallelization-over-prefixesprompts": ""
        },
        "text_between_h2": [
            "The format of such a task may not be very natural for the model.",
            "",
            "A Transformer encoder maps a sequence of \\(L\\) tokens to \\(L\\) embeddings. Many tasks (e.g., classification) have fixed-length outputs. How do we get 1 embedding vector from a Transformer encoder?",
            "",
            "where for self-attention, we set \\(L'=L\\) and define \\(K = W_{\\text{key}}\\x\\), \\(V = W_{\\text{value}}\\x\\) and \\(Q = W_{\\text{query}}\\x\\), where \\(W_{\\text{key}}, W_{ \\text{value}}, W_{\\text{query}}\\) are learned weight matrices. where \\(Q = W_{\\text{query}}\\x\\) as in regular self-attention. Trainable parameters at all layers helps where \\(W_{\\text{down}}\\in \\R^{r \\times d}\\) and \\(W_{\\text{up}}\\in \\R^{d \\times r}\\) are learned weights that project \\(x\\) down to a bottleneck dimension \\(r\\) and back up to dimension \\(d\\), and \\(\\sigma\\) is a non-linear activation function. The result \\(\\text{Adapter}(x)\\) is a vector in \\(\\R^d\\) with the same dimensionality as \\(x\\).",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#robustness-of-lightweight-fine-tuning": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#robustness-of-lightweight-fine-tuning": ""
        },
        "text_between_h2": [
            "The format of such a task may not be very natural for the model.",
            "",
            "A Transformer encoder maps a sequence of \\(L\\) tokens to \\(L\\) embeddings. Many tasks (e.g., classification) have fixed-length outputs. How do we get 1 embedding vector from a Transformer encoder?",
            "",
            "where for self-attention, we set \\(L'=L\\) and define \\(K = W_{\\text{key}}\\x\\), \\(V = W_{\\text{value}}\\x\\) and \\(Q = W_{\\text{query}}\\x\\), where \\(W_{\\text{key}}, W_{ \\text{value}}, W_{\\text{query}}\\) are learned weight matrices. where \\(Q = W_{\\text{query}}\\x\\) as in regular self-attention. Trainable parameters at all layers helps where \\(W_{\\text{down}}\\in \\R^{r \\times d}\\) and \\(W_{\\text{up}}\\in \\R^{d \\times r}\\) are learned weights that project \\(x\\) down to a bottleneck dimension \\(r\\) and back up to dimension \\(d\\), and \\(\\sigma\\) is a non-linear activation function. The result \\(\\text{Adapter}(x)\\) is a vector in \\(\\R^d\\) with the same dimensionality as \\(x\\).",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#overall-summary": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#overall-summary": ""
        },
        "text_between_h2": [
            "The format of such a task may not be very natural for the model.",
            "",
            "A Transformer encoder maps a sequence of \\(L\\) tokens to \\(L\\) embeddings. Many tasks (e.g., classification) have fixed-length outputs. How do we get 1 embedding vector from a Transformer encoder?",
            "",
            "where for self-attention, we set \\(L'=L\\) and define \\(K = W_{\\text{key}}\\x\\), \\(V = W_{\\text{value}}\\x\\) and \\(Q = W_{\\text{query}}\\x\\), where \\(W_{\\text{key}}, W_{ \\text{value}}, W_{\\text{query}}\\) are learned weight matrices. where \\(Q = W_{\\text{query}}\\x\\) as in regular self-attention. Trainable parameters at all layers helps where \\(W_{\\text{down}}\\in \\R^{r \\times d}\\) and \\(W_{\\text{up}}\\in \\R^{d \\times r}\\) are learned weights that project \\(x\\) down to a bottleneck dimension \\(r\\) and back up to dimension \\(d\\), and \\(\\sigma\\) is a non-linear activation function. The result \\(\\text{Adapter}(x)\\) is a vector in \\(\\R^d\\) with the same dimensionality as \\(x\\).",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#further-reading": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/adaptation/#further-reading": ""
        },
        "text_between_h2": [
            "The format of such a task may not be very natural for the model.",
            "",
            "A Transformer encoder maps a sequence of \\(L\\) tokens to \\(L\\) embeddings. Many tasks (e.g., classification) have fixed-length outputs. How do we get 1 embedding vector from a Transformer encoder?",
            "",
            "where for self-attention, we set \\(L'=L\\) and define \\(K = W_{\\text{key}}\\x\\), \\(V = W_{\\text{value}}\\x\\) and \\(Q = W_{\\text{query}}\\x\\), where \\(W_{\\text{key}}, W_{ \\text{value}}, W_{\\text{query}}\\) are learned weight matrices. where \\(Q = W_{\\text{query}}\\x\\) as in regular self-attention. Trainable parameters at all layers helps where \\(W_{\\text{down}}\\in \\R^{r \\times d}\\) and \\(W_{\\text{up}}\\in \\R^{d \\times r}\\) are learned weights that project \\(x\\) down to a bottleneck dimension \\(r\\) and back up to dimension \\(d\\), and \\(\\sigma\\) is a non-linear activation function. The result \\(\\text{Adapter}(x)\\) is a vector in \\(\\R^d\\) with the same dimensionality as \\(x\\).",
            "",
            ""
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/environment/": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/environment/": "Environmental impact",
            "https://stanford-cs324.github.io/winter2022/lectures/environment/#life-cycle-assessment": "",
            "https://stanford-cs324.github.io/winter2022/lectures/environment/#climate-change": "",
            "https://stanford-cs324.github.io/winter2022/lectures/environment/#energy-use-and-greenhouse-gas-emissions": "",
            "https://stanford-cs324.github.io/winter2022/lectures/environment/#estimating-emissions-for-training-models": "",
            "https://stanford-cs324.github.io/winter2022/lectures/environment/#strubell-et-al-2018": "",
            "https://stanford-cs324.github.io/winter2022/lectures/environment/#patterson-et-al-2021": "",
            "https://stanford-cs324.github.io/winter2022/lectures/environment/#python-packages": "",
            "https://stanford-cs324.github.io/winter2022/lectures/environment/#summary": "",
            "https://stanford-cs324.github.io/winter2022/lectures/environment/#further-reading": ""
        },
        "text_between_h2": [
            "This section is largely based onLigozat et al. (2021). Philosophy. Most work on the environmental impact of AI and machine learning focuses on greenhouse gas emissions (motivated by climate change), but it is important (though difficult) to take a systems approach to think about: Life cycle assessment (LCA). Life cycle of IT equipment: Considerations in the life cycle: The \u2018Use\u2019 stage:  Environmental impact: Other second-order effects (more details):",
            "While it is important to think about the full life cycle, we will primarily focus on climate change and greenhouse gas emissions, since this is what much of the environmental impact of AI and machine learning focuses on. Temperaturesare rising: Negative impacts: Causes: Measurement of carbon emissions iskg CO2 eq:",
            "We have so far discussed greenhouse gas emissions and its effect on climate change, an especially salient form of environmental impact. Data centers use energy (in the form of electricity). How does that map onto emissions? The answer is it depends how that electricity is being generated. Carbon intensity: amount of carbon emitted per kilowatt hour of energy used (source)  FromLacoste et al. (2019): de Chalendar et al. 2019 Data centersstatistics (Md Abu Bakar Siddik et al., 2021):",
            "Now let us try to compute the energy use and therefore greenhouse gas emissions for training jobs. ML CO2 Impact Calculator(Lacoste et al., 2019) provides an easy way to estimate emissions based on hardware, hours use, provider, and region. This was the first paper to really spark awareness of environmental impact within the NLP community. Compute power use in kWh: They used average values: Results.",
            "Simple formula: Many design decisions  For training: Estimates of models: Rebuttal to Strubell et al. (2019)\u2019s neural architecture search estimate: Points:",
            "",
            "",
            "General information:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/environment/#life-cycle-assessment": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/environment/#life-cycle-assessment": ""
        },
        "text_between_h2": [
            "This section is largely based onLigozat et al. (2021). Philosophy. Most work on the environmental impact of AI and machine learning focuses on greenhouse gas emissions (motivated by climate change), but it is important (though difficult) to take a systems approach to think about: Life cycle assessment (LCA). Life cycle of IT equipment: Considerations in the life cycle: The \u2018Use\u2019 stage:  Environmental impact: Other second-order effects (more details):",
            "While it is important to think about the full life cycle, we will primarily focus on climate change and greenhouse gas emissions, since this is what much of the environmental impact of AI and machine learning focuses on. Temperaturesare rising: Negative impacts: Causes: Measurement of carbon emissions iskg CO2 eq:",
            "We have so far discussed greenhouse gas emissions and its effect on climate change, an especially salient form of environmental impact. Data centers use energy (in the form of electricity). How does that map onto emissions? The answer is it depends how that electricity is being generated. Carbon intensity: amount of carbon emitted per kilowatt hour of energy used (source)  FromLacoste et al. (2019): de Chalendar et al. 2019 Data centersstatistics (Md Abu Bakar Siddik et al., 2021):",
            "Now let us try to compute the energy use and therefore greenhouse gas emissions for training jobs. ML CO2 Impact Calculator(Lacoste et al., 2019) provides an easy way to estimate emissions based on hardware, hours use, provider, and region. This was the first paper to really spark awareness of environmental impact within the NLP community. Compute power use in kWh: They used average values: Results.",
            "Simple formula: Many design decisions  For training: Estimates of models: Rebuttal to Strubell et al. (2019)\u2019s neural architecture search estimate: Points:",
            "",
            "",
            "General information:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/environment/#climate-change": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/environment/#climate-change": ""
        },
        "text_between_h2": [
            "This section is largely based onLigozat et al. (2021). Philosophy. Most work on the environmental impact of AI and machine learning focuses on greenhouse gas emissions (motivated by climate change), but it is important (though difficult) to take a systems approach to think about: Life cycle assessment (LCA). Life cycle of IT equipment: Considerations in the life cycle: The \u2018Use\u2019 stage:  Environmental impact: Other second-order effects (more details):",
            "While it is important to think about the full life cycle, we will primarily focus on climate change and greenhouse gas emissions, since this is what much of the environmental impact of AI and machine learning focuses on. Temperaturesare rising: Negative impacts: Causes: Measurement of carbon emissions iskg CO2 eq:",
            "We have so far discussed greenhouse gas emissions and its effect on climate change, an especially salient form of environmental impact. Data centers use energy (in the form of electricity). How does that map onto emissions? The answer is it depends how that electricity is being generated. Carbon intensity: amount of carbon emitted per kilowatt hour of energy used (source)  FromLacoste et al. (2019): de Chalendar et al. 2019 Data centersstatistics (Md Abu Bakar Siddik et al., 2021):",
            "Now let us try to compute the energy use and therefore greenhouse gas emissions for training jobs. ML CO2 Impact Calculator(Lacoste et al., 2019) provides an easy way to estimate emissions based on hardware, hours use, provider, and region. This was the first paper to really spark awareness of environmental impact within the NLP community. Compute power use in kWh: They used average values: Results.",
            "Simple formula: Many design decisions  For training: Estimates of models: Rebuttal to Strubell et al. (2019)\u2019s neural architecture search estimate: Points:",
            "",
            "",
            "General information:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/environment/#energy-use-and-greenhouse-gas-emissions": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/environment/#energy-use-and-greenhouse-gas-emissions": ""
        },
        "text_between_h2": [
            "This section is largely based onLigozat et al. (2021). Philosophy. Most work on the environmental impact of AI and machine learning focuses on greenhouse gas emissions (motivated by climate change), but it is important (though difficult) to take a systems approach to think about: Life cycle assessment (LCA). Life cycle of IT equipment: Considerations in the life cycle: The \u2018Use\u2019 stage:  Environmental impact: Other second-order effects (more details):",
            "While it is important to think about the full life cycle, we will primarily focus on climate change and greenhouse gas emissions, since this is what much of the environmental impact of AI and machine learning focuses on. Temperaturesare rising: Negative impacts: Causes: Measurement of carbon emissions iskg CO2 eq:",
            "We have so far discussed greenhouse gas emissions and its effect on climate change, an especially salient form of environmental impact. Data centers use energy (in the form of electricity). How does that map onto emissions? The answer is it depends how that electricity is being generated. Carbon intensity: amount of carbon emitted per kilowatt hour of energy used (source)  FromLacoste et al. (2019): de Chalendar et al. 2019 Data centersstatistics (Md Abu Bakar Siddik et al., 2021):",
            "Now let us try to compute the energy use and therefore greenhouse gas emissions for training jobs. ML CO2 Impact Calculator(Lacoste et al., 2019) provides an easy way to estimate emissions based on hardware, hours use, provider, and region. This was the first paper to really spark awareness of environmental impact within the NLP community. Compute power use in kWh: They used average values: Results.",
            "Simple formula: Many design decisions  For training: Estimates of models: Rebuttal to Strubell et al. (2019)\u2019s neural architecture search estimate: Points:",
            "",
            "",
            "General information:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/environment/#estimating-emissions-for-training-models": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/environment/#estimating-emissions-for-training-models": ""
        },
        "text_between_h2": [
            "This section is largely based onLigozat et al. (2021). Philosophy. Most work on the environmental impact of AI and machine learning focuses on greenhouse gas emissions (motivated by climate change), but it is important (though difficult) to take a systems approach to think about: Life cycle assessment (LCA). Life cycle of IT equipment: Considerations in the life cycle: The \u2018Use\u2019 stage:  Environmental impact: Other second-order effects (more details):",
            "While it is important to think about the full life cycle, we will primarily focus on climate change and greenhouse gas emissions, since this is what much of the environmental impact of AI and machine learning focuses on. Temperaturesare rising: Negative impacts: Causes: Measurement of carbon emissions iskg CO2 eq:",
            "We have so far discussed greenhouse gas emissions and its effect on climate change, an especially salient form of environmental impact. Data centers use energy (in the form of electricity). How does that map onto emissions? The answer is it depends how that electricity is being generated. Carbon intensity: amount of carbon emitted per kilowatt hour of energy used (source)  FromLacoste et al. (2019): de Chalendar et al. 2019 Data centersstatistics (Md Abu Bakar Siddik et al., 2021):",
            "Now let us try to compute the energy use and therefore greenhouse gas emissions for training jobs. ML CO2 Impact Calculator(Lacoste et al., 2019) provides an easy way to estimate emissions based on hardware, hours use, provider, and region. This was the first paper to really spark awareness of environmental impact within the NLP community. Compute power use in kWh: They used average values: Results.",
            "Simple formula: Many design decisions  For training: Estimates of models: Rebuttal to Strubell et al. (2019)\u2019s neural architecture search estimate: Points:",
            "",
            "",
            "General information:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/environment/#strubell-et-al-2018": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/environment/#strubell-et-al-2018": ""
        },
        "text_between_h2": [
            "This section is largely based onLigozat et al. (2021). Philosophy. Most work on the environmental impact of AI and machine learning focuses on greenhouse gas emissions (motivated by climate change), but it is important (though difficult) to take a systems approach to think about: Life cycle assessment (LCA). Life cycle of IT equipment: Considerations in the life cycle: The \u2018Use\u2019 stage:  Environmental impact: Other second-order effects (more details):",
            "While it is important to think about the full life cycle, we will primarily focus on climate change and greenhouse gas emissions, since this is what much of the environmental impact of AI and machine learning focuses on. Temperaturesare rising: Negative impacts: Causes: Measurement of carbon emissions iskg CO2 eq:",
            "We have so far discussed greenhouse gas emissions and its effect on climate change, an especially salient form of environmental impact. Data centers use energy (in the form of electricity). How does that map onto emissions? The answer is it depends how that electricity is being generated. Carbon intensity: amount of carbon emitted per kilowatt hour of energy used (source)  FromLacoste et al. (2019): de Chalendar et al. 2019 Data centersstatistics (Md Abu Bakar Siddik et al., 2021):",
            "Now let us try to compute the energy use and therefore greenhouse gas emissions for training jobs. ML CO2 Impact Calculator(Lacoste et al., 2019) provides an easy way to estimate emissions based on hardware, hours use, provider, and region. This was the first paper to really spark awareness of environmental impact within the NLP community. Compute power use in kWh: They used average values: Results.",
            "Simple formula: Many design decisions  For training: Estimates of models: Rebuttal to Strubell et al. (2019)\u2019s neural architecture search estimate: Points:",
            "",
            "",
            "General information:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/environment/#patterson-et-al-2021": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/environment/#patterson-et-al-2021": ""
        },
        "text_between_h2": [
            "This section is largely based onLigozat et al. (2021). Philosophy. Most work on the environmental impact of AI and machine learning focuses on greenhouse gas emissions (motivated by climate change), but it is important (though difficult) to take a systems approach to think about: Life cycle assessment (LCA). Life cycle of IT equipment: Considerations in the life cycle: The \u2018Use\u2019 stage:  Environmental impact: Other second-order effects (more details):",
            "While it is important to think about the full life cycle, we will primarily focus on climate change and greenhouse gas emissions, since this is what much of the environmental impact of AI and machine learning focuses on. Temperaturesare rising: Negative impacts: Causes: Measurement of carbon emissions iskg CO2 eq:",
            "We have so far discussed greenhouse gas emissions and its effect on climate change, an especially salient form of environmental impact. Data centers use energy (in the form of electricity). How does that map onto emissions? The answer is it depends how that electricity is being generated. Carbon intensity: amount of carbon emitted per kilowatt hour of energy used (source)  FromLacoste et al. (2019): de Chalendar et al. 2019 Data centersstatistics (Md Abu Bakar Siddik et al., 2021):",
            "Now let us try to compute the energy use and therefore greenhouse gas emissions for training jobs. ML CO2 Impact Calculator(Lacoste et al., 2019) provides an easy way to estimate emissions based on hardware, hours use, provider, and region. This was the first paper to really spark awareness of environmental impact within the NLP community. Compute power use in kWh: They used average values: Results.",
            "Simple formula: Many design decisions  For training: Estimates of models: Rebuttal to Strubell et al. (2019)\u2019s neural architecture search estimate: Points:",
            "",
            "",
            "General information:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/environment/#python-packages": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/environment/#python-packages": ""
        },
        "text_between_h2": [
            "This section is largely based onLigozat et al. (2021). Philosophy. Most work on the environmental impact of AI and machine learning focuses on greenhouse gas emissions (motivated by climate change), but it is important (though difficult) to take a systems approach to think about: Life cycle assessment (LCA). Life cycle of IT equipment: Considerations in the life cycle: The \u2018Use\u2019 stage:  Environmental impact: Other second-order effects (more details):",
            "While it is important to think about the full life cycle, we will primarily focus on climate change and greenhouse gas emissions, since this is what much of the environmental impact of AI and machine learning focuses on. Temperaturesare rising: Negative impacts: Causes: Measurement of carbon emissions iskg CO2 eq:",
            "We have so far discussed greenhouse gas emissions and its effect on climate change, an especially salient form of environmental impact. Data centers use energy (in the form of electricity). How does that map onto emissions? The answer is it depends how that electricity is being generated. Carbon intensity: amount of carbon emitted per kilowatt hour of energy used (source)  FromLacoste et al. (2019): de Chalendar et al. 2019 Data centersstatistics (Md Abu Bakar Siddik et al., 2021):",
            "Now let us try to compute the energy use and therefore greenhouse gas emissions for training jobs. ML CO2 Impact Calculator(Lacoste et al., 2019) provides an easy way to estimate emissions based on hardware, hours use, provider, and region. This was the first paper to really spark awareness of environmental impact within the NLP community. Compute power use in kWh: They used average values: Results.",
            "Simple formula: Many design decisions  For training: Estimates of models: Rebuttal to Strubell et al. (2019)\u2019s neural architecture search estimate: Points:",
            "",
            "",
            "General information:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/environment/#summary": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/environment/#summary": ""
        },
        "text_between_h2": [
            "This section is largely based onLigozat et al. (2021). Philosophy. Most work on the environmental impact of AI and machine learning focuses on greenhouse gas emissions (motivated by climate change), but it is important (though difficult) to take a systems approach to think about: Life cycle assessment (LCA). Life cycle of IT equipment: Considerations in the life cycle: The \u2018Use\u2019 stage:  Environmental impact: Other second-order effects (more details):",
            "While it is important to think about the full life cycle, we will primarily focus on climate change and greenhouse gas emissions, since this is what much of the environmental impact of AI and machine learning focuses on. Temperaturesare rising: Negative impacts: Causes: Measurement of carbon emissions iskg CO2 eq:",
            "We have so far discussed greenhouse gas emissions and its effect on climate change, an especially salient form of environmental impact. Data centers use energy (in the form of electricity). How does that map onto emissions? The answer is it depends how that electricity is being generated. Carbon intensity: amount of carbon emitted per kilowatt hour of energy used (source)  FromLacoste et al. (2019): de Chalendar et al. 2019 Data centersstatistics (Md Abu Bakar Siddik et al., 2021):",
            "Now let us try to compute the energy use and therefore greenhouse gas emissions for training jobs. ML CO2 Impact Calculator(Lacoste et al., 2019) provides an easy way to estimate emissions based on hardware, hours use, provider, and region. This was the first paper to really spark awareness of environmental impact within the NLP community. Compute power use in kWh: They used average values: Results.",
            "Simple formula: Many design decisions  For training: Estimates of models: Rebuttal to Strubell et al. (2019)\u2019s neural architecture search estimate: Points:",
            "",
            "",
            "General information:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/environment/#further-reading": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/environment/#further-reading": ""
        },
        "text_between_h2": [
            "This section is largely based onLigozat et al. (2021). Philosophy. Most work on the environmental impact of AI and machine learning focuses on greenhouse gas emissions (motivated by climate change), but it is important (though difficult) to take a systems approach to think about: Life cycle assessment (LCA). Life cycle of IT equipment: Considerations in the life cycle: The \u2018Use\u2019 stage:  Environmental impact: Other second-order effects (more details):",
            "While it is important to think about the full life cycle, we will primarily focus on climate change and greenhouse gas emissions, since this is what much of the environmental impact of AI and machine learning focuses on. Temperaturesare rising: Negative impacts: Causes: Measurement of carbon emissions iskg CO2 eq:",
            "We have so far discussed greenhouse gas emissions and its effect on climate change, an especially salient form of environmental impact. Data centers use energy (in the form of electricity). How does that map onto emissions? The answer is it depends how that electricity is being generated. Carbon intensity: amount of carbon emitted per kilowatt hour of energy used (source)  FromLacoste et al. (2019): de Chalendar et al. 2019 Data centersstatistics (Md Abu Bakar Siddik et al., 2021):",
            "Now let us try to compute the energy use and therefore greenhouse gas emissions for training jobs. ML CO2 Impact Calculator(Lacoste et al., 2019) provides an easy way to estimate emissions based on hardware, hours use, provider, and region. This was the first paper to really spark awareness of environmental impact within the NLP community. Compute power use in kWh: They used average values: Results.",
            "Simple formula: Many design decisions  For training: Estimates of models: Rebuttal to Strubell et al. (2019)\u2019s neural architecture search estimate: Points:",
            "",
            "",
            "General information:"
        ]
    },
    "https://stanford-cs324.github.io/winter2022/lectures/#cs324-lecture-notes-winter-2022": {
        "links": {
            "https://stanford-cs324.github.io/winter2022/lectures/#cs324-lecture-notes-winter-2022": ""
        },
        "text_between_h2": [
            ""
        ]
    }
}